---
title: "Are you ready for an adventure?"
author:
  - name: Mihai Bizovi
    id: mb
    degrees: 
    - "VP of Decision Science"
format:
  html:
    toc-location: left
lang: en
---


There is one question I lose sleep over: "How should data science be taught?" In order to bring more clarity, I'll refer to this applied, quantitative, and interdisciplinary field as **decision science**. My first promise is that by following the recommended readings, you will understand what is meant by overused and often ambiguous terms like AI, ML, analytics, and data mining. 

::: {.column-margin}
As the author of the course -- all views and mistakes are entirely my own and do not represent any organization. 
:::

I want you to realize, that despite all the (justified) hype around deep learning and generative AI -- the well established fields of statistics, econometrics, operations research, control theory, dynamical systems, cognitive science, and CS have been evolving as well. These are not just prerequisites and intellectual forerunners for AI, but tools routinely and successfully used in a large problem space.

This website is not a self-contained course or book, but a vision, philosophy, and concrete roadmap for learning and teaching decision science. It emerges out of:

- Lessons learned from my industry experience as a data scientist, engineering and product manager, then leader 
- Teaching to creative and motivated students, who are confused by the overwhelming amount of content available online
- My frustrations about the shortcomings of statistical education
- Standing on the shoulder of giants. My journey was possible grace to outstanding teachers who made their courses freely available. I will polish, but not reinvent what is taught well! ^[Speaking pragmatically, I don't think we need another book on machine learning or data science, but in-depth and hands-on case-studies.]

I aspire to contribute to the understanding of this complex landscape and teach people how to navigate it, how to develop **valuable skills**, and become more **effective at problem-solving**. To achieve this, we need to master "[the fundamentals]{.underline}."

I believe that improving **decision-making** is one of the main reasons why we practice "data science" and "AI". The danger when an academic program lacks unified vision and a coordinated roadmap, is that we miss the interdisciplinary, big picture. To me, it's local optimization and a global valley. ^[For example, students might have an excellently taught Linear Algebra class from a mathematical point of view, but never see its relevance for ML or its geometric / computational aspects]  We can easily get lost in the weeds and forget the why.


::: {.callout-note}
## Who should read this guide?

*Anyone lost, confused, stuck or overwhelmed by Data Science and Machine Learning complexities, who wants to see the big picture, a path forward, and the possibilities.*

If you stumbled upon this website, you're probably a student in Business Analytics, or know me personally -- well, because I shamelessly promoted it.

Maybe, you're an **engineer** getting curious about ML or an **analyst** with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you're a **product manager** or an **entrepreneur** who wants to infuse AI into your startup.

In my opinion, junior data scientists and ML practitioners a few years into their journey will benefit the most from the re-contextualization of fundamentals that I'm doing here, which could enable them to take another **leap in career**.
:::

The commitment needed to get started in the field is so high, that a student is guaranteed to get lost in details, be overwhelmed by all those 800 page hardcore textbooks. On the other hand, seemingly pragmatic, cookbook and bootcamp-style approaches miss depth, nuance, key theoretical ideas, and methodological aspects. 


## Decision-Making for the Brave and True {.unnumbered}

This is an introductory, free, and open-source "course" which bridges the gap between theory and practice,^[There is a similar gap between the "math world" of elegant models / abstractions and messy "real world" of data and decisions in businesses
] cultivating the skills and understanding necessary to bring value to organisations by **improving decision-making**. It will help you develop a solid foundation, so you can choose your own path and adventure intentionally, in accord with your passion and career objectives.

A lot has been written about the journey from novice to expert and the process of mastery in chess, tennis, painting, music, etc. Decision science is a challenging and rewarding field, which will require two kinds of intentional practice and active learning over a long period of time:

- [Contemplating in the library]{.underline}: understanding the inner workings and relevance of key theoretical ideas
- [Engineering in the trenches]{.underline}: practicing battle-tested technologies and solving increasingly messier, more complicated and realistic problems



I have to clarify what I mean by contemplating in the library and  engineering in the trenches. In contemplation, we try to see how fundamental theoretical ideas translate into real-world stories and solutions to practical problems. We also focus on understanding the problem space in various domains, our clients' needs and challenges -- which enables us to ask the right questions, formulate the problem correctly, then pick the right model and methodology for the task. Last, but not least, we'll gain insights into pitfalls and common mistakes to avoid.

::: {.column-margin}
During the course, I present stories and toy problems which motivate probability distributions, LLN/CLT, Bayes' rule, sampling, measurement, etc. This is a way to highlight the **practical relevance** of otherwise dry theoretical ideas
:::

On the other hand, engineering in the trenches is a skill. The only way to become better at modeling and data analysis is hands-on practice. It is not sufficient to know about the modeling workflow and methodology. In practice, we have to develop pragmatic solutions and test, implement our ideas in code. If we are to increase our chances of success, we'll also have to follow best practices of reproducible research, automation, and model operationalization.


This is the **course I wish I had** when starting my journey in data science, which would prepare me for the realities of industry, often very different from the academic world. People usually blame this gap on programming and software engineering aspects. It is indeed correct, but we've got so many excellent resources about how to operationalize models and do reproducible data analysis, that it's not a huge problem in 2025. 

I suspect the real problem is methodological and organizational: a mismatch between business objectives, actions, constraints, tradeoffs, domain specificities and modeling. In other words, the ability to ask good questions, to formulate a problem, and think scientifically is critical. Therefore, I will blame most failed ML/AI projects and replication crisis in research on the **adhockery**. ^[Adhockery in modeling, in product-management, in data-mining, and software engineering to operationalize the models]

I will have to throw Kaggle under the bus here. Despite being an excellent platform for honing ML skills and developing better models, I think it gave too many people the impression that this is what data science is about. Kaggle misses the most important aspects: it is devoid of most of the original business context, problem formulation, and decision-making.

::: {.callout-tip}
## Think of youself as a business person with superpowers

This is the **best advice** I ever got as a novice data scientist. It is easy to lose track of this idea, especially if inamored with the technicalities of statistics and machine learning.
:::

In my teaching, I try very hard to bring back the scientific process and methodology in "data science". Understanding and applying a statistical tool or model by itself is not sufficient -- it would be skill without wisdom. We need case-studies in which we put the business context, decisions, domain understanding, and working software at the forefront. As R. McElreath sais, science before statistics!


The roadmap is challenging due to its breadth across disciplines and depth (starting from fundamentals).^[Of course, you can just listen, read and extract valuable insights, but the course ultimately fails without deep self-study.] In service of understanding, it becomes conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. Keep in mind that:

- It is NOT a theoretical or academic journey. Despite that, there is no shortcut around probability, mathematical statistics, and other prerequisites. There are two possibilities:
  - You already had those courses and will look at them with a fresh perspective on the second pass. It is ok to go back and reinforce fundamentals, clear up misunderstandings!
  - Otherwise, treat it as a learning roadmap of the most important ideas. It is easier to study e.g. probability, if you know its stories and practical relevance 
- At the same time, this is NOT a bootcamp, nor a replacement for learning programming in Python/R/SQL for data science
  - Therefore, it is not enough by itself to land you a job
  - For that, there are many wonderful resources for practice and study provided below

Think of it as a skeleton, a **conceptual frame** which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea years later, with greater wisdom and skill -- to unlock its real power. We should embrace the fact that learning is not linear. 

::: {.callout-warning}
## Overwhelming amount of content. What is best for learning?

Given the over-abundance of learning resources, it is too much to sift through a hundred pages of Coursera in the data science section, books, and tutorials to find optimal ones. There is a lot of great content, but it can be hard to decide where to start and our time is very limited. Even worse, poor quality resources can do more damage than good. 

The roadmap provided here should help you navigate the landscape of data science and find the shortest path towards developing your skills and better decisions in your firm. Pick a topic you're passionate about, for example, generalized linear models and look in the study guide for tips on where to read and how to practice.
:::


## Why should you care?

You might've heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, deep reinforcement learning models are beating people at go, Dota, chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week. There are so many cool applications of AI and modeling.

::: {.column-margin}
![](img/dont.jpg){width="80%"}
:::

::: {.callout-important}
## How do I keep up?

You don't, at least if you want to have a balanced life. That's why I choose to focus on fundamentals which stood the **test of time**, which are anyways a prerequisite before you dive into understanding the technicalities of those cutting-edge models and systems.

You will be surprised **how far can you go with simple, even linear models**. In the end, it is not a competition, nor am I against deep learning: we learn to solve a very different class of problems that businesses encounter.
:::

What does it actually mean, if we step outside the hype and buzzwords, use a plain language, and apply these ideas in down-to-earth, day-to-day problems and challenges in businesses? Data science and AI can have a function of decision-making support or be an essential part of the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. 

To get a better sense of what I mean by decision-making support, here is a list of challenges many data scientists are working on. Notice the common thread of optimization at a large scale, that many of these applications are related to important decisions in the value chain of a firm. These problems also can't be solved in an optimal way by simple rule-based business logic, classical algorithms, simple statistics, or exploratory data analysis.

- Demand forecasting, inventory optimization, production planning, routing and allocation in logistics
- Revenue management, pricing, and promotion optimization
- Advertisement, marketing mix, and conversion optimization
- Customer behavior and preferences:
  - Churn, repurchase, engagement, LTV, willingness to pay
  - Fraud, credit default, insurance risk
  - Choice modeling, recommender systems, targeting and uplift models
- Improving products, assortment, merchandising

::: {.column-margin}
Speaking from experience, every problem listed here is challenging. The good news is that by having a good grasp of fundamentals, we can tackle any one of them. What I can't give you though, are universal recipes -- they do not exist!
:::

> I want you to take away ONE thing, that is "AI" and Data Science in Businesses boils down to: **Decision-Making under Uncertainty at Scale**


Even if you are not a data scientist, you will work with them in one form or another *(Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Decision-Maker, Domain Expert)*. Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.

In university years, you've probably been tortured by (or enjoyed) linear algebra, mathematical analysis, probability and statistics, operations research, differential equations, mathematical economics and cybernetics, algorithms and data structures, databases, object-oriented programming, econometrics and so on.

::: {.callout-tip}
## Why study all of this?

We live in a volatile, uncertain, complex and ambiguous world,[^index-2] but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence and robust predictions. 

We studied all of those prerequisites precisely in order to be well equipped for such challenges. For a more in-depth explanation of the essence of each subject and its relevance for decision science, read [section iv.](01_fundamentals/prerequisites.qmd)

:::

[^index-2]: [VUCA: a mental model to better understand the world](https://www.vuca-world.org/where-does-the-term-vuca-come-from/)


When somebody asks what have you learned in this course, I suggest two metaphors: one of simplification and another of seeing relations


::: {layout="[45, -2, 55]"}
![We see Pollock's **messy reality**, which is the data and observations. We want to get to Picasso's bare bones **essence**, for clarity and better decision-making](01_fundamentals/img/logo.jpeg "Reality"){.preview-image width="90%"}

![This is a **big picture** course, which re-contextualizes everything you have learned before, but didn't see how it fits together or how can it be implemented in practice to bring value to organisations, that is: be useful](01_fundamentals/img/elephant-blind.jpeg){width="90%"}
:::


## What will you learn?

At this point you've heard the word "model" a lot of times. In the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. Reality is overwhelming and combinatorially explosive in its possibilities. ^[Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etc] 

We build models, because we want to understand a system and capture / explain the essential aspects of the phenomena of interest, in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen. 

A model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions. This lack of focus on context, objectives, actions, causes, and outcomes is one of the biggest pitfalls in the teaching and practice of machine learning or econometrics. So, let's reframe our study objective.


::: {.callout-important}
## Modeling as a part of decision-making process 

$$Question \longrightarrow Modeling \longrightarrow Insight \longrightarrow Action \longrightarrow Outcome $$

We will study the necessary, but insufficient ^[The insufficiency comes from the fact that this course involves a lot of self-study and I can't ever claim an exhaustive coverage of such a complex field] fundamentals of **business economics** and **statistical modeling**:

- in order to ask the right questions
- so that we can build custom statistical models 
- which bring insight into consequences of actions and interventions
- so that we're informed which actions that should be taken
- such that a firm can achieve its tactical and strategic objectives

:::

We collect data, perform experiments, and build models in order to minimize the effect of our biases and foolishness. It's also important to remember that all models have assumptions, pressupositions, and limitations. They are little machines, golemns of prague which follow instructions precisely, and can backfire if used outside their range of validity. Remember that all models are wrong, but some are useful. They can be powerful, but lack wisdom, which is found in your domain / problem understanding and scientific thinking.

::: {.callout-tip}
## Three challenges in statisical modeling

A. Gelman highlights three different aspects of statistical inference. Always remember this when designing your study or learning about a new statistical tool! We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.

$$Sample \longrightarrow Population$$

$$Treatment \longrightarrow Control$$

$$Measurement \longrightarrow Construct$$

The holy grail is to build statistical models based on the [causal processes]{.underline} informed by theories and hypotheses. If we take into account how we [measured]{.underline},^[We'll dedicate one lecture on measurement, but you might have whole specialized classes on this topic] and [collected data]{.underline}, we'll increase our chances to generalize our conclusions and will have stronger evidence.
:::

You can think of this proposed roadmap as jumping through multiple buckets of decision science, as shown in the diagram below. It happens that in universities, each of these buckets is a course, for example, linear algebra, calculus, probability theory and mathematical statistics, applied regression, research methods, etc.

I will explain the prerequisites and "modules" in the next sections, but here is the gist of it. First, we need to be competent in programming for data analysis with R/Python/Julia and SQL in order to do anything even remotely sophisticated in practice. ^[Most universities introduce it far too late, but things are changing. I would argue that one has to start coding and analyzing data from stats 101 and linear algebra] Probability theory, inferential statistics, and regression is the foundation on which we can develop more powerful modeling approaches. The more in-depth knowledge, experience, and understanding we have in probability and stats, the more challenging problems we can tackle. 

Second, we need to understand the problem space, the domain we're working in, and what challenges businesses face. It doesn't have to be about business, just replace with your field of interest, e.g. finance, medicine, psychology, politics, ecology etc. An awareness of the scientific method in the general sense and research methods of the field will be extremely helpful in this pursuit.  

At last, we will develop 3 approaches to problem-solving which conveniently correspond to main challenges in statistics outlined above: experiment design, causal inference, and machine learning.^[I'm leaving out optimization and algorithms, which are very important, but more on the computer science side of things] Each of these approaches have their own concepts, ideas, tools, methodologies, processes, and workflows -- with some particularities, depending on the target domain. The common, unifying thread is prediction of unknown quantities and effects of causes, consequences of interventions.


![I want to highlight that if you have a good grasp of fundamental concepts in probability and statistics, then study Bayesian generalized linear models from the perspective of causal inference -- you will see the whole field as unified and have a very flexible tool to attempt to solve any challenge](img/roadmap.png) 

Bayesian inference and (multilevel) generalized linear models are general-purpose tools, which can be used for analyzing data and making inferences in a complicated experimental setup, in predictive applications, and for estimating causal effects. Think of this modeling approach as a more powerful, flexible, but computationally expensive workhorse in contrast with the linear regression.


::: {.callout-tip}
## Go to "Study Guide" pages for practice!

Conceptual understanding by itself is not enough. So, I curated a list of resources to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.

Also, keep an eye on the course [github repo](https://github.com/Bizovi/decision-making), in which we'll do a lot of simulations, some exciting projects (full stack data apps) and investigate common challenges with a fresh perspective.
:::


## On the shoulder of giants

I mentioned that my teaching (and industry practice) stands on the shoulder of giants, but I have to explain why I'm not writing a book yet and linking you instead to the best resouces I know of. First, I love programming, but teaching it is not my passion -- one has to pick their battles. The same is true for linear algebra and mathematical analysis, or generally, "math for ML".

When it comes to statistical modeling and machine learning, I can only aspire to get to the level of clarity and rigor provided by [Trevor Hastie](https://www.statlearning.com/), Bradley Efron, [Andrew Ng](https://www.andrewng.org/courses/), Yaser Abu-Mostafa, Shai Ben-David, [Richard McElreath](https://xcelab.net/rm/statistical-rethinking/), [Andrew Gelman](http://www.stat.columbia.edu/~gelman/book/), Jennifer Hill, Aki Vehtari, Stephen Boyd, Gilbert Strang, Steven Burton, Nathan Kutz, etc. ^[The list won't be exhaustive, even if I wanted]

No one has time to study it all and there is no single book which does everything best.^[A cheeky reference to the no free lunch theorem] So, by cherry-picking lectures, chapters, examples, and applications; then arranging them into a coherent roadmap, we get different perspectives and will start developing preferences for one way of teaching and one approach over another. 

The cleanest code associated to their applications is not always written by the authors. Often, the open-source community significantly improves it and we'll take advantage  of this fact. Sometimes, I don't like any code and will do it myself, but this is really, polishing an existing idea rather than an original contribution.

From the point of view of teaching, the only reason to write my own book is the following: we need engaging, high-quality, in-depth, hands-on, realistic case-studies. ^[Ok, enough hyphenated adjectives, you get the point] We have more than enough tutorials and toy examples. Often, the only criticism I can address to some of the books is that the code examples are applied to boring and overused problems and datasets. It is not a fair criticism, since their main goal is teaching how the models work. Moreover, no one will publish a thousand page textbook.

> The best kind of applications are relatively simple, but interesting and pose real modeling challenges. They capture the essence of the problem. This is why I heavily rely on Richard McElreath's "Statistical Rethinking, 2nd ed." and "Bayesian Data Analysis, 3rd ed." by Andrew Gelman et. al. 

In my opinion, writing such case-studies takes more time than actually solving the problem inside a firm. I will do it, someday. In general, after you studied from the linked materials,  tried to write the code without reference, and understood the examples in depth -- it is your responsibility to come up with a project or case-study. It can be on openly available or private data, on messy, real data, or your own, clean simulations. It's a good idea to choose a domain, problem, and question you're passionate about.


## Prerequisites and background


I think this course will bring the most value to masters' students, professionals, and students in their last year of BSc, because of presumed exposure and competence in the following subjects. For more resources and explanations, see [section iv.](01_fundamentals/prerequisites.qmd)

::: {.column-margin}
![Decision-making, ML, and causal inference is hard. Practice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!](01_fundamentals/img/karate-kid.webp "Practice"){width="90%"}
:::


- For **Linear Algebra** and **Calculus** -- only exposure is needed, but competence and mathematical maturity will help a lot. If you need a refresher, a crash course will do -- for example, Grant Sanderson's ["Essence of Linear Algebra"](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&si=3pvYphbdL56dnYVH)
- For **Probability Theory** -- competence is needed, even though we start from the very beginning with a review of combinatorics. I suggest you read along and practice with a more comprehensive resource, like Joe Blitzstein's ["Probability 110"](https://projects.iq.harvard.edu/stat110/home)
- For **Mathematical Statistics** the story is the same as for Probability. You will need at least to be familiar with estimators, sampling distributions, LLN/CLT, hypothesis testing and regression. This course can catalyze the process. 
- **Python or R programming** for Data Science is **mandatory**, including data wrangling, visualization, scientific computing, databases. I recommend two free books:
  - ["R for Data Science"](https://r4ds.had.co.nz/) by Hadley Wickham 
  - ["Python for Data Analysis"](https://wesmckinney.com/book/) by Wes McKinney.
  - Of course, the more experience you have in one or both, the better
- This is completely optional, but if you have to deal with financial statements at your job or have an interest in finance, I strongly suggest you check out A. Damodaran's [crash course](https://youtube.com/playlist?list=PLUkh9m2BorqmKaLrNBjKtFDhpdFdi8f7C&si=UjEkr1_smeILJ8ZF) on accounting and corporate finance.

Besides technical prerequisites, there is a list of concepts and ideas from research design, which can be helpful in practice, but are often not part of statistical curriculum. I highly recommend A. Zand Scholten's introductory course on [Quantitative Methods](https://www.coursera.org/learn/quantitative-methods). It is worth to invest a bit of time to get familiar with the following:

::: {.column-margin}
Taken in isolation, topics like measurement and sampling are very abstract. Therefore, they have to be integrated into the case-studies whenever they bring added value. This is not easy, so .. work in progress
:::

- **Measurement** is especially relevant and tricky in social science. Sometimes, we do not measure what we think we do (underlying construct). For measuring abstract things like happiness or personality, we need to dive into scales, instrumentation, operationalization, validity, and reliability. 
- **Sampling** isn't limited to simple random samples, as these rarely occur in practice. We need to be aware what tools exist so that we can generalize to the population of interest. For example, weighting and post-stratification.
- **Threats to validity**, including confounds, artifacts, and biases. On the one hand we use statistics as a guardrail against foolishness, on the other hand there a lot of the same problems that can compromise our study.
- **Reproducible research** and literate programming has become a de-facto standard in the data science world. We have amazing tools right now, but it's still not easy to achieve end-to-end reproducibility and automation.
- **Academic writing** is an important skill, even in businesses. Writing is both a way of thinking, problem-solving, and communication. In my opinion, the structure of a scientific paper is very helpful to crystalize your ideas.

Last, but not least, there is value in understanding the **scientific process**, in its broad sense. In [section v.](sci_process.qmd), I adapt it to the context of decision science in businesses and show how most processes and workflows like CRISP-DM,  Tuckey's EDA are a part of it.



## Module 1: Business School for Data Scientists

Remember the advice "think of yourself as a business person with superpowers"? It means that as a decision scientist, it's helpful to think question, action, and outcome first -- and only then about models. In other words, we have to deeply understand our client's business domain and **problem space**.  The best way to achieve that, is experience and collaboration on the job.^[Studying business economics, marketing, management, finance, and logistics will also help a lot] Therefore, my role is to tell as many stories from the trenches, so that you know what to expect in practice.



A good starting place is thinking about the purpose of a business from different perspectives.  Then, identify key decisions that firms have to make across many industries and domains. This exercise will show us how widespread are data science applications and use-cases, even in places we don't suspect. 

At this point it is important to clarify what we mean by AI, ML, analytics, data mining, statistics and when is it appropriate to choose one approach over another. You'll be surprised, but statistics is the hardest to define and often misunderstood, because of its inferential, experimental, and causal facets.

Next, we'll learn a few frameworks for understanding a firm's performance evolution, value chain, strategy and tie them all together in an analyst's workflow. These tools will help us ask good (statistical) questions and increase the chances that our modeling efforts result in actionable insights.  

::: {.column-margin} 
In time, I'll add many more case-studies from advertisement, promotion, merchandising, engagement, CRM, conversion rate optimization, demand planning, supply chain, etc.
:::

An essential aid in this pursuit of improving decision-making at scale, are different processes and **methodologies** for machine learning, experiment design, causal inference, and exploratory data analysis. It is important to mention that methodology is not a recipe, but way of structuring our work, a set of guiding principles and constraints over the space of all the things that we could do in analysis. Don't think of these constraints as limiting your freedom, but as helpful allies in effective problem problem solving.

> "What is methodology good for? Sounds like I could use that time to learn ML"

These methodolgical fundamentals are not "just theory", it is what will make or break projects in practice. There are so many pitfalls in ML and statistics that we cannot afford to do it ad-hoc.

::: {.callout-note}
## Newsvendor problem, Demand planning, Price optimization

If you worry that the business school is too conceptual -- don't despair, as we'll have a few hands-on, technical lectures -- involving math, modeling, and programming. We will come back to these problems after getting the required background in probability and statistical modeling.

- I use the newsvendor problem as a starting point in modeling and optimization for demand planning
- The pricing optimization as a starting point towards revenue management
:::



## Module 2: Probability and Statistics

I think that most students will benefit from re-framing the fundamentals of probability and statistics. A lot of mistakes in practice come from a fundamental misunderstanding of the nature of statistical inference. If we view statistics as **changing our minds and actions in the face of evidence** -- the good ol' t-test will shine in a new light. It will become clear why those models and procedures were invented in the first place.

::: {.column-margin}
Another benefit of revising these fundamentals is that we can develop an appreciation for the stories and ideas behind these methods. Also, it can be fun.
:::

We start with combinatorics and urn models, probability trees, distributions, conditioning, laws of large numbers, central limit theorem, Bayes' rule. For each one of these topics, I present the story, use-case, and hands-on simulations to show that even very simple tools can be useful.

Perhaps the most important lecture is also very abstract, where I formally define what is probability, a random variable, what is the key difference between probability and statistics. Understanding what is collectivity, statistical population, sample; estimands, estimators, estimates; and formally, what is a statistical model -- will give you the right terminology and set you on a good path towards more advanced models.

The module culminates in practical aspects of experiment design and A/B testing, sample size justification, **power analysis**, and a conversation about the causes and remedies to the replication crisis. After all this effort put into the study of fundamentals, you can go with ease and confidence into Bayesian Statistics, Machine Learning, and Causal Inference.

::: {.column-margin}
When discussing hypothesis testing, it is very important to recognize that most statistical tests can be framed as linear models.
:::

::: {.callout-note}
## The art of formulating a hypothesis

In statistics' classes, the problem is usually completely framed and students focus on computation / interpretation. Experiment design in practice is tricky and more complicated than it looks in introductory hypothesis testing. We will use Cassie Kozyrkov's 12 steps of statistics to make sure that we don't forget some important aspect or potential pitfall. 
:::


My preferred sequence is to go in parallel with the "business school" and statistics, since there is a great synergy. The prior focuses on the problem space and processes (over content) of analytics, ML, statistics. I also recommend in case of a very short and intensive course to study  bayesian generalized linear models before the experiment design topics. 



## Module 3: Applied Bayesian Statistics

Once we have a confident grasp of the fundamentals, we continue on the path of applied Bayesian statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. 

::: {.column-margin}
![*(Source: [R. McElreath](https://youtu.be/KNPYUVmY3NM) -- Science before statistics)*](01_fundamentals/img/meme-causal.jpeg){width="90%"}
:::

Instead of applying an out-of-the-box model, we will build custom ones for each individual application in an iterative way. Choosing appropriate probability distributions and modeling assumptions will be critical in this process, along with model critique and validation. There are many ways to teach bayesian statistics and generalized linear models, but I think R. McElreath has the best approach, by giving a lot of importance to DAGs of influence, simulation, and ideas from causal inference.

::: {.callout-tip}
## Hierarchical Generalized Linear Models

Many experts in the field argue that this should be the default way we do regression analysis and that we need a very strong justification to show that a linear regression is the appropriate model. To put it bluntly, this modeling approach will enable us to improve on most challenges in decision-making and inference that businesses face. 
:::


One notable difference between the Bayesian approach and the traditional way advanced econometrics is taught, is that we will focus on computation instead of proofs and heavy-duty mathematical statistics. We declare the model and the sampler does the job for us! If it explodes, we probably modeled it wrong.


### Notes on Causal Inference and ML

Did you get comfortable with building custom statistical models for inference and prediction? What about decisions with high stakes, where we sometimes want to do randomized experiments?


Often, A/B tests and randomized controlled trials are **unfeasible** or **unethical**. Also realize that we cannot reach a causal conclusion from observational data alone -- we can talk just about **associations**. We need a theory, which is our understanding of how the "world" works -- translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).

This motivates a deep-dive into the field of causal inference. Think of it as a link between the theoretical models and observational data, where we sometimes can take advantage of certain "natural experiments". Causal inference requires deep thinking and understanding, which is truly challenging -- an art and science, in contrast with the "auto-magic", unreasonably effective pattern recognition of ML. 

Causal thinking is useful in description, inference, and prediction, as we need to know the causes of what makes our sample different from target population (for example, differential non-response, selection bias, or non-probability sampling). It also gives us a principled approach to deal with missing data, measurement error, and prevents us from drawing wrong conclusions when exploring data.

 One of the most important lessons is to be very careful when using ML to make predictions, when we actually want to intervene in a system. It can be shown that a model optimized for prediction can suggest nonsensical interventions and that a causal model can make subpar predictions.


::: {.callout-important}
## What if I care only about ML?

Even if you're interested only in machine learning, most practitioners will emphasize the importance of mastering regression (often starting with generalized linear models) and doing A/B tests resulting in evidence that our new model brings an improvement (or not). 

This is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML -- the lines between these two fields are indeed a bit blurry. This is especially the case when considering flexible models like Gaussian Processes and Bayesian Additive Regression Trees.
:::


::: {.column-margin}

![Navigate the complexities of the field and choose your path & adventure. *(Source: Generated with DALL-E)*](img/dalle_garden_forking_paths.png) 

:::

Other times, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.

The icing on the cake is miscellaneous topics dear to me and usually not covered in such a course: demand forecasting, recommender systems, and natural language processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.

::: {.callout-tip}
## Project: 5-page applied research paper

After finishing the first three modules, a good chance to put the knowledge to practice, is to write a 5-page research paper on a question / problem you're passionate about.^[In the labs, I give detailed instructions on how a good project should look like, starting from structure and ending in presentation] This will test your study design, modeling, programming, and writing skills.

We will see how easy is it to publish your article / paper / report with quarto and github pages.
:::


### Full-Stack Data Apps

I mentioned before that a key prerequisite is competence in Python/R, SQL, data wrangling, EDA, visualization, simulation, literate programming. There are many wonderful resources to learn and practice these skills and topics. During the labs we'll build from the ground up a tech stack for reproducible data analysis, model and data pipelines.


If you got to this point and want to continue, you will probably move onto Machine Learning and Causal Inference. There is, however, one huge problem when it comes to bringing added value in businesses -- [operationalizing models]{.underline}. 

I have been speaking at conferences about this topic for 6 years: from data, MLOps, product management, and organizational perspectives -- and it's still very relevant. Of course, the way we operationalize a model and build a system depends on the use-case and many factors -- however, the most general case to me is via a "Full Stack Data-Driven App" (with user interface, backend, database; data, training, and serving pipelines).



::: {.callout-tip}
## Build an impressive project for your portfolio

A full-stack data-driven app is your final project (for year two course) and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools and frameworks to make it easy for us. This will force you to think about the user need and how your software product solves their problem.

Don't worry about getting everything perfect, but focus on a problem and single area from the course you're passionate about: it could be statistical modeling, ML pipelines, or even frontend development and data visualization. 

:::
