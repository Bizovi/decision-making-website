---
title: "Are you ready for an adventure?"
author:
  - name: Mihai Bizovi
    id: mb
    degrees: 
    - "VP of Decision Science"
format:
  html:
    toc-location: left
lang: en
---


There are three questions I lose sleep over: What is data science? Why is it important? How should it be taught? The standard "intersection of statistics, economics, and computer science", answers none. Instead, I find it helpful to think of this [applied]{.underline}, [quantitative]{.underline}, and [interdisciplinary]{.underline} field as **decision science**.


::: {.column-margin}
As the author of the course -- all views and mistakes are entirely my own and do not represent any organization. 
:::

In the context of businesses, we want to improve financial and non-financial outcomes like revenue, profitability (EBITDA), market share, unit economics, production efficiency, customer acquisition, user experience, customer lifetime value, etc. A firm will increase its chances to improve performance if it has an accurate diagnosis of the current state and formulated a good strategy, but it still has to test their ideas and make many decisions under uncertainty.

A decision scientist should collaborate with domain experts, decision-makers, clients, and stakeholders to understand their domain and challenges, ask the right questions, and frame the problem space (formulate a problem). Then, they will collect data, build statistical models, and apply optimization algorithms which bring insights into consequences of actions, interventions, and policies. ^[Inspired by A. Fleischhacker's [Business Analyst Workflow](https://www.causact.com/becoming-a-data-driven-business-analyst#becoming-a-data-driven-business-analyst)] These insights, inferences, and predictions will inform the firm which decisions are more promising and whether their hypotheses hold up to evidence.

$$Question \longrightarrow Model \longrightarrow Insight \longrightarrow Action \longrightarrow Outcome $$ 

Not gonna lie, this task and endeavor is not easy at all. In this website, I will explain how to study and [master the fundamentals]{.underline} of statistical modeling, business economics, and programming for data analysis which will help you become more effective at problem-solving. These skills will be valuable in the majority of career paths you might choose.

Last, but not least, I aspire to teach people how to navigate this interdisciplinary landscape and gain an understanding of how statistics, machine learning, econometrics, operations research, AI, cognitive science, and scientific method fit together. ^[The terms of AI, data science, quant, analytics are overused. We will have to define them more precisely]


::: {.callout-note}
## Who should read this guide?

*Anyone lost, confused, stuck, or overwhelmed by data science and machine learning complexities, who wants to see the big picture, a path forward, and the possibilities afforded by decision science.*

If you stumbled upon this website, you're probably a student in Business Analytics, or know me personally -- well, because I shamelessly promoted it.

Maybe, you're an **engineer** getting curious about ML or an **analyst** with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you're a **product manager** or an **entrepreneur** who wants to infuse AI into your startup.

In my opinion, junior data scientists and ML practitioners a few years into their journey will benefit the most from the re-contextualization of fundamentals that I'm doing here, which could enable them to take another **leap in career**.
:::


This website is not a self-contained course or book, but a vision, philosophy, and concrete roadmap for learning and teaching decision science. It emerges out of:

- Lessons learned from my industry experience as a data scientist, engineering and product manager, then leader 
- Teaching to creative and motivated students, who are confused by the overwhelming amount of content available online
- The need to bridge the gap between theory and practice, between mathematical world of elegant abstractions and messy real world of data and decisions in businesses ^[There are many excellent textbooks, but we need better case-studies]
- The work of outstanding teachers and researchers who made their courses freely available. Without them, both my career and this guide would be impossible

A lot has been written about the journey from novice to expert and the process of mastery in programming, chess, tennis, painting, music, etc. Decision science is challenging due to its breadth across disciplines and depth (starting from fundamentals). It will require two kinds of intentional practice and active learning over a long period of time:

- [Contemplating in the library]{.underline}: understanding the inner workings and practical relevance of key theoretical ideas
- [Engineering in the trenches]{.underline}: practicing battle-tested technologies and solving increasingly messier, more complicated and realistic problems

In contemplation, we try to see how fundamental theoretical ideas translate into real-world stories and solutions to practical problems. In service of understanding, it becomes conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. We also focus on understanding the problem space in various domains, our clients' needs and challenges -- which enables us to ask the right questions, formulate the problem correctly, then pick the right model and methodology for the task. Last, but not least, we'll gain insights into pitfalls and common mistakes to avoid.

::: {.column-margin}
I my teaching, I present stories and use-cases which motivate probability distributions, LLN/CLT, Bayes' rule, sampling, measurement, etc.
:::

On the other hand, engineering in the trenches is a skill. The only way to become better at modeling and data analysis is hands-on practice. It is not sufficient to know about the modeling workflow and methodology. In practice, we have to develop pragmatic solutions and test, implement our ideas in code. If we are to increase our chances of success, we'll also have to follow best practices of reproducible research, automation, and model operationalization.


## Why is this journey rewarding?

You might've heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, deep reinforcement learning models are beating people at go, Dota, chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week. There are so many cool applications of AI and modeling.

::: {.column-margin}
![](img/dont.jpg){width="80%"}
:::


::: {.callout-important}
## How do I keep up?

You don't, at least if you want to have a balanced life. That's why I choose to focus on fundamentals which stood the test of time. You will be surprised how many problems that businesses encounter can be solved well with simple, even linear models.


These (statistical) fundamentals are anyways a prerequisite before diving into understanding the technicalities of those cutting-edge models and systems. I have nothing against deep learning, but we need to develop the wisdom to use the right tool for the right job.
:::

I want you to realize, that despite all the (justified) hype around deep learning and generative AI -- well established fields of statistics, econometrics, causal inference, operations research, control theory, dynamical systems, cognitive science, and computer science have been evolving as well. These are not just prerequisites and intellectual forerunners for AI, but tools routinely and successfully used in a large problem space.

::: {.callout-tip}
## Why study all of this?

We live in a volatile, uncertain, complex and ambiguous world,[^index-2] but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence, and robust predictions. 

People study the fields outlined above in order to be well equipped for such challenges. For a more in-depth explanation of prerequisites, purposes of each subject, and its relevance for decision science, read [section iv.](01_fundamentals/prerequisites.qmd)

:::

[^index-2]: [VUCA: a mental model to better understand the world](https://www.vuca-world.org/where-does-the-term-vuca-come-from/)



In businesses, data science and AI can have a function of decision-making support, process improvement, or be an essential part of the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. To get a better sense of what I mean by decision-making support, I will present a few challenges many data scientists are working on. 

These problems generally can't be solved in an "optimal" way by simple rule-based business logic, classical algorithms, or exploratory data analysis. Notice the common thread of optimization at a large scale and that many of these applications are related to important decisions in the value chain of a firm. 


- Demand forecasting and inventory optimization
- Production planning and quality control
- Revenue management, pricing optimization, and personalized promotions
- Estimating the impact of advertisement, marketing mix modeling, and conversion optimization
- Customer churn, repurchase, engagement, LTV
- Fraud detection, credit default prediction, insurance risk
- Choice modeling, recommender systems, targeting and uplift models
- Improving products, assortment, and merchandising

Don't worry if you're not familiar with some of the applications or that it's not clear yet which models and fundamentals will be used. We'll discuss it in great detail in the first module called "Business School for Data Scientists".



> I want you to take away one thing, that is "AI" and Data Science in Businesses boils down to: **Decision-Making under Uncertainty at Scale**


Even if you are not a decision scientist, you will work with them in one form or another *(Data Scientist, Statistician, Researcher, Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Domain Expert)*. Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.


::: {.callout-tip}
## Think of youself as a business person with superpowers

This is the **best advice** I ever got as a novice data scientist and it will be really helpful for peopleÂ who care a lot about the technical aspects of statistical and machine learning models.   

Your superpower is the ability to solve problems at scale and answer difficult questions by building models and programming. However, you should always put yourself in the shoes of the client and deeply understand their domain, data, objectives, tradeoffs, and decisions. Communicate persuasively, in their language.

This mindset will ensure that your solution is valuable and used. Otherwise there is a risk of it being interesting, but not understood, not trusted, not actionable and thus, not adopted.

:::



## Don't travel without a map

When you travel to an unknown place, you need a map to know your location and where are you going. Consider this guide a **conceptual frame** which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea years later, with greater wisdom and skill -- to unlock its real power. We should embrace the fact that learning is not linear. 

At the risk of annoying you with my metaphors, let me explain what is wrong with our existing maps. Some are too simplistic and low resolution, resulting in too much wandering around. Others are not maps at all, but a sequence of subjects to study -- which misses the interdisciplinary nature of decision science. At last, many are too detailed, overwhelming, and unclear about what is most important.

::: {.column-margin}
![This is a **big picture** course, which re-contextualizes everything you have learned before, but didn't see how it fits together or how can it be implemented in practice to bring value to organisations, that is: be useful](01_fundamentals/img/elephant-blind.jpeg){width="90%"}
:::


::: {.callout-warning}
## Overwhelming amount of content

If you go to the data science section in Coursera, you will see a hundred pages of courses and specializations. The situation is even worse with the amount of available tutorials.

A student is guaranteed to get lost in details and be overwhelmed by all those 800 page hardcore textbooks required by some classes. On the other hand, seemingly pragmatic, cookbook and bootcamp-style approaches miss depth, nuance, key theoretical ideas, and methodological aspects. 

Inevitably, you have to use the recommendations provided by other people. I really hope you will like the references I provide, which were carefully curated during the past 10 years. Almost all are free, open-source, and supplement the lectures / theory with code examples.
:::


This is the **course I wish I had** when starting my journey in data science, which would prepare me for the realities of industry, often very different from the academic world. In the following sections and chapters, I will outline a powerful set of fundamentals, the common threads and connections between them, where to read or watch to gain understanding, and how to practice in order to develop your skills.






## What is a model?

By now you've heard the word "model" a lot of times. In the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. Reality is overwhelming and combinatorially explosive in its possibilities. ^[Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etc] 

We build models, because we want to understand a system and capture / explain the essential aspects of the phenomena of interest, in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen. 

A model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions. I know this sounds very abstract, but be patient, I'll formalize what a statistical model means and give plenty of practical applications and examples. Until then, keep the following metaphors in mind: 

::: {.column-margin}
![What does Pollock and Picasso have to do with statistical modeling and quantitative questions?](01_fundamentals/img/logo.jpeg "Reality"){.preview-image width="90%"}
:::

::: {.callout-tip}
## Philosophical detour on latent processes

One of the most important ideas in statistics is that the causes are not found in data, but in scientific theories. In other words, the answer to our quantitative question is an unobserved, latent quantity or process. We collect data and perform experiments in order to infer those parameters. Hence, the data is the phenomenon, at the surface (Pollock's canvas) and we would like to know what mechanism could plausibly generate it.

On the other hand, models are useful because if successful, they separate the noise from the signal and synthesize all the relevant information we have in our sample. This generalization (abstracting away from idiosyncrasies of each observation) allows us to make predictions and gain insights. Think of the way Picasso drew a camel -- lots of artists will agree that simplification is also beautiful.

:::

We collect data, perform experiments, and build models in order to minimize the effect of our biases and foolishness. It's also important to remember that all models have assumptions, pressupositions, and limitations. They are little machines, golemns of prague which follow instructions precisely, and can backfire if used outside their range of validity. All models are wrong, but some are useful. They can be powerful, but lack wisdom, which is found in your domain / problem understanding and scientific thinking.


::: {.callout-tip}
## Three challenges in statisical modeling

A. Gelman highlights three different aspects of statistical inference. Always remember this when designing your study or learning about a new statistical tool! We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.

$$Sample \longrightarrow Population$$

$$Treatment \longrightarrow Control$$

$$Measurement \longrightarrow Construct$$

The holy grail is to build statistical models based on the [causal processes]{.underline} informed by theories and hypotheses. If we take into account how we [measured]{.underline},^[We'll dedicate one lecture on measurement, but you might have whole specialized classes on this topic] and [collected data]{.underline}, we'll increase our chances to generalize our conclusions and will have stronger evidence.
:::

## Prerequisites and background


I think this roadmap will bring the most value to masters' students, professionals, and students in their last year of BSc. If you never had a linear algebra, calculus, statistics, probability, and programming classes (or are very rusty in them) -- read [section iv.](01_fundamentals/prerequisites.qmd) for a guide on how to integrate these foundational subjects into your study plan.

Most people I know studied statistics and probability at some point, but the presentation was highly theoretical and without any programming or data analysis. I propose we go back to the most important ideas, discuss their practical relevance, and really understand them by coding up the simulations for our real-world stories. We will see that statistics is not just useful, but can be a lot of fun!

::: {.column-margin}
![Practice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!](01_fundamentals/img/karate-kid.webp "Practice"){width="90%"}
:::


- For **Linear Algebra** and **Calculus** -- only exposure is needed, but competence and mathematical maturity will help a lot. It is a personal choice how deep to dive, but generally, it will make your journey much easier.
- For **Probability Theory** -- competence is needed, even though we start from the very beginning with a review of combinatorics. I suggest you read along and practice with a more comprehensive resource, like Joe Blitzstein's ["Probability 110"](https://projects.iq.harvard.edu/stat110/home). As your understanding of probability grows, you can tackle more challenges. 
- For **Mathematical Statistics** the story is the same as for Probability. You will need at least to be familiar with estimators, sampling distributions, LLN/CLT, hypothesis testing and regression. This course attempts to clear up misunderstandings so that you don't fall into common pitfalls of statistical practice.
- **Python or R programming** for data science is **mandatory** in order to do anything even remotely sophisticated in practice. ^[Most universities introduce it far too late, but things are changing. I would argue that one has to start coding and analyzing data from stats 101 and linear algebra] Besides the basics of the language, we need to develop competence in data wrangling, visualization, SQL / databases, and reproducible data analysis. I recommend two free books:
  - ["R for Data Science"](https://r4ds.had.co.nz/) by Hadley Wickham 
  - ["Python for Data Analysis"](https://wesmckinney.com/book/) by Wes McKinney.
  - Of course, the more experience you have in one or both, the better
- If you have to deal with financial statements at your job or have an interest in finance, I strongly suggest you check out A. Damodaran's [crash course](https://youtube.com/playlist?list=PLUkh9m2BorqmKaLrNBjKtFDhpdFdi8f7C&si=UjEkr1_smeILJ8ZF) on accounting and corporate finance. Depending on your job, knowledge of microeconomics, marketing, management, operations, and logistics might be essential.

Besides technical prerequisites, there is a list of concepts and ideas from research design, which can be helpful in practice, but are often not part of statistical curriculum. I highly recommend A. Zand Scholten's introductory course on [Quantitative Methods](https://www.coursera.org/learn/quantitative-methods). It is worth to invest a bit of time to get familiar with the following:

::: {.column-margin}
Taken in isolation, topics like measurement and sampling are very abstract. Therefore, they have to be integrated into the case-studies whenever they bring added value. This is not easy, so .. work in progress
:::

- **Measurement** is especially relevant and tricky in social science. Sometimes, we do not measure what we think we do (underlying construct). For measuring abstract things like happiness or personality, we need to dive into scales, instrumentation, operationalization, validity, and reliability. 
- **Sampling** isn't limited to simple random samples, as these rarely occur in practice. We need to be aware what tools exist so that we can generalize to the population of interest. For example, weighting and post-stratification.
- **Threats to validity**, including confounds, artifacts, and biases. On the one hand we use statistics as a guardrail against foolishness, on the other hand there a lot of the same problems that can compromise our study.
- **Reproducible research** and literate programming has become a de-facto standard in the data science world. We have amazing tools right now, but it's still not easy to achieve end-to-end reproducibility and automation.
- **Academic writing** is an important skill, even in businesses. Writing is both a way of thinking, problem-solving, and communication. In my opinion, the structure of a scientific paper is very helpful to crystalize your ideas.

Last, but not least, there is value in understanding the **scientific process**, in its broad sense. In [section iii.](sci_process.qmd), I adapt it to the context of decision science in businesses and show how most processes and workflows like CRISP-DM,  Tuckey's EDA are a part of it.

![This poorly drawn diagram is a way to visualize the prerequisites (top and bottom rows) for the three modeling and problem-solving approaches which we will develop in the following modules. ](img/roadmap.png) 

What can you expect from the three modules? You probably heard the distinction between descriptive, predictive, and prescriptive analytics. I want to propose a better way of thinking about the problem-solving approaches available in our toolkit.

We leverage these prerequisites to deeply understand statistical inference applied to generalized linear models and complement it with causal thinking. This foundation will enable us to respond to the 3 challenges in statistics: generalizing from sample to population, from treatment to control group, and from measurement to the underlying construct / causal effect. 

In other words, we will learn how to design experiments to test our hypotheses and the effects of interventions, make predictions to optimize processes, and make inferences about the (potential) causes of the observed performance. ^[I'm leaving out optimization algorithms, which are very important, but more are more of an operations research and computer science topic]  Each of these approaches have their own concepts, ideas, tools, methodologies, processes, and workflows -- with some particularities, depending on the target domain. 


## Module 1: Business School and Methodology

Remember the advice "think of yourself as a business person with superpowers"? It means that as a decision scientist, it's helpful to think question, action, and outcome first -- and only then about models. In other words, we have to deeply understand our client's business domain and **problem space**.  The best way to achieve that, is experience and collaboration on the job. Therefore, my role is to tell as many stories from the trenches, so that you know what to expect in practice.

A good starting place is thinking about the purpose of a business from different perspectives.  Then, identify key decisions that firms have to make across many industries and domains. This exercise will show us how widespread are data science applications and use-cases, even in places we don't suspect. 

At this point it is important to clarify what we mean by AI, ML, analytics, data mining, statistics and when is it appropriate to choose one approach over another. You'll be surprised, but statistics is the hardest to define and often misunderstood, because of its inferential, experimental, and causal facets.

Next, we'll learn a few frameworks for understanding a firm's performance evolution, value chain, strategy and tie them all together in an analyst's workflow. These tools will help us ask good (statistical) questions and increase the chances that our modeling efforts result in actionable insights.  

::: {.column-margin} 
In time, I'll add many more case-studies from advertisement, promotion, merchandising, engagement, CRM, conversion rate optimization, demand planning, supply chain, etc.
:::

An essential aid in this pursuit of improving decision-making at scale, are different processes and **methodologies** for machine learning, experiment design, causal inference, and exploratory data analysis. It is important to mention that methodology is not a recipe, but way of structuring our work, a set of guiding principles and constraints over the space of all the things that we could do in analysis. Don't think of these constraints as limiting your freedom, but as helpful allies in effective problem problem solving.

These methodolgical fundamentals are not "just theory", it is what will make or break projects in practice. There are so many pitfalls in ML and statistics that we cannot afford to do it ad-hoc. In my teaching, I try very hard to bring back the scientific process and methodology into "data science". Understanding and applying a statistical tool or model by itself is not sufficient. We need case-studies in which we put the business context, decisions, domain understanding, and working software at the forefront. As R. McElreath sais, science before statistics! 

::: {.callout-warning}
## Why most AI/ML projects fail?

A lot of attention has been paid in the industry to the software engineering aspects of machine learning as one of the causes of failed projects. In the last 5 years, a lot of tools and best practices have emerged which can help us mitigate those risks and increase our chances of successful operationalization.

In my opinion, a bigger problem is methodological and organizational: a mismatch between business objectives, actions, constraints, tradeoffs, domain specificities vs modeling. This is why the ability to ask good questions, to formulate a problem, and think scientifically is critical. Therefore, an important goal of my writing is to recognize and avoid **adhockery**. ^[Adhockery in modeling, in product-management, in data-mining, and software engineering] 

There is one more problem you have to be aware of, which I call the "Kaggle phenomenon". Despite Kaggle competitions being an excellent platform for honing ML skills and developing better models, I think it gave too many people the impression that this is what data science is about. Kaggle misses the most important aspects of problem-solving and it is devoid of most of the original business context and decision-making.

:::


If you worry that the business school is too conceptual -- don't despair, as we'll have a few hands-on, technical lectures -- involving math, modeling, and programming. We will come back to these problems after getting the required background in probability and statistical modeling.

- I use the newsvendor problem as a starting point in modeling and optimization for demand planning
- The pricing optimization as a starting point towards revenue management



::: {.callout-tip}
## Go to "Study Guide" pages for practice!

Conceptual understanding by itself is not enough. So, I curated a list of resources to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.

Also, keep an eye on the course [github repo](https://github.com/Bizovi/decision-making), in which we'll do a lot of simulations, some exciting projects (full stack data apps) and investigate common challenges with a fresh perspective.
:::



## Module 2: Probability and Statistics

I think that most students will benefit from re-framing the fundamentals of probability and statistics. A lot of mistakes in practice come from a fundamental misunderstanding of the nature of statistical inference. If we view statistics as **changing our minds and actions in the face of evidence** -- the good ol' t-test will shine in a new light. It will become clear why those models and procedures were invented in the first place.

::: {.column-margin}
Another benefit of revising these fundamentals is that we can develop an appreciation for the stories and ideas behind these methods. Also, it can be fun.
:::

We start with combinatorics and urn models, probability trees, distributions, conditioning, laws of large numbers, central limit theorem, Bayes' rule. For each one of these topics, I present the story, use-case, and [hands-on simulations]{.underline} to show that even very simple tools can be useful.

Perhaps the most important lecture is also very abstract, where I formally define what is probability, a random variable, what is the key difference between probability and statistics. Understanding what is collectivity, statistical population, sample; estimands, estimators, estimates; and formally, what is a statistical model -- will give you the right terminology and set you on a good path towards more advanced models.

The module culminates in practical aspects of experiment design and A/B testing, sample size justification, **power analysis**, and a conversation about the causes and remedies to the replication crisis. After all this effort put into the study of fundamentals, you can go with ease and confidence into Bayesian Statistics, Machine Learning, and Causal Inference.

::: {.column-margin}
When discussing hypothesis testing, it is very important to recognize that most statistical tests can be framed as linear models.
:::

::: {.callout-note}
## The art of formulating a hypothesis

In statistics' classes, the problem is usually completely framed and students focus on computation / interpretation. Experiment design in practice is tricky and more complicated than it looks in introductory hypothesis testing. We will use Cassie Kozyrkov's 12 steps of statistics to make sure that we don't forget some important aspect or potential pitfall. 
:::


My preferred sequence is to go in parallel with the "business school" and statistics, since there is a great synergy. The prior focuses on the problem space and processes of analytics, ML, statistics. I also recommend in case of a very short and intensive course to study  bayesian generalized linear models before the experiment design topics. 



## Module 3: Applied Bayesian Statistics

Once we have a confident grasp of the fundamentals, we continue on the path of applied Bayesian statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. We will learn new tools to deal in a principled way with missing data, measurement error, sampling biases, nonlinearities, discreteness, dynamics, and heterogeneity.

::: {.column-margin}
![*(Source: [R. McElreath](https://youtu.be/KNPYUVmY3NM) -- Science before statistics)*](01_fundamentals/img/meme-causal.jpeg){width="90%"}
:::

Instead of applying an out-of-the-box model, we will build custom ones for each individual application in an iterative way. Choosing appropriate probability distributions and modeling assumptions will be critical in this process, along with model critique and validation. There are many ways to teach bayesian statistics and generalized linear models, but I think R. McElreath has the best approach, by giving a lot of importance to DAGs of influence, simulation, and ideas from causal inference.

::: {.callout-tip}
## Hierarchical Generalized Linear Models

Many experts in the field argue that this should be the default way we do regression analysis and that we need a strong justification to show that a simple linear regression is the appropriate model. 

This modeling approach will enable us to overcome the limitations of simpler models encountered before. Handling correlated and multilevel data is essential to make efficient inferences and draw correct conclusions.
:::

One notable difference between the Bayesian approach and the traditional way advanced econometrics is taught, is that we will focus on computation instead of proofs and heavy-duty mathematical statistics. In a sense, it means that we work with a single estimator which updates our prior beliefs about parameter distributions. We declare the model and the sampler does the job for us!  If it explodes, we probably mis-specified the model or priors.


### Notes on Causal Inference and ML

Did you get comfortable with building custom statistical models for inference and prediction? What about decisions with high stakes, where we sometimes want to do randomized experiments?


Often, A/B tests and randomized controlled trials are **unfeasible** or **unethical**. Also realize that we cannot reach a causal conclusion from observational data alone -- we can talk just about **associations**. We need a theory, which is our understanding of how the "world" works -- translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).

This motivates a deep-dive into the field of causal inference. Think of it as a link between the theoretical models and observational data, where we sometimes can take advantage of certain "natural experiments". Causal inference requires deep thinking and understanding, which is truly challenging -- an art and science, in contrast with the "auto-magic", unreasonably effective pattern recognition of ML. 

Causal thinking is useful in description, inference, and prediction, as we need to know the causes of what makes our sample different from target population (for example, differential non-response, selection bias, or non-probability sampling). It also gives us a principled approach to deal with missing data, measurement error, and prevents us from drawing wrong conclusions when exploring data.

 One of the most important lessons is to be very careful when using ML to make predictions, when we actually want to intervene in a system. It can be shown that a model optimized for prediction can suggest nonsensical interventions and that a causal model can make subpar predictions.


::: {.callout-important}
## What if I care only about ML?

Even if you're interested only in machine learning, most practitioners will emphasize the importance of mastering regression (often starting with generalized linear models) and doing A/B tests resulting in evidence that our new model brings an improvement (or not). 

This is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML -- the lines between these two fields are blurry. This is especially the case when considering flexible models like Gaussian Processes and Bayesian Additive Regression Trees.
:::


Other times, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.

There are other miscellaneous topics dear to me and usually not covered in such courses: demand forecasting, recommender systems, and natural language processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.

::: {.callout-tip}
## Project: 5-page applied research paper

After finishing the first three modules, a good chance to put the knowledge to practice, is to write a 5-page research paper on a question / problem you're passionate about.^[In the labs, I give detailed instructions on how a good project should look like, starting from structure and ending in presentation] This will test your study design, modeling, programming, and writing skills.

We will see how easy is it to publish your article / paper / report with quarto and github pages.
:::


### Full-Stack Data Apps

I mentioned before that a key prerequisite is competence in Python/R, SQL, data wrangling, EDA, visualization, and literate programming. There are many wonderful resources to learn and practice these skills. During the labs we'll build from the ground up a tech stack for reproducible data analysis, model and data pipelines.


If you got to this point and want to continue, you will probably move onto Machine Learning and Causal Inference. There is, however, one huge problem when it comes to bringing added value in businesses -- [operationalizing models]{.underline}. 

I have been speaking at conferences about this topic for 6 years: from data, MLOps, product management, and organizational perspectives -- and it's still very relevant. Of course, the way we operationalize a model and build a system depends on the use-case and many factors -- however, the most general case to me is via a "Full Stack Data-Driven App" (with user interface, backend, database; data, training, and serving pipelines).



::: {.callout-tip}
## Build an impressive project for your portfolio

A full-stack data-driven app is your final project and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools and frameworks to make it easy for us. This will force you to think about the user need and how your software product solves their problem.

Don't worry about getting everything perfect, but focus on a problem and single area from the course you're passionate about: it could be statistical modeling, ML pipelines, or even frontend development and data visualization. 

:::



## Acknowledgements

I mentioned that my teaching and industry success stands on the shoulder of giants. This is a metaphor scientists use to emphasize that we incrementally build on the work of others. 

When it comes to statistical modeling and machine learning, the following people had a large influence on the way I think and in turn, I will refer you to their excellent books and courses: [Trevor Hastie](https://www.statlearning.com/), Bradley Efron, Yaser Abu-Mostafa, Shai Ben-David, [Richard McElreath](https://xcelab.net/rm/statistical-rethinking/), [Andrew Gelman](http://www.stat.columbia.edu/~gelman/book/), Jennifer Hill, Aki Vehtari, Stephen Boyd, Gilbert Strang, Steven Burton, Nathan Kutz, Cassie Kozyrkov. 

At last, I have to explain why I'm not writing a book yet and linking you instead to the best resouces I know of. I think we have more than enough excellent books to learn the theory and develop skills in the fields within decision science. 

From the point of view of teaching, the only reason to write my own book is the following: we need engaging, high-quality, in-depth, hands-on, realistic case-studies. ^[Ok, enough hyphenated adjectives, you get the point] We have more than enough tutorials and toy examples. Often, the only criticism I can address to some of the books is that the code examples are applied to boring and overused problems and datasets. It is not a fair criticism, since their main goal is teaching how the models work. Moreover, no one will publish a thousand page textbook.


No one has time to study it all and there is no single book which does everything best.^[A cheeky reference to the no free lunch theorem] So, by cherry-picking lectures, chapters, examples, and applications; then arranging them into a coherent roadmap, we get different perspectives and will start developing preferences for one way of teaching and one approach over another. 

The cleanest code associated to applications is not always written by the authors. Often, the open-source community significantly improves it and we'll take advantage  of this fact. Sometimes, I don't like any code and will do it myself, but this is really, polishing an existing idea rather than an original contribution.


> The best kind of applications are relatively simple, but interesting and pose real modeling challenges. They capture the essence of the problem. This is why I heavily rely on Richard McElreath's "Statistical Rethinking, 2nd ed." and "Bayesian Data Analysis, 3rd ed." by Andrew Gelman et. al. 

In my opinion, writing such case-studies takes more time than actually solving the problem inside a firm. I will do it, someday. In general, after you studied from the linked materials,  tried to write the code without reference, and understood the examples in depth -- it is your responsibility to come up with a project or case-study. It can be on openly available or private data, on messy, real data, or your own, clean simulations. It's a good idea to choose a domain, problem, and question you're passionate about.
