---
format:
  html:
    toc-location: left
title: Fundamentals of probability and statistics
subtitle: A study guide through simulation
author:
  - name: Mihai Bizovi
    id: mb
    degrees: 
    - "VP of Decision Science"
---

In the introduction and first module (business school for data scientists), I tried to convince you that data science and AI in businesses is about quantitative questions, decision-making, and improving outcomes. By walking through many use-cases in different domains, we saw how widespread are its applications and how valuable a data scientist could be to a firm.


Previous readings should give you enough motivation, context, and background to decide if you want to follow the proposed path. If the answer is yes, the question becomes how to actually do it^[As a reminder: ask the right questions, in order to build models which bring insight into the consequences of our actions, to improve  outcomes.] and where to start. There are two key insights to remember:

- Methodology, asking good questions, and problem formulation is what will make or break your project in practice 
- The importance of mastering the fundamentals

In the introduction, I highlighted two types of prerequisites, which roughly translate to hard skills and soft skills. The hard skills are linear algebra, calculus, probability theory, inferential statistics, and programming for data science. The soft skills have a big overlap with research methods, scientific thinking, and business understanding.

Before explaining how to study and what are the most important concepts and skills to develop in probability and statistics, I want to tell you two important lessons from Patrick Winston's talk at MIT, "How to speak": 

- The quality of your idea and solution is a function of knowledge, practice, and talent; where talent is the least important -- $f(\mathbb{K}, \mathbf{P}, T)$
- Your success will be largely determined by your ability to speak, write, and the quality of your ideas -- $g(S, W, Q)$


My point is that you need both: skills and understanding, soft skills and hard skills. Many mistakes in statistical modeling come from a misunderstanding of the nature of statistical inference. Other mistakes are not statistical, but a poorly formulated problem, which leads to using the wrong tool, or the right tool in an incorrect way. There is this always-present gap between the real world and mathematical abstraction.

## Prerequisites

The way you study probability and statistics will depend on your background. In terms of prerequisites, you need an exposure to linear algebra, calculus or mathematical analysis, and inferential statistics (hypothesis testing and linear regression).

In this module, I make little use of linear algebra or calculus, as it is an applied, not proof-based or theoretical course. However, if you never had those courses, it will be very difficult to understand the meaning behind mathematical notation and abstractions. If you never had a statistics class on hypothesis testing and linear regression, you won't know the pain. Jokes aside, it means you will need to use another resource provided in section iv, in parallel with this module.

You might be at a point in your studies, usually [second year of bachelor's degree]{.underline}, when you don't know how to code in Python or R yet, and have a probability & mathematical statistics class. Maybe you have another one on applied regression and econometrics as well. This is a perfect moment to study the module fully,^[for example, in a simulation or scientific computing class] in order to:

- Learn programming for data analysis in R or Python, including data wrangling, visualization, and reproducible research
- Apply theoretical concepts and models to real-world stories and case-studies. This will highlight why is that theory relevant, useful, and why was it invented in the first place
- Understand how and why the theory works by doing lots of simulations. Find out under what conditions it fails 

If you already had the classes mentioned above, you might be in third, 4th year, masters' degree, or finished your formal studies. It's likely you know how to solve problems and get stuff done in R or Python. Then, I recommend you go through this module in an accelerated way,^[for example, using some parts of the module as a review in the context of a data science or econometrics' class] both in terms of theory and coding. You might ask, why not skip it entirely and focus on statistical modeling?

- First, it's a good review and warm-up with fun and realistic case-studies. We all get rusty when not practicing for a while
- A much more important reason is the re-contextualization of the fundamentals and getting rid of misconceptions and bad habits or practices
- I think that statistics is so tricky, that it's almost guaranteed we misunderstood something the first time we studied it


## Roadmap and the big picture

The module is build around stories and case-studies highlighting key concepts from probability theory, mathematical statistics, and applied regression modeling. We start from the very beginning, with counting and combinatorics, ending up with more sophisticated statistical models. 

There are a few purely theoretical lectures, but they are strictly necessary in order to combat common misconceptions and establish a correct, consistent terminology. The mind map below organizes the lectures into three main tracks: probability theory and simple Bayesian models, mathematical statistics and experiment design, regression modeling and estimation, sampling, and optimization algorithms. 



```{mermaid}
%%| label: fig-mermaid
%%| fig-width: 9
%%| fig-cap: |
%%|    Lectures in pink are purely theoretical. Lectures in yellow present the most important algorithms for estimating our workhorse models. The vast majority involve coding and data analysis

flowchart TD

    U[Introduction] -- Prob & Bayes --> Comb[Birthday problem. Combinatorics] --> PTr[Safari reservations. Binomial, Prob. trees. LLN] --> RVAR[Probability triple. Random variables]  --> NEWS[Newsvendor problem. Distribution stories] --> B[Medical testing. Bayes and Conditioning] --> BB[Left-handed proportion. Beta-Binomial] --> GP[Waiting times. Asthma. Gamma-Poisson ]

    U -- Math. stats --> E[Splitting U.S. Schools? Central limit theorem] --> SM[Population, sample. Estimands, estimators. ] --> PV[ P-values of many studies. Error control]  --> CI[Wikipedia A/B test. Confidence intervals] --> PC[How much data? Statistical Power. Experiment design]  --> RC[Dead salmon experiment. Replication Crisis] 

    U -- Modeling. Algorithms --> EC[Divorce rates. Three confounders] --> BV[Bootstrap conf intervals. Out of sample error] --> MCMC[Visiting islands. Markov Chain Monte Carlo] --> UCLA[Berkeley Admissions. Logistic regression] --> OPT[Gradient Descent. Convex optimization in GLMs] 

    style MCMC fill:#f7f5bc
    style OPT fill:#f7f5bc
    style BV fill:#f7f5bc

    style RVAR fill:#f5cdc4 
    style SM fill:#f5cdc4 
```

These topics and case-studies are designed for a whole semester of intensive study. My promise is that by the end of it, you will become a more confident programmer and will have a very solid foundation for further studies of statistical modeling, causal inference, and machine learning.

When teaching to masters' students, in the context of a data science class, I still feel it's necessary to walk through many of these topics. What is sacrificed though, is depth and a limited amount of time available for students to practice on these case-studies. In my opinion, this is evidence that everyone can benefit from this module: undergraduate and graduate students, people already working in the field as data scientists and analysts. Even machine learning engineers. 


There is one more catch. For each of these topics and case-studies, you might feel the need to dive deeper into the underlying mathematics, philosophy, and explore more applications. In other words, many rabbit holes. Your feeling is correct, there is so much to learn and explore, but this should not intimidate you. Instead, let it serve as motivation, dive into a topic that resonates with you and see where does it lead.

::: {.column-margin}
Time, attention, and energy are the only constraints you should keep in mind while diving much deeper into your chosen topic. 
:::


## Combinatorics. Sampling. Urn Models

There are several excellent courses which start from counting, and I think there are some useful and practical things ideas you missed about combinatorics. There are use-cases and stories with which you will never have to remember important results about sampling with/without replacement by heart. [^combinat-refs]

- Multiplication rule and the "garden of forking paths"
- Sampling with and without replacement. Ordered vs unordered
- Naive Probability and motivation for Kolmogorov's breakthrough
- Read this [short article](https://johnkerl.org/doc/prbstat/prbstat.html) about the difference between probability and statistics

::: {.callout-note}
## Birthday Paradox. Pigenhole Principle. Collision Problems

One of the reasons we're not intuitively good at probability, is that in many cases it is difficult to count the way favorable/possible outcomes could occur. The birthday problem is one of such examples and we'll do simulations and derive the analytical solution -- suggesting unexpected applications in computer science and engineering.
:::

[^combinat-refs]: There are three resources I recommend: Joe Blitzstein's [Lecture 1/2](https://youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&si=t-fvklHEnNNPY_mX) of Probability 110, Santosh Venkatesh's Probability Lectures, [Tableau 2](https://www.santoshvenkatesh.com/video-lectures), and Richard McElreath's ["Bayesian inference is just counting"](https://speakerdeck.com/rmcelreath/bayesian-inference-is-just-counting).

## Probability Triple. Random Variables. Models

If you understand and can explain the above ideas in a simple, yet rigorous way  -- you're ready for the journey. Otherwise, if it feels shaky[^shaky], here are some readings:

- Probability Triple and Random Variables - a quasi formal introduction is written in [this chapter](https://course.economic-cybernetics.com/01_fundamentals/stat_foundations.html) of the course website. From my experience, not many students have this understanding after their probability theory classes.
- Collectivity ("physical" structure), Statistical Population, Sample. We need to be a bit more precise in what we mean by a statistical model and a DGP. 
    - Defining the population and sampling process is a critical step in statistics. 
    - The population is the "contract" we're bound to when talking about inferences
    - It is a much more nuanced topic than it looks, explained well [here](https://openintro-ims.netlify.app/data-design.html) and [here](https://crumplab.com/statistics/04-SamplesPopulations.html).
- Parameter (Estimand), Estimator, Estimation/Statistic. Never confuse those! 


[^shaky]: Some of you might find reviewing this insulting because it's "trivial", or useless theory, or a frustrating reminder of probability classes. Please, bear with me -- because we will eliminate a whole class of errors practitioners make by not keeping these things in mind.


::: {.callout-important}
## Hypotheses, Process models, Statistical models

Some of the biggest debates in science, spanning across decades and causing much confusion and controversy could've been resolved much quicker by having this explicit distinction about Scientific Hypotheses, Process Models, and Statistical Models or procedures. 

I highly recommend this first [lecture](https://www.youtube.com/watch?v=FdnMWdICdRs) by Richard McElreath, showing how tricky could it be to map the correspondences between these three.

:::

## Stories behind distributions

You studied the properties of a whole zoo of probability distributions, but then, in statistics, encountered just a few -- especially in the context of hypothesis testing. In Module 3 (Applied Bayesian Statistics) we will need to know the stories[^stories] behind most of them, since the goal will be to build custom models for each application. 

[^stories]: There are particular physical processes and phenomena (stories, in general) which underly the patterns we observe. Often, those patterns can be accurately described by a particular probability distribution, governed by its parameters

- Bernoulli, Binomial, Hypergeometric, Negative Binomial [^prob-simulations]
- Poisson. Limiting and Conditioning. Overdispersion
- Beta, Gamma, Exponential. Exponential Family and Information Theory
- Statistical superstars: $\chi^2_k$,  $t_k$, $N(\mu, \sigma)$, $F(d_1, d_2)$
- Weirdos: Mixtures, Dirichlet, Multinomial, Weibull, heavy tails
- Remember the differences between PMF, PDF, CDF, MGF, $\mathbb{E}$, $\mathbb{V}$, $\mathbb{E}g(x)$


[^prob-simulations]: Look at some [examples with simulations](https://mathstat.slu.edu/~speegle/_book/probchapter.html#simulationsprob) and [stories](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view) / applications with more math from Joe Blitzstein.


::: {.callout-note}
## Couples showing up to a safari

We will analyze and simulate a simple, but tricky example to find out what is the probability of a given number of people who signed up to actually go for the safari.
:::

::: {.callout-note}
## The hot hand strikes again

We model streaks of successful shots in basketball and ask whether streaks of $k$ are surprising. I review the original paper by Kahneman and Tversky, what they did very well, but also potential problems with the statistical model
:::

::: {.callout-note}
## Poisson and the Prussian army

Originally, Poisson distribution was used to estimate deaths by horses in the Prussian Army. Here is the [historical data](https://rpubs.com/SmilodonCub/567089) and a [blog post](https://towardsdatascience.com/poisson-distribution-from-horse-kick-history-data-to-modern-analytic-5eb49e60fb5f) telling the story.
:::

I limit the number of hands-on applications for this chapter/lecture, not only because of time constraints, but also because most use-cases come in Module 3, in the context of more realistic problems. I hope, however, that I sparked an interest about how to approach Probability, especially when we draw DAGs to tell stories.


## Conditioning and Bayes Rule

There is a quote I like a lot: "Conditioning is the soul of statistics". The Bayes rule, which follows directly from the axioms of probability, is an essential in decision-making and the most important tool in this course -- both conceptually and technically. Any introduction to the subject will work out:

- A few excellent resources are Chapter 1/2 of [BDA3](http://www.stat.columbia.edu/~gelman/book/), or Chapter 1/2 of [Bayes Rules](https://www.bayesrulesbook.com/chapter-2.html), or Chapter 1/2 of [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). They will teach you about:
    - Conditioning, Marginalization, Priors, and Updating
- If you prefer videos, enjoy the 3Blue1Brown [visual masterpiece](https://www.youtube.com/watch?v=HZGCoVF3YvM) on how to think like a Bayesian or the explanation [here](https://www.coursera.org/learn/statistical-inferences/lecture/R6nV5/bayesian-thinking).
- I introduce the idea of Likelihood, which would serve us in future use-cases. It is another important perspective over statistical modeling to consider

::: {.callout-note}
## Medical testing for rare diseases
**Medical testing** for rare diseases, hypothetical example with code in my [course repository](https://github.com/Bizovi/decision-making/blob/main/playground/02_bayes.ipynb). We use the same idea to reason about how confident are we our code has no bugs.

If you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could've applied the same idea!

:::

::: {.column-margin}
Or maybe you're passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence
:::

For the simplest models, one approach of comparing different hypotheses is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look it up in the following courses [here](https://www.coursera.org/learn/statistical-inferences/supplement/IPkZK/assignment-2-2-bayesian-statistics) and [here](https://www.coursera.org/learn/bayesian/home/week/3) for the theory and examples.

::: {.callout-note}
## Football spreads and betting experts

(BDA3, Ch1): **Football spreads**, that can be estimated from [data about matches](http://www.stat.columbia.edu/~gelman/book/data/football.asc). What is the probability that a team wins? Are experts right, on average?

- If you're into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?
- For brevity, I won't elaborate much from now on, how to take an use-case and example to its limit. **If you're passionate about a particular topic -- go for it!**

:::

::: {.callout-note}
## Spelling Correction

(BDA3, Ch1): **Spelling correction**, based on [empirical frequencies](http://norvig.com/ngrams/) provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself -- it is good for a warm-up, but challenging enough to keep you occupied.

:::



::: {.callout-note}
## Monty Hall. Simpson's Paradox and DAGs

The Simpson's paradox is usually introduced to highlight the importance of conditioning. However, the only resource I found which gets to the core of the problem is Bradley Neal's [first lecture](https://www.bradyneal.com/causal-inference-course) on causal inference. 

The "paradox" part of it is resolved (or at least not puzzling), when we think about the causal structure of the problem (or the DAG of influences).
:::


## LLNs, CLTs. Estimator properties

Halfway in the module, we switch from Probability Theory to Mathematical Statistics. The goal is to develop the fundamentals needed for applied statistics, designing randomized experiments, and even machine learning. [^4]

- We continue with the key idea of estimators and sampling distributions, review laws of large numbers and the central limit theorem. See simulations [here](https://mathstat.slu.edu/~speegle/_book/SimulationRV.html#centrallimittheorem).
- If you're interested in the underlying theory, I go on a technical detour about convergence types: in probability, in distribution, and almost-sure
- What does a statistician want? Review important properties of estimators.
    - For an accessible explanation of Bias, Consistency, Efficiency -- showcased with the corresponding R code, see [openforecast](https://openforecast.org/sba/estimatesProperties.html)

[^4]: Although the perspective I take in Module 3 is Bayesian, I will take time in Module 2 to cover and re-contextualize the Neyman-Pearson frequentism

::: {.callout-note}
## The most dangerous equation

I think that ["The most dangerous equation"](http://assets.press.princeton.edu/chapters/s8863.pdf) is a must read for anyone, not just practicing scientists and statisticians. The example I usually do a demonstration on is about the dubious U.S. policy of splitting the bigger schools. 

:::

::: {.callout-note}
## Calling Bullshit: Best Barbecue

Continuing on the reddit examples, there are some amazing case-studies in the "Calling Bullshit" website and book. One of them is exactly such a ranking problem: [best barbecue in the states](https://www.callingbullshit.org/case_studies/case_study_barbecue.html). I recommend you watch the whole playlist and work through the case studies: it is fun and an essential skill -- to call out the bullshit.

Online platforms which have to rank posts and comments, face the challenges of how to take the sample size into account. It depends, but for inspiration, see the hackernoon ranking algorithm.
:::


## Bias-Variance. Fisher Information

I spend another lecture to deep-dive into estimators, because the concepts of bias-variance tradeoff and Fisher information have far-reaching consequences in a myriad of tools, applications and fields -- especially machine learning. It is also an appropriate point in time to introduce a technique which was revolutionary at its time: bootstrap.

There are objections to the Bias-Variance decomposition when seen as a tradeoff, in the context of Deep Learning -- however, in the most general sense, it is a universal problem not only in statistics, but also for human cognition. For an intuitive explanation, watch [lecture 8, slides](https://work.caltech.edu/lectures.html). See how this [tradeoff needs an update](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update) for the modern deep learning. 


::: {.column-margin}

This lecture is highly mathematical, but we will get some powerful intuitions about some fundamental tradeoffs we make in statistics, when selecting a model or estimator.

:::

- Bias-Variance decomposition and the curse of dimensionality
- Fisher Information and Rao-Cramer lower bound
- The Bootstrap scheme: motivation, applications, and limitations

::: {.callout-note}
## Bootstraping for Confidence Intervals

Bias-variance can be made more relatable in code, simulations, and visualization. However, I will not leave you hanging without introducing a technique you can use for solving practical and concrete problems, namely -- bootstrap.

:::


## Hypothesis testing. Neyman-Pearson

In order to make sense of frequentist hypothesis testing, I strongly recommend you read about the original idea of Neyman and Pearson (error control -- don't make a fool of yourself too often in the long run). It is a "path of action" perspective of statistics.

[This is certainly the most difficult lecture of the module, combining the math, programming, and even philosophy.]{.aside}

I start from the first principles and will let go of mechanical application of procedures and conventions (p-values, $\alpha, \beta$, test choice). You should to be able to justify all the choices you make during the phase of experiment design. 


- Picking a default action. [Type I, II errors](https://openintro-ims.netlify.app/decerr.html). How costly is each type of mistakes?
- Minimal **effect size** of interest, [Cohen's](https://rpsychologist.com/cohend/) $d$
- [Power Analysis](https://rpsychologist.com/d3/nhst/) and Sample Size justification. How surprising are significant findings under each hypothesis? Positive Predictive Value
- p-values [simulation](https://rpsychologist.com/pvalue/), [p-curve](https://rpsychologist.com/d3/pdist/) under $H_0, H_A$. 
- Confidence Intervals - first check out this [simulation](https://rpsychologist.com/d3/ci/).  Also [chapter 12](https://openintro-ims.netlify.app/foundations-bootstrapping.html), uses bootstrap to estimate those. The tricky idea of "capture percent"


::: {.column-margin}
In order to put everything together, there are four resources I can recommend:

- Speegle's [book](https://mathstat.slu.edu/~speegle/_book/HTCI.html) on data+probability+R
- Huber's [Chapter 6](https://www.huber.embl.de/msmb/06-chap.html) of Modern Statistics 
- [Statistical thinking for 21st century](https://statsthinking21.github.io/statsthinking21-core-site/ci-effect-size-power.html)
- [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/)
:::

::: {.callout-note}
## Asking better questions

The most complicated part of hypothesis testing is asking better questions. I mean that in a highly technical sense, and whole-heartedly recommend you the following course from a TU Eindhoven professor, named ["Improving your statistical questions"](https://www.coursera.org/learn/improving-statistical-questions/).

- Make riskier predictions: [Non-Inferiority testing](https://rpsychologist.com/d3/equivalence/), Equivalence Testing, Range predictions
- Publication bias, open science, pre-registrations
- Minimal Effect Size of interest: telescope method and resource-based
- Type 3 errors (solving the wrong problem)
- Read Werner Stahel's ["Relevance"](https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf) paper and Gelman's "Sign and Magnitude" [paper](https://stat.columbia.edu/~gelman/research/published/retropower_final.pdf)

:::


::: {.callout-note}
## A detour on the philosophy of science

- Understanding the philosophy of falsification and how it applies to hypothesis testing. [Week2 of this course](https://www.coursera.org/learn/improving-statistical-questions/lecture/j6Duu/lecture-2-1-falsifying-predictions-in-theory) has a great 20 minute explanation.
- Philosophy of science: Popper and Latakos, in this [lecture](https://www.youtube.com/watch?v=cgvKG_3Ck7Y). "The null is always false"
:::


::: {.callout-tip}
## Common statistical tests are linear models

There is a zoo of different statistical tests and procedures, which might be very confusing -- especially trying to remember their particularities. It's important to realize that a lot of seemingly unrelated statistical tests in frequentist statistics are particular versions of linear models.

- [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/#1_the_simplicity_underlying_common_tests) and the [python port](https://www.georgeho.org/tests-as-linear/) 
- Choosing a statistical test: difference in proportions and means, test of $\sigma$, correlations
- For a bayesian alternative to t-tests, see [Krutsche's example](https://www.pymc.io/projects/examples/en/latest/case_studies/BEST.html)
- If you're not clear if your distributional assumptions hold, use a nonparametric test
:::


## Frequentism vs Likelihood vs Bayes

There are three main schools of thought in statistics, which have their [respective metaphors](https://www.coursera.org/learn/statistical-inferences/lecture/qC3A1/frequentism-likelihoods-bayesian-statistics): "path of action" (Neyman-Pearson frequentism), "path of devotion" (Fisherian Likelihood), and "path of belief / knowledge" (Bayesian). I like very much the presentation of each school of thought in the book of Hastie/Efron "Computer Age Statistical Inference", [chapter 2, 3, 4](https://hastie.su.domains/CASI_files/PDF/casi.pdf). 

Each one has their strenghts, weaknesses, and contribute tools & insights for our future use-cases. When we got into the topic of A/B testing and experiment design, we unavoidably stumbled upon a few fascinating philosophical questions in relation to the nature of evidence. The philosophical debate is fierce, but in statistical practice, less so. I suggest a level of pragmatism to pick the right tool/perspective for the particular job. In the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It's an useful skill when critically reading the literature.


::: {.column-margin}
By now, you encountered the Neyman-Pearson (frequentist) approach. If you want another presentation, watch this [lecture](https://www.youtube.com/watch?v=LYcu3LoGqKc) by Zoltan Dienes to get a sense of the orthodox approach: its power and limitations.
:::

The likelihood approach is widely used in Machine Learning / Statistical Learning teaching and practice. This [lecture](https://www.youtube.com/watch?v=NHFfJEvzPIo) by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations. 



::: {.callout-note}
## Three approaches to single-parameter models

We can pick a simple example of inferring a proportion, which has many practical applications that you might remember from "Distribution Stories". We care not just about the estimation, but also about confidence/credible intervals and the practical workflow.

- Frequentist: Normal Approximation, Agresti-Coull intervals
- Likelihood: Maximum likelihood, point estimates, bootstrapping. Check out [this interactive visualization](https://rpsychologist.com/likelihood/) an [lecture / lab](https://www.coursera.org/learn/statistical-inferences/lecture/8yZDk/likelihoods). 
- Bayes: The full posterior distribution, the tricky business of prior choice

:::


## Dead Salmon Experiment. Replication Crisis 

Lastly, we can't avoid a conversation about the replication crisis happening in multiple disciplines, but especially in social sciences. What scientific literature can we trust? This is relevant not just for research and science, but will help you avoid many pitfalls in the business practice -- therefore, you will be less likely to be fooled by randomness.

- Multiple testing, p-hacking, HARKING, snooping. Ethics and Integrity
- Underpowered studies and vague questions
- Publication Bias, Open Science, Pre-registration and simulation
- False-discovery rate, Bonferoni correction
- Confounding, Mediation and all that causal jazz
- Computational Reproducibility vs Replication. Meta-Analysis


::: {.callout-note}
## Dead salmon experiment

An examination of a famous experiment in neuroscience, putting into question standard/current statistical practices, leads to a conversation of controversies in medicine, psychology, and social science. 

Just think about how important this experiment was for the field of medicine -- it won the [nobel prize](https://blogs.scientificamerican.com/scicurious-brain/ignobel-prize-in-neuroscience-the-dead-salmon-study/)!
:::


