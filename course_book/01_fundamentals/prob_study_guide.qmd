---
format:
  html:
    toc-location: left
title: Probability and statistics
subtitle: A study guide through simulation (WIP)
author:
  - name: Mihai Bizovi
    id: mb
    degrees: 
    - "VP of Decision Science"
---

This study guide is designed as a complementary resource to "Simulation of economic processes". It provides additional references for studying and practicing the fundamentals of probability and statistics.

It's possible you didn't enjoy your statistics classes. This happened in my case, despite a passion for the field. In this journey of decision science, we have to give statistics another chance, but we dramatically change the strategy of how we learn it. We'll heavily use simulations, stories, and case-studies which highlight the practical relevance of key theoretical ideas.^[Just flip through a modern book which takes a similar approach to get a sense of how different it looks from traditional teaching.]  Speegle's [Probability, Statistics, and Data: A fresh approach using R](https://mathstat.slu.edu/~speegle/_book/preface.html) and  Cetinkaya-Runde's [Introduction to Modern Statistics](https://openintro-ims.netlify.app/index.html) are excellent examples of this new way of teaching.

There is absolutely no shame in going back to a subject again with a mindset of mastering the fundamentals. For me, this exercise turned out to be one of the most valuable things I've done.

## Probability theory

First, read the [introductory chapter](/sim/1_intro.qmd) in which I explain how probability fits in the larger context of decision science and how can we use simulation to learn and appreciate it better. I highly recommend these [lectures](https://www.santoshvenkatesh.com/video-lectures) by Santosh A. Venkatesh (SV) and Joseph Blitzstein's [Probability 110](https://projects.iq.harvard.edu/stat110/home) course (JB) as your main references for probability theory.

### Combinatorics and sampling

In the [first lab](/sim/1L_bday_problem.qmd), where we simulated the [birthday problem]{.underline}, I mentioned the importance of knowing the stories behind core concepts in combinatorics, like ${n \choose k}$ "n choose k", factorial, falling factorial, and the multiplication rule. They translate into urn models and sampling procedures (ordered, unordered, with and without replacement).

 The first three lectures from SV and JB should be more than sufficient as a review of combinatorics and naive probability.^[Pay close attention to their story proofs and the effort they put in to get you over the fear of mathematical abstraction] In the latter you will see how urn models were successfully applied in modeling different phenomena in physics.


::: {.callout-warning }
## Rabbit hole: unordered sampling with replacement

In S. Venkatesh, Tableau 3, there is an elegant proof of unordered sampling with replacement, which was used by Bose and Einstein to describe the behavior of bosons. The result will be useful to us when we'll introduce bootstrap, but you can check out a neat [explanation](https://rpubs.com/riazakhan94/bootstrap_distinct_sample) by Riaz Khan.
:::

I understand if your reaction at this point is: "Are you kidding me, this is just a warm-up?". I would like to convince you that the time investment is not that big and the payoff is huge. You will encounter fun stories and applications in multiple domains, get comfortable with mathematical abstraction, and see that probability doesn't have to be a chore or just a prerequisite you have to get over with.


### Probability Triple and Random Variables

Read the [second lecture](/01_fundamentals/stat_foundations.html), which is a bit abstract and theoretical, but essential for understanding probability. By understanding how the probability triple is constructed and why we need the abstraction of random variables, the rest of machinery in probability will start making much more sense.


::: {.callout-note}
## Hypotheses, Process models, Statistical models

Some of the biggest debates in science, spanning across decades, causing much confusion and controversy could've been resolved much quicker by having this explicit distinction between Scientific Hypotheses, Process Models, and Statistical Models or estimators. 

I highly recommend this first [lecture](https://www.youtube.com/watch?v=FdnMWdICdRs) by Richard McElreath, where he shows how tricky could it be to map the correspondences between these three.
:::

This is a good place for you to understand the distinction between collectivity (physical structure / entities), statistical population, and sampling procedures.^[Sampling is a more nuanced topic than it looks, explained well [here](https://openintro-ims.netlify.app/data-design.html) and [here](https://crumplab.com/statistics/04-SamplesPopulations.html)] The second distinction is concerned with the data generating process: parameters (estimands), estimators, and estimates (statistic). Never confuse those!

### Binomial distribution. Simulation

In the [second lab](/sim/2L_safari.qmd), I introduce the Bernoulli (coin flip) and Binomial distribution with a story of people booking and showing up to a safari trip. We will see how our conclusions and calculations change when people are not deciding to show up independently.

::: {.callout-warning}
## Rabbit hole: the hot hand strikes again

You might've heard about the hot hand fallacy. We model streaks of successful shots in basketball and ask whether streaks of $k$ are surprising. Read the [original paper](https://www.sciencedirect.com/science/article/abs/pii/0010028585900106) A. Tversky and ignore the statistical test they designed.

Their insight that fans and experts in basketball overestimate the magnitude of this effect is fundamentally correct. However, what doesn't follow from their research is that there is no effect, which is pointed out by a few Bayesian statisticians in the recent years. The debate goes on, but the limitations of the original statistical test are undeniable.

This is a true rabbit hole. Right now, we have the tools to simulte what can we expect if there was no effect or a small effect, but we can't design a statistical test yet. For that, refer to the last section in Tableau 10 of SV, A. Tversky's original paper, and recent papers which challenge the statistical methodology and modeling approach based on which the original conclusion was reached.^[You can look at [this article](http://shiny.calpoly.sh/Hothand/hot_hand_paper_final.pdf) by Kevin Ross for a good overview of the original permutation test and about the ongoing debate]
:::

Despite the ongoing debate about the magnitude of the effect in the hot hand phenomenon, it is undeniable that such simulations are extremely valuable -- they prevent us from being fooled by randomness. Let's look at a few more examples in which simulation can be valuable in figuring out what's going on:

- Amateur traders might see "winning" patterns in stock markets which can be explained by a random walk. Be suspicious any time you see a "technical analysis"
- Policy-makers might not realize that amazing SAT performances in some schools with few students could be just due to small sample sizes
- In chess, some might attribute an observed pattern of ELO by gender to nonsensical explanations, when a simple simulation will show that an initial disparity in the number of players, combined with estimating a proportion in top k overall players results in similar outcomes
- Pollers and political scientists might underestimate the support for a political party due to differential non-response
- Medical research might overestimate the effectiveness of a treatment or underestimate side-effects due to drop-out and right-censoring 
- We might think there is an association between variables, when both can be explained by a common cause, a confounder


### Three core theorems: CLT, LLN, Uniform

In the [third lecture](/sim/3_lln.qmd), I introduce the most important theorems in probability: central limit theorem, law of large numbers, and the universality of uniform. The latter two justify why simulation works and gives us an immediate practical tool for numerical integration. Moreover, we'll get to know the 3 convergence types the theorems are referring to.

In both the lecture and [lab](/sim/3L_CLT.qmd), I use simulation to show graphically how the theorems work. We also get to know the stories and use-cases behind common distributions used in statistical modeling and the relationships between them.^[You will also encounter the empirical cumulative density function, which will be a very important tool in nonparametric approaches to statistics] The accept-reject method will serve us as an useful tool for sampling from complex distributions (including mixtures) not available out-of-the-box in R or Python.

I think that ["The most dangerous equation"](http://assets.press.princeton.edu/chapters/s8863.pdf) is a must read for anyone, not just practicing scientists and statisticians. The example I use for simulation is about the dubious U.S. policy in 90's of splitting big schools into smaller ones. The key theoretical idea we will encounter for the rest of our journey is sampling distribution of sample mean (or any other estimator) -- and we will always have to worry about sample size, especially when comparing groups.


::: {.callout-note}
## Calling Bullshit: Best Barbecue

I highly recommend the case-study from Bergstrom and West about the [best barbecue in US](https://www.callingbullshit.org/case_studies/case_study_barbecue.html), which is a great exercise in critical and statistical thinking. This problem is prevalent in online platforms which have to rank products, posts, and comments. Moreover, they face the challenge of how to take the sample size into account. It depends on their objectives, but for inspiration, I recommend you check out the HackerNews [ranking algorithm](https://vigneshwarar.substack.com/p/hackernews-ranking-algorithm-how).
:::

Out of this lecture and lab, there are two directions we have to investigate further: [probability distributions]{.underline} and properties of estimators. The latter is a topic in mathematical statistics which is relevant even for machine learning.


### Newsvendor problem. Distribution stories

You probably studied the properties of a whole range of probability distributions, but then, in statistics, encountered just a few -- especially in the context of hypothesis testing. In the previous lecture we saw the motivation behind $\chi^2_k$,  $t_k$, $N(\mu, \sigma)$, $F(d_1, d_2)$ and the relationships between them.^[There are particular physical processes and phenomena (stories, in general) which underly the patterns we observe. This is the reason they can be accurately described by particular probability distributions]

We also did simulations for Beta, Gamma, Exponential, Negative Binomial and saw how mixtures are helpful in modeling various business outcomes, like customer purchasing patterns.^[For more details, look at more [examples with simulations](https://mathstat.slu.edu/~speegle/_book/probchapter.html#simulationsprob) and [stories](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view) / applications] These distributions will be our building blocks when developing more complicated and realistic Bayesian models.


::: {.callout-tip}
## "I haven't used Poisson outside that probability class"

If you empathise with this statement, you're either aware that it's important and wonder why it didn't come up in practice or believe it was a tedious academic exercise. The point is not about Poisson distribution, but about probability and statistics theory in general. First, let me assure you that it is helpful in practical applications.

Poisson distribution and process can be a good choice to model counts of **events per unit of time**, space, with a large number of "trials", each with a small probability of success.

$$
P(X=k) = \frac{e^{−\lambda} \lambda^k}{k!}; \space k=0, 1, ...
$$

- Arrivals per hour: requests in a call center, arrivals at a restaurant, website visits. We can use it for capacity planning.
- Bankrupcies filed per month, mechanical piece failures per week, engine shutdowns, work-related accidents. We can use these insights to assess risk and improve safety.
- Forecasting slow-moving items in a retail store, e.g. for clothing
- A famous example is of L. Bortkiewicz: in Prussian army there were 0.70 deaths per one corps per one year caused by horse kicks. *("Law of small numbers")*. Here is the [historical data](https://rpubs.com/SmilodonCub/567089) and a [blog post](https://towardsdatascience.com/poisson-distribution-from-horse-kick-history-data-to-modern-analytic-5eb49e60fb5f) telling the story.
- Number of asthma or kindey cancer related deaths per US county (examples from Gelman's Bayesian Data Analysis, 3rd ed., recommended in next appendix)

Just before you get all excited about these applications, keep in mind that every distribution also has a set of assumptions that have to be met.
:::

Next, we're going to simulate the newsvendor problem which I outlined in the [introduction](/sim/1_intro.qmd) and use different probability distributions to represent different types of demand patterns which we might encounter in practice (slow moving, fast moving, intermittent).^[Demand forecasting and inventory optimization is a large field, and we'll need to come back to it with more powerful statistical models]

<!-- 

## Conditioning and Bayes Rule

There is a quote I like a lot: "Conditioning is the soul of statistics". The Bayes rule, which follows directly from the axioms of probability, is an essential in decision-making and the most important tool in this course -- both conceptually and technically. Any introduction to the subject will work out:

- A few excellent resources are Chapter 1/2 of [BDA3](http://www.stat.columbia.edu/~gelman/book/), or Chapter 1/2 of [Bayes Rules](https://www.bayesrulesbook.com/chapter-2.html), or Chapter 1/2 of [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). They will teach you about:
    - Conditioning, Marginalization, Priors, and Updating
- If you prefer videos, enjoy the 3Blue1Brown [visual masterpiece](https://www.youtube.com/watch?v=HZGCoVF3YvM) on how to think like a Bayesian or the explanation [here](https://www.coursera.org/learn/statistical-inferences/lecture/R6nV5/bayesian-thinking).
- I introduce the idea of Likelihood, which would serve us in future use-cases. It is another important perspective over statistical modeling to consider

::: {.callout-note}
## Medical testing for rare diseases
**Medical testing** for rare diseases, hypothetical example with code in my [course repository](https://github.com/Bizovi/decision-making/blob/main/playground/02_bayes.ipynb). We use the same idea to reason about how confident are we our code has no bugs.

If you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could've applied the same idea!

:::

::: {.column-margin}
Or maybe you're passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence
:::

For the simplest models, one approach of comparing different hypotheses is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look it up in the following courses [here](https://www.coursera.org/learn/statistical-inferences/supplement/IPkZK/assignment-2-2-bayesian-statistics) and [here](https://www.coursera.org/learn/bayesian/home/week/3) for the theory and examples.

::: {.callout-note}
## Football spreads and betting experts

(BDA3, Ch1): **Football spreads**, that can be estimated from [data about matches](http://www.stat.columbia.edu/~gelman/book/data/football.asc). What is the probability that a team wins? Are experts right, on average?

- If you're into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?
- For brevity, I won't elaborate much from now on, how to take an use-case and example to its limit. **If you're passionate about a particular topic -- go for it!**

:::

::: {.callout-note}
## Spelling Correction

(BDA3, Ch1): **Spelling correction**, based on [empirical frequencies](http://norvig.com/ngrams/) provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself -- it is good for a warm-up, but challenging enough to keep you occupied.

:::



::: {.callout-note}
## Monty Hall. Simpson's Paradox and DAGs

The Simpson's paradox is usually introduced to highlight the importance of conditioning. However, the only resource I found which gets to the core of the problem is Bradley Neal's [first lecture](https://www.bradyneal.com/causal-inference-course) on causal inference. 

The "paradox" part of it is resolved (or at least not puzzling), when we think about the causal structure of the problem (or the DAG of influences).
:::



::: {.column-margin}
This is completely optional, but I highly recommend the following provocative talk by 
Richard McElreath, "Bayesian inference is just counting" ([slides](https://speakerdeck.com/rmcelreath/bayesian-inference-is-just-counting) and [talk](https://www.youtube.com/watch?v=_NEMHM1wDfI)). The understanding you'll gain from it will make learning Bayesian statistics much easier.
:::


 -->

## Mathematical statistics

### Properties of estimators

We will worry a lot about causal and methodological aspects when we'll learn statistical modeling, but everything else being equal, there are some estimators which make better use of our data than others (if assumptions hold).

For an accessible explanation of bias, consistency, efficiency, showcased with the corresponding R code, see [Chapter 6](https://openforecast.org/sba/estimatesProperties.html) of I. Svetunkov's "Statistics for Business Analytics". An important idea in the likelihood approach to estimation is Fisher's information, which is presented clearly in Chapter 4 of B. Efron's ["Computer age statistical inference"](https://hastie.su.domains/CASI_files/PDF/casi.pdf), along with an important result of Rao-Cramer lower bound.  

In modern statistics, we often have to deal with highly multidimensional parameter spaces, where unbiasedness is no longer the most important criteria for a good estimation. Therefore, knowing about the curse of dimensionality and the bias-variance tradeoff is tremendously important in applied statistics. For this topic, I recommend [Lecture 8](https://work.caltech.edu/lectures.html) of Y. Abu-Mostafa's "Learning from Data" course.

There are objections in the context of Deep Learning to the traditional bias-variance decomposition. See how this tradeoff [needs an update](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update) for the modern deep learning. However, in the most general sense, it is a universal problem not only in statistics, but also for human cognition.^[When it comes to notions of rationality, it's useful to think about the tradeoff between efficiency and resilience]  


### Bootstrap and Nonparametric Statistics

Often, we can't affort to make strict distributional assumptions about our data-generating process, functional relationship, or population model. This means that we sometimes resort to more flexible or robust models, but we're out of luck with having theoretical, closed-form sampling distributions. Thus, we need computational methods for calculating the confidence intervals.

Bootstrap was a revolutionary technique at the time, which enabled people to use estimators which were too hard or impossible to use before. [Chapter 12](https://openintro-ims.netlify.app/foundations-bootstrapping.html) of "Introduction to Modern Statistics, 2nd ed." is a good introduction to it.


<!-- 

## Hypothesis testing. Neyman-Pearson

In order to make sense of frequentist hypothesis testing, I strongly recommend you read about the original idea of Neyman and Pearson (error control -- don't make a fool of yourself too often in the long run). It is a "path of action" perspective of statistics.

[This is certainly the most difficult lecture of the module, combining the math, programming, and even philosophy.]{.aside}

I start from the first principles and will let go of mechanical application of procedures and conventions (p-values, $\alpha, \beta$, test choice). You should to be able to justify all the choices you make during the phase of experiment design. 


- Picking a default action. [Type I, II errors](https://openintro-ims.netlify.app/decerr.html). How costly is each type of mistakes?
- Minimal **effect size** of interest, [Cohen's](https://rpsychologist.com/cohend/) $d$
- [Power Analysis](https://rpsychologist.com/d3/nhst/) and Sample Size justification. How surprising are significant findings under each hypothesis? Positive Predictive Value
- p-values [simulation](https://rpsychologist.com/pvalue/), [p-curve](https://rpsychologist.com/d3/pdist/) under $H_0, H_A$. 
- Confidence Intervals - first check out this [simulation](https://rpsychologist.com/d3/ci/).  The tricky idea of "capture percent"


::: {.column-margin}
In order to put everything together, there are four resources I can recommend:

- Speegle's [book](https://mathstat.slu.edu/~speegle/_book/HTCI.html) on data+probability+R
- Huber's [Chapter 6](https://www.huber.embl.de/msmb/06-chap.html) of Modern Statistics 
- [Statistical thinking for 21st century](https://statsthinking21.github.io/statsthinking21-core-site/ci-effect-size-power.html)
- [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/)
:::

::: {.callout-note}
## Asking better questions

The most complicated part of hypothesis testing is asking better questions. I mean that in a highly technical sense, and whole-heartedly recommend you the following course from a TU Eindhoven professor, named ["Improving your statistical questions"](https://www.coursera.org/learn/improving-statistical-questions/).

- Make riskier predictions: [Non-Inferiority testing](https://rpsychologist.com/d3/equivalence/), Equivalence Testing, Range predictions
- Publication bias, open science, pre-registrations
- Minimal Effect Size of interest: telescope method and resource-based
- Type 3 errors (solving the wrong problem)
- Read Werner Stahel's ["Relevance"](https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf) paper and Gelman's "Sign and Magnitude" [paper](https://stat.columbia.edu/~gelman/research/published/retropower_final.pdf)

:::


::: {.column-margin}

I think that the best way to start understanding inferential statistics is Daniel Lakens' book on "Improving your statistical inferences", combined with another resource like Speegle's "Probability, Statistics, and Data" for a deep-dive on technicalities.

:::


::: {.callout-note}
## A detour on the philosophy of science

- Understanding the philosophy of falsification and how it applies to hypothesis testing. [Week2 of this course](https://www.coursera.org/learn/improving-statistical-questions/lecture/j6Duu/lecture-2-1-falsifying-predictions-in-theory) has a great 20 minute explanation.
- Philosophy of science: Popper and Latakos, in this [lecture](https://www.youtube.com/watch?v=cgvKG_3Ck7Y). "The null is always false"
:::


::: {.callout-tip}
## Common statistical tests are linear models

There is a zoo of different statistical tests and procedures, which might be very confusing -- especially trying to remember their particularities. It's important to realize that a lot of seemingly unrelated statistical tests in frequentist statistics are particular versions of linear models.

- [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/#1_the_simplicity_underlying_common_tests) and the [python port](https://www.georgeho.org/tests-as-linear/) 
- Choosing a statistical test: difference in proportions and means, test of $\sigma$, correlations
- For a bayesian alternative to t-tests, see [Krutsche's example](https://www.pymc.io/projects/examples/en/latest/case_studies/BEST.html)
:::


## Frequentism vs Likelihood vs Bayes

There are three main schools of thought in statistics, which have their [respective metaphors](https://www.coursera.org/learn/statistical-inferences/lecture/qC3A1/frequentism-likelihoods-bayesian-statistics): "path of action" (Neyman-Pearson frequentism), "path of devotion" (Fisherian Likelihood), and "path of belief / knowledge" (Bayesian). I like very much the presentation of each school of thought in the book of Hastie/Efron "Computer Age Statistical Inference", [chapter 2, 3, 4](https://hastie.su.domains/CASI_files/PDF/casi.pdf). 

Each one has their strenghts, weaknesses, and contribute tools & insights for our future use-cases. When we got into the topic of A/B testing and experiment design, we unavoidably stumbled upon a few fascinating philosophical questions in relation to the nature of evidence. The philosophical debate is fierce, but in statistical practice, less so. I suggest a level of pragmatism to pick the right tool/perspective for the particular job. In the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It's an useful skill when critically reading the literature.


::: {.column-margin}
By now, you encountered the Neyman-Pearson (frequentist) approach. If you want another presentation, watch this [lecture](https://www.youtube.com/watch?v=LYcu3LoGqKc) by Zoltan Dienes to get a sense of the orthodox approach: its power and limitations.
:::

The likelihood approach is widely used in Machine Learning / Statistical Learning teaching and practice. This [lecture](https://www.youtube.com/watch?v=NHFfJEvzPIo) by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations. 



::: {.callout-note}
## Three approaches to single-parameter models

We can pick a simple example of inferring a proportion, which has many practical applications that you might remember from "Distribution Stories". We care not just about the estimation, but also about confidence/credible intervals and the practical workflow.

- Frequentist: Normal Approximation, Agresti-Coull intervals
- Likelihood: Maximum likelihood, point estimates, bootstrapping. Check out [this interactive visualization](https://rpsychologist.com/likelihood/) an [lecture / lab](https://www.coursera.org/learn/statistical-inferences/lecture/8yZDk/likelihoods). 
- Bayes: The full posterior distribution, the tricky business of prior choice

:::


## Dead Salmon Experiment. Replication Crisis 

Lastly, we can't avoid a conversation about the replication crisis happening in multiple disciplines, but especially in social sciences. What scientific literature can we trust? This is relevant not just for research and science, but will help you avoid many pitfalls in the business practice -- therefore, you will be less likely to be fooled by randomness.

- Multiple testing, p-hacking, HARKING, snooping. Ethics and Integrity
- Underpowered studies and vague questions
- Publication Bias, Open Science, Pre-registration and simulation
- False-discovery rate, Bonferoni correction
- Confounding, Mediation and all that causal jazz
- Computational Reproducibility vs Replication. Meta-Analysis


::: {.callout-note}
## Dead salmon experiment

An examination of a famous experiment in neuroscience, putting into question standard/current statistical practices, leads to a conversation of controversies in medicine, psychology, and social science. 

Just think about how important this experiment was for the field of medicine -- it won the [nobel prize](https://blogs.scientificamerican.com/scicurious-brain/ignobel-prize-in-neuroscience-the-dead-salmon-study/)!
:::





::: {.column-page-inset-right}

|   | Lecture Agenda  | Key Ideas  | Case Studies / Activities |
|---|------------------|----------------------------------|---------------------------|
| 1* | Combinatorics. Sampling & Urn Models | Sampling with and without replacement, multiplication rule, naive $\mathbb{P}$, probability trees | Birthday Paradox simulation, Why bootstrap works |
| 2 | Probability Triple. Random Variables. What is a model? | collectivity, population, sample; estimand, estimator, estimation; $(\Omega, \mathcal{F}, \mathbb{P})$, $X$ r.v., $H(y | F(u), \phi(y, u))$| Probability vs Statistics. $\mathcal{H}$, Process models, Statistical models, Inference |
| 3 | **Stories behind distributions** | Binomial, Poisson, Mixtures, $N(\mu, \sigma)$, $\chi^2_k$, $t_k$, F, Beta, Gamma, Exponential, NBin, HGeom | couples showing up to safari, arrival times, basketball shots, hot hand |
| 4* | Conditioning and Bayes Rule. Likelihood Ratios | Updating prior beliefs, Bayesian vs frequentist interpretation of probability. Graphical models | Monty Hall, Simpsons' Paradox, Football Spreads, Medical testing |
| 5* | **CLT** and LLNs. Convergence types. Estimator properties | Never forget about sample size. Why simulation works? Limitation: rare events, fat tails, mixtures, non-iid | The most dangerous equation: US schools, gold coins. Simulation |
| 6* | Fisher Information. **Bias-Variance** tradeoff | Efficiency, bias, James-Stein paradox and the curse of dimensionality. Rao-Cramer. Likelihood approach | Overfitting, underfitting, and mis-specification. Bootstrap, simulations |
| 7* | Neyman-Pearson frequentism. Long-run action | Type I, II errors, confint, p-value, PPV, effect sizes, **power analysis, range predictions**, $H_0$, $H_A$ | How to ask better questions. Stahel's relevance, Interval $\mathcal{H}$ testing. Default action  |
| 8 | Frequentist vs Likelihood vs Bayesian inference | Popperian philosophical roots, practical agreements, strengths and weaknesses | Discussion on practical interpretation: path of action vs devotion vs belief  |
| 9 | Dead Salmon Experiment. Replication Crisis | multiple testing, harking, snooping, publication bias, open science, underpowered studies | Examination of controversies in medicine, psychology, and social science  |

:::

<br>

 -->
