[
  {
    "objectID": "01_fundamentals/stat_foundations.html",
    "href": "01_fundamentals/stat_foundations.html",
    "title": "Foundations of Probability",
    "section": "",
    "text": "In order to be a successful Data Scientist, one has to speak the language of probability and statistics. It is the foundation on which we can build towards more realistic and avanced models, with the purpose of improving decision-making under uncertainty. This foundation is a prerequisite for all three perspectives: analytics/mining, machine learning and causal inference.\n\nIn order to build adequate models of economic and other complex phenomena, we have to take into account their inherent stochastic nature.\n\nData is just the appearance, an external manifestation of some latent processes (seen as random mechanisms). Even though we won’t know the exact outcome for sure, we can model general regularities and relationships as a result of the large scale of phenomena.\nThink of measurements taken from the Large Hadron Collider experiments: the volume, noise, and complexity of data. Ultimately, we want to make inferences about the underlying, physical processes which govern the behavior and interaction of particles. How do we know that inference is accurate and true? A necessary, but not sufficient condition is that the conditions of that experiment are repeatable – such that “on average”, a pattern emerges.\n\n\n\n\n\n\nA word of encouragement\n\n\n\nReviewing the fundamentals of statistics doesn’t have to be boring! We can put “the classics” in context of modern statistics, big data challenges, and use simulation instead of heavy mathematics and proof-based approaches.\nMoreover, theoretical ideas underlying statistical practice, which we often take for granted, deserve an explicit articulation. This will improve our awareness, understanding, and grasp of the field – such that we can become more effective practitioners.\n\n\n\n\n\n\n\n\nflowchart TD\n    Motiv[Prob vs Stats] --&gt; C[Combinatorics] --&gt; BD[Birthday Paradox] --&gt; SP[Simpson's Paradox] --&gt; Bayes[Bayes Rule] --&gt; LL[Likelihood]\n    \n    Bayes --&gt; Ind[Independence] --&gt; Ex[Exchangeability]\n    Motiv --&gt; ProbT[Probability Triple] --&gt; SI[Uncertainty] --&gt; RV[Random Variables] --&gt; PDF[Stories Distribution] --&gt; DAG[DAGs] --&gt; LLN[LLNs]\n\n    Motiv --&gt; MS[Mathematical Statistics] --&gt; GP[Prague Golemn] --&gt; Estim[Estimators] --&gt; Prop[Desired Properties] --&gt; US[CLT & US Schools] --&gt; NP[Neyman-Pearson]\n\n\n\n\n\nFigure 1: In this chapter and set of lectures, I attempt to review fundamental statistical ideas, concepts, and tools, in order to show their relevance in the day-to-day practice and decision-making. A secondary objective is to fill in the conceptual gaps left by the fact that it’s hard to make sense of it all the first time we’re encountering it in the classroom.\n\n\n\n\n\nThe plan for this lecture is the following: we start by constructing the probability triple, formalize sources of uncertainty, and investigate the building blocks of a statistical model. I then highlight in a few case-studies what can go wrong in the process of statistical inference and how difficult it is to choose the right model.\n\n\n\n\n\n\nRabbit hole warning\n\n\n\nHow far you go back into the abstract foundations? For practical intents and purposes, you won’t need measure theory and proof-based mathematical analysis.\nI find those interesting for their own sake and understanding the foundations of higher-level tools we use. However, I can’t argue it’s an efficient use of your time.\n\n\nThe most technical, heavy, and mathematical part is about estimators and their properties, but it is necessary both for hypothesis testing and machine learning. To make it more accessible and intuitive, we will use simulation and visualization during the labs to get an understanding and intuition on how estimators behave. We wrap up the lecture by looking at the statistical process in firms and put it in contrast with the process for ML, predictive systems. Last, but not least, there is one more cautionary tale about multiple testing – which will be our gateway into truly modern statistics.\n\n\nIn the previous lecture, I mentioned why did we study probability theory. However, there is one more useful metaphor: remember how important is logic in mathematics, computer science, and philosophy; it’s one of the prerequisites in each of those, an essential tool for reasoning. Then, probability theory is the logic of uncertainty, a formal language. 1\n1 If you studied fuzzy set theory, you might have a case for it being the candidate, however, it fell out of favor in practice – so I would suggest to focus on probability and Bayesian reasoning.Often, probability and mathematical statistics are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Probability theory is concerned with the construction of probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\).\nThat is the foundation for developing useful tools like random variables and probability/cumulative density functions. Then, extending those to joint, conditional probabilities and multidimensional cases, introducing the machinery to operate all of that: like expectation, variance, moment-generating functions, conditioning, marginalization, Bayes theorem. This leads to the laws of large numbers, properties of distributions and their transformations.\nWe use all of those results to model relationships between observed and latent variables in business processes and phenomena. It’s a building block for answering the “simple” question: does X cause Y?\n\n\n\n\n\n\n“I haven’t used Poisson outside that probability class”\n\n\n\nIf you empathise with this statement, you’re probably aware that it’s important, but wonder if it didn’t come up in practice – what about the rest of the machinery I described? My answer is that it’s so, so ubiquitous, but we need to learn to “see” the opportunities to use this set of tools in decision-making.\nPoisson distribution and process can be a good choice to model counts of events per unit of time, space, with a large number of “trials”, each with a small probability of success.\n\\[\nP(X=k) = \\frac{e^{−\\lambda} \\lambda^k}{k!}; \\space k=0, 1, ...\n\\]\n\nArrivals per hour: requests in a call center, arrivals at a restaurant, website visits. We can use it for capacity planning.\nBankrupcies filed per month, mechanical piece failures per week, engine shutdowns, work-related accidents. We can use these insights to assess risk and improve safety.\nForecasting slow-moving items in a retail store, e.g. for clothing. We’ll investigate the newsvendor problem in excruciating detail, where we have to purchase the inventories ahead of time.\nA famous example is of L. Bortkiewicz: in Prussian army there were 0.70 deaths per one corps per one year caused by horse kicks. (“Law of small numbers”).\n\nJust before you get all excited about these applications, keep in mind that every distribution has a story behind it, and a set of assumptions that have to be met.\n\n\nOther tools are as prevalent and useful: Bayes rule, DAGs (directed acyclic graphs of random variables), the exponential family, laws of large numbers and the cental limit theorem. It holds both in applications and statistics itself.\nIn probability theory, we’re still in a mathematical world trying to capture the essence of the real world, but ultimately, we need statistical inference to estimate those parameters from data. Before we get into it in more detail: mathematically, computationally and in terms of business cases, we have to define that foundation – the probability triple.\nTo make the following more clear, let’s start with an experiment where we show a customer ten clothing items (a collection), and they have to pick the one they like best. If we repeated it with many customers, preferably representative for the population of interest,2 a pattern would emerge. This is an example of a discrete Outcome Set (or universal set, of all possible results). Alternatively, think of pebbles in an urn, where each one represents an outcome.\n2 You might intuitively know what a population is, but there are surprisingly many pitfalls, so we’ll investigate that notion with lots of care.As an academic aside, we have to thank Kolmogorov for putting probability into a rigorous, axiomatic framework based on set theory and making it open for mathematical enquiry with tools from mathematical analysis, which by that time were well-established. That is important, because we can’t work the same naive way with continuous measurements and phenomena, as we do with pebbles.\n\nA random experiment (\\(\\mathscr{E}\\)) is a set of conditions which are favorable for an event in a given form. It is that real-world process of interest which we try to simplify, with the following properties:\n\nPossible results and outcomes are known apriori and exhaustably. For example: a coin/dice toss, quantity sold, time in a subscription, a default on the credit, a choice between subscriptions.\nIt’s never known which of the results of \\(\\mathscr{E}\\) will manifest or appear before we run the experiment, the “experiment” amounting to randomly picking that clothing item or a pebble from the urn.\nDespite that, there is a perceptible regularity, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic “law” in the results. That regularity could be a result of the large scale of the phenomena, for example, many customers seeing a product on the shelf.\nRepeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we’re not thinking in long-run frequency terms. 3\n\nElementary event as an auxiliaty construction: one of the possible results of \\(\\mathscr{E}\\), usually denoted by \\(\\omega_i \\in \\Omega\\).\nUniversal set \\(\\Omega = \\{ \\omega_1, \\omega_2, \\dots \\}\\) is also called (Outcome/ State/ Selection space). It suggests the idea of complementarity and stochasticity: we don’t know which \\(\\omega_i\\) will manifest, thus is a key object for a further formalization of probability measures.\nWe care not only about an event \\(A = \\bigcup\\limits_{i = 1}^n \\omega_i\\) (which is an union of elementary events) and its realization, but also about other events in the Universal Set, because they might contribute with additional information (about the probability) of our event of interest – remember conditioning? This means that we we’re interested in “all” other events.\nThe event space \\(\\mathcal{F}\\) is a sigma-algebra, should be defined on sets of subsets of \\(\\Omega\\) and this is where measure theory shines. For technical reasons, we usually can’t define a probability measure on all sets of subsets. On an intuitive note, we define the probability measure on sigma-algebras because if those conditions did not hold, the measure wouldn’t make sense, unions of events would step out of the bounds of event space.\nProbability as an extension of the measure: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability (number between 0 and 1) to elementary events: \\(\\mathbb{P}(\\omega_i)\\). This is why additivity properties are key, as we care about random events, not only elementary events.\n\n3 \n\n\nSource: Ross This is the view of probability as a long-run frequency of events occuring, for example, flipping a coin\n\n\n\nDef: Algebra and Sigma-Algebra\nA set of subsets \\(\\mathcal{F} \\subset 2^\\Omega\\) is an algebra (field) if the following holds:\n\n\\(\\Omega \\in \\mathcal{F}\\) and \\(\\varnothing \\in \\mathcal{F}\\)\nIf \\(A \\in \\mathcal{F}\\) then \\(A^C \\in \\mathcal{F}\\) (closed under complements)\nIf \\(A, B \\in \\mathcal{F}\\) then \\(A \\cup B \\in \\mathcal{F}\\) (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection\nThe additional condition for sigma-algebra: sigma refers to countability. If \\(\\{  A_i \\}_{i \\ge 1} \\in \\mathcal{F}\\) then \\(\\bigcup\\limits_{i \\ge 1} A_i \\in \\mathcal{F}\\) (closed under countable union)\n\n\n\n\nSource: Wikimedia A beautiful visual representation\n\n\n\n\n\n\n\n\n\nProbability Measure\n\n\n\nSuppose we have defined a measurable space \\((\\Omega, \\mathcal{F})\\), where \\(\\mathcal{F}\\) is a sigma-algebra. A probability measure is the function \\(\\mathbb{P}:\\mathcal{F} \\rightarrow [0, 1]\\) such that:\n\n\\(\\mathbb{P}(\\Omega) = 1\\) \nFor countable sequences of mutually disjoint effects, i.e. \\(\\forall \\{ A_i \\}_{i \\ge 1}\\) where \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing\\), the following holds \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\)\n\n\n\nA probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is the fundamental object the whole probability theory is constructed upon. Again, Kolmogorov took the informal, gambling-type probability and put it onto axiomatic foundations – which enabled future breakthroughs. Notice that this definition of probability is not the naive one, of number of successes over the total possible numbers an event could arise.\n\n\n\n\n\n\nMeasure theory rabbit hole: Why not all subsets?\n\n\n\nThe reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you’re interested in these technical details, you can check out my relatively accessible introudction to measure theory and the Caratheodori extension theorem.\n\nEven though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its measure-theoretic foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.\nEvans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If \\(\\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\\) is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.\n\\[\\begin{equation}\n    p(x) = w_1 \\chi_{[0,1]}(x) +  w_2\\delta_1(x)\n\\end{equation}\\]\nEven though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.\n\n\n\nJeffrey Rosenthal begins his book by a similar motivation, constructing the following random variable as a coin toss between a discrete \\(X \\sim Pois(\\lambda)\\) and continuous \\(Y \\sim \\mathcal{N}(0,1)\\) r.v.\n\\[\\begin{equation}\n    Z = \\begin{cases}\n    X, p = 0.5 \\\\\n    Y, p = 0.5\n    \\end{cases}\n\\end{equation}\\]\nHe then challenges the readers to come up with the expected value \\(\\mathbb{E}[Z^2]\\) and asks on what is it defined? It is indeed a hard question.\nWe stumble upon different interpretations of probability (frequentists vs bayesians), when trying to clarify that “perceptible regularity”, despite the mathematics of probability theory being exactly the same. These are not “theorems” to prove, but rather axioms and philosophies often taken in practice as the starting point.\nThe frequency theory defines the probability of an outcome as the limiting frequency with which that outcome appears in a long series of similar events. If our experiment or investigation is such that those relative frequencies converge, then we can prove the LLN and the CLT. Basically, that we should view probability and predictions based on historical data. 4\n4 “To a frequentist, the probability of an event is intrinsic to that event’s nature and not something perceived, only discovered.”Formally, we can represent this statement by Bernoulli’s theorem, which is a special case of Law of Large Numbers, where \\(m_n\\) is the number of times an event \\(A\\) occurs in \\(n\\) trials.\n\\[\n\\lim_{n \\to \\infty} \\frac{m_n}{n} = p\n\\]\n\n\n\n\n\n\nFrequency Convergence as Statistical Stability\n\n\n\nSince we don’t have an infinite number of trials, the best we can do is to say that “experimental evidence strongly suggests statistical stability”. This applies really well in gambling (by design) and physics, but is it plausible in human behavior? I don’t think so, but it might be unreasonably effective, even if not true.\n\n\n\n\nSo, which one do you recommend? It depends, learn both! But I would agree with Richard McElreath’s meme: it’s the scientific process that has the last laugh\n\nBayesians view probability as the degree of confidence in a belief, an apriori guess or knowledge, that is, before seeing the data. Then, using inverse probability, you update your beliefs in the face of evidence and data. Often, you do have knowledge and can define your prior probabilities by a process of ellicitation – it should’t be too vague, neither dogmatic. Other Bayesians put in great effort to pick non-informative priors, as the data should quickly overwhelm it.\nTODO: Ellicitation of probability -&gt; we’ll do more later with inference\n\n\nThank you for bearing with me through the theory you have probably seen before, but we’re not done. We’re still in the land of set theory, and it is very hard to operate that way in practice – so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.\n\n\n\n\n\n\nThe breakthrough idea of a Random Variable\n\n\n\nWe started from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantifiable characteristics of the objects. Meaning, we start working with numbers instead of some qualitative properties. Now, we’re in business!\n\n\n\n\n\n\n\n\nRandom Variable is not a variable, nor random\n\n\n\nA random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can’t be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of measurability, meaning we can assign a meaningful “volume”.\n\\[\\begin{align}\nX(\\omega):\\Omega \\rightarrow \\mathbb{R} \\\\\ns.t. ~~ \\{\\omega \\in \\Omega | X(\\omega) \\leq r, \\forall r \\in \\mathbb{R} \\} \\in \\mathcal{F}\n\\end{align}\\]\n\n\nLet’s figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest.\nThe idea is that the preimage defined above \\(X^{-1}((-\\infty,r]) = E \\in \\mathcal{F}\\) on the following interval corresponds to an event E which should be in the event space \\(\\mathcal{F}\\). Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is \\(F_X(X \\le r)\\).\n\n\nCDF, one of the most important constructs in probability is a direct consequence of the definition of the random variable:\n\\[\nP(A ≤ r) = F_X(X ≤ r)\n\\]\nIn the practice of modeling, we often work with probability density functions, because it is more convenient in many cases. Then, in order to translate to probabilities, we would think in terms of areas under that curve. For sure, you remember the following duality between CFD and PDF:\n\\[\nF'(x) = p(x)\n\\]\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some relevant aspect of it to us)!\n\n\n\nAs a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of compositional data analysis5 and time series analysis. What I want to say, is that for more “exotic” applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.\n5 \n\n\nSource: Dumuid: Data in a Simplex, which is later translated to \\(R^n\\) by a carefully constructed basis expansion\n\n\n\n\n\n\n\n\nThe curious case of Compositional Data Analysis\n\n\n\nSometimes, the data doesn’t “live” in our normal, intuitive, euclidian space \\(\\mathbb{R}^n\\). There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.\nWe don’t necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods we have make sense for compositions.\nCompositional data analysis solves those issues by defining a probability triple over the simplex (instead of \\(\\mathbb{R}^n\\)): \\((\\mathcal{S}^n, \\mathcal{F}, \\mathbb{P})\\). This leads to a different definition of the event space \\(\\mathcal{F}\\), which is also a sigma-algebra and a different definition of the probability measure \\(\\mathbb{P}\\).\n\n\nRemember our exaple of pigeon superstition in the context of learning? It is not surprising to me that measure theory becomes important in Learning Theory,6 which is exactly those carefully formulated principles that will prevent our automated learners to become supersitious.\n6 Even though most courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr), according to Mikio’s Brown answer it’s essential in the idea of uniform convergence and its bounds, where “you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”.For the next example, you don’t have to understand what Gaussian Processes are or are used for. However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results, in contrast with choosing a wrong model or distribution.\nIt’s not that these methods don’t have parameters, but the parametrization varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don’t know what is a reasonable distribution or parametric functional form for the relationship that we model.\n\n\n\n\n\n\nNonparametrics and Gaussian Processes\n\n\n\n\nIf we’re thinking about a regression from the nonparametric perspective: that is, over a set of abstract functions: \\(f(x) \\in \\mathscr{C}^2:X \\rightarrow \\mathbb{R}\\), 7 we might want to know how a draw of samples from an infinite set of continuous differentiable functions might look like.\n\\[\nf(x) \\sim GP(\\mu(x); K(x,x'))\n\\]\nThe questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.\n\\[\np(f(x) \\, |\\left \\{ x\\right \\})=\\frac{p(\\left \\{ x\\right \\}| \\, f) \\, \\mathbf{p(f)}}{p(\\left \\{ x\\right \\})}\n\\]\n\n\n7 \n\n\nSource: Bizovi: A posterior distribution of the Gaussian Processes, when conditioned on data\n\n\n\nIn the case of stochastic processes, we work with a sequence of random variables \\(\\{X_t, t \\in T \\}\\) and start asking questions:\n\nWhat kind of time dependency is there? (autocorrelation)\nWhat is the long-run behavior?\nCan we say something about extreme events?\n\n\nA lot of important applications in economics and finance are dynamic, so we have to work with time series very often. It gets worse when data is correlated not only in time, but also geographically – which is why the field of spatio-temporal data analysis is in such demand right now for policy-making.\nThus, a natural extension of this probability machinery we discussed so far is stochastic processes, underlying these dynamical systems. We can look at our time series as a manifestation, a particular instantiation of this latent process. Depending on which one we choose, we can model a wide range of phenomena.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics",
      "1. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "href": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "title": "Foundations of Probability",
    "section": "",
    "text": "In the previous lecture, I mentioned why did we study probability theory. However, there is one more useful metaphor: remember how important is logic in mathematics, computer science, and philosophy; it’s one of the prerequisites in each of those, an essential tool for reasoning. Then, probability theory is the logic of uncertainty, a formal language. 1\n1 If you studied fuzzy set theory, you might have a case for it being the candidate, however, it fell out of favor in practice – so I would suggest to focus on probability and Bayesian reasoning.Often, probability and mathematical statistics are bundled together, as they make perfect sense in sequence, but they have different objectives in mind. Probability theory is concerned with the construction of probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\).\nThat is the foundation for developing useful tools like random variables and probability/cumulative density functions. Then, extending those to joint, conditional probabilities and multidimensional cases, introducing the machinery to operate all of that: like expectation, variance, moment-generating functions, conditioning, marginalization, Bayes theorem. This leads to the laws of large numbers, properties of distributions and their transformations.\nWe use all of those results to model relationships between observed and latent variables in business processes and phenomena. It’s a building block for answering the “simple” question: does X cause Y?\n\n\n\n\n\n\n“I haven’t used Poisson outside that probability class”\n\n\n\nIf you empathise with this statement, you’re probably aware that it’s important, but wonder if it didn’t come up in practice – what about the rest of the machinery I described? My answer is that it’s so, so ubiquitous, but we need to learn to “see” the opportunities to use this set of tools in decision-making.\nPoisson distribution and process can be a good choice to model counts of events per unit of time, space, with a large number of “trials”, each with a small probability of success.\n\\[\nP(X=k) = \\frac{e^{−\\lambda} \\lambda^k}{k!}; \\space k=0, 1, ...\n\\]\n\nArrivals per hour: requests in a call center, arrivals at a restaurant, website visits. We can use it for capacity planning.\nBankrupcies filed per month, mechanical piece failures per week, engine shutdowns, work-related accidents. We can use these insights to assess risk and improve safety.\nForecasting slow-moving items in a retail store, e.g. for clothing. We’ll investigate the newsvendor problem in excruciating detail, where we have to purchase the inventories ahead of time.\nA famous example is of L. Bortkiewicz: in Prussian army there were 0.70 deaths per one corps per one year caused by horse kicks. (“Law of small numbers”).\n\nJust before you get all excited about these applications, keep in mind that every distribution has a story behind it, and a set of assumptions that have to be met.\n\n\nOther tools are as prevalent and useful: Bayes rule, DAGs (directed acyclic graphs of random variables), the exponential family, laws of large numbers and the cental limit theorem. It holds both in applications and statistics itself.\nIn probability theory, we’re still in a mathematical world trying to capture the essence of the real world, but ultimately, we need statistical inference to estimate those parameters from data. Before we get into it in more detail: mathematically, computationally and in terms of business cases, we have to define that foundation – the probability triple.\nTo make the following more clear, let’s start with an experiment where we show a customer ten clothing items (a collection), and they have to pick the one they like best. If we repeated it with many customers, preferably representative for the population of interest,2 a pattern would emerge. This is an example of a discrete Outcome Set (or universal set, of all possible results). Alternatively, think of pebbles in an urn, where each one represents an outcome.\n2 You might intuitively know what a population is, but there are surprisingly many pitfalls, so we’ll investigate that notion with lots of care.As an academic aside, we have to thank Kolmogorov for putting probability into a rigorous, axiomatic framework based on set theory and making it open for mathematical enquiry with tools from mathematical analysis, which by that time were well-established. That is important, because we can’t work the same naive way with continuous measurements and phenomena, as we do with pebbles.\n\nA random experiment (\\(\\mathscr{E}\\)) is a set of conditions which are favorable for an event in a given form. It is that real-world process of interest which we try to simplify, with the following properties:\n\nPossible results and outcomes are known apriori and exhaustably. For example: a coin/dice toss, quantity sold, time in a subscription, a default on the credit, a choice between subscriptions.\nIt’s never known which of the results of \\(\\mathscr{E}\\) will manifest or appear before we run the experiment, the “experiment” amounting to randomly picking that clothing item or a pebble from the urn.\nDespite that, there is a perceptible regularity, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic “law” in the results. That regularity could be a result of the large scale of the phenomena, for example, many customers seeing a product on the shelf.\nRepeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we’re not thinking in long-run frequency terms. 3\n\nElementary event as an auxiliaty construction: one of the possible results of \\(\\mathscr{E}\\), usually denoted by \\(\\omega_i \\in \\Omega\\).\nUniversal set \\(\\Omega = \\{ \\omega_1, \\omega_2, \\dots \\}\\) is also called (Outcome/ State/ Selection space). It suggests the idea of complementarity and stochasticity: we don’t know which \\(\\omega_i\\) will manifest, thus is a key object for a further formalization of probability measures.\nWe care not only about an event \\(A = \\bigcup\\limits_{i = 1}^n \\omega_i\\) (which is an union of elementary events) and its realization, but also about other events in the Universal Set, because they might contribute with additional information (about the probability) of our event of interest – remember conditioning? This means that we we’re interested in “all” other events.\nThe event space \\(\\mathcal{F}\\) is a sigma-algebra, should be defined on sets of subsets of \\(\\Omega\\) and this is where measure theory shines. For technical reasons, we usually can’t define a probability measure on all sets of subsets. On an intuitive note, we define the probability measure on sigma-algebras because if those conditions did not hold, the measure wouldn’t make sense, unions of events would step out of the bounds of event space.\nProbability as an extension of the measure: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability (number between 0 and 1) to elementary events: \\(\\mathbb{P}(\\omega_i)\\). This is why additivity properties are key, as we care about random events, not only elementary events.\n\n3 \n\n\nSource: Ross This is the view of probability as a long-run frequency of events occuring, for example, flipping a coin\n\n\n\nDef: Algebra and Sigma-Algebra\nA set of subsets \\(\\mathcal{F} \\subset 2^\\Omega\\) is an algebra (field) if the following holds:\n\n\\(\\Omega \\in \\mathcal{F}\\) and \\(\\varnothing \\in \\mathcal{F}\\)\nIf \\(A \\in \\mathcal{F}\\) then \\(A^C \\in \\mathcal{F}\\) (closed under complements)\nIf \\(A, B \\in \\mathcal{F}\\) then \\(A \\cup B \\in \\mathcal{F}\\) (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection\nThe additional condition for sigma-algebra: sigma refers to countability. If \\(\\{  A_i \\}_{i \\ge 1} \\in \\mathcal{F}\\) then \\(\\bigcup\\limits_{i \\ge 1} A_i \\in \\mathcal{F}\\) (closed under countable union)\n\n\n\n\nSource: Wikimedia A beautiful visual representation\n\n\n\n\n\n\n\n\n\nProbability Measure\n\n\n\nSuppose we have defined a measurable space \\((\\Omega, \\mathcal{F})\\), where \\(\\mathcal{F}\\) is a sigma-algebra. A probability measure is the function \\(\\mathbb{P}:\\mathcal{F} \\rightarrow [0, 1]\\) such that:\n\n\\(\\mathbb{P}(\\Omega) = 1\\) \nFor countable sequences of mutually disjoint effects, i.e. \\(\\forall \\{ A_i \\}_{i \\ge 1}\\) where \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing\\), the following holds \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\)\n\n\n\nA probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is the fundamental object the whole probability theory is constructed upon. Again, Kolmogorov took the informal, gambling-type probability and put it onto axiomatic foundations – which enabled future breakthroughs. Notice that this definition of probability is not the naive one, of number of successes over the total possible numbers an event could arise.\n\n\n\n\n\n\nMeasure theory rabbit hole: Why not all subsets?\n\n\n\nThe reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you’re interested in these technical details, you can check out my relatively accessible introudction to measure theory and the Caratheodori extension theorem.\n\nEven though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its measure-theoretic foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.\nEvans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If \\(\\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\\) is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.\n\\[\\begin{equation}\n    p(x) = w_1 \\chi_{[0,1]}(x) +  w_2\\delta_1(x)\n\\end{equation}\\]\nEven though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.\n\n\n\nJeffrey Rosenthal begins his book by a similar motivation, constructing the following random variable as a coin toss between a discrete \\(X \\sim Pois(\\lambda)\\) and continuous \\(Y \\sim \\mathcal{N}(0,1)\\) r.v.\n\\[\\begin{equation}\n    Z = \\begin{cases}\n    X, p = 0.5 \\\\\n    Y, p = 0.5\n    \\end{cases}\n\\end{equation}\\]\nHe then challenges the readers to come up with the expected value \\(\\mathbb{E}[Z^2]\\) and asks on what is it defined? It is indeed a hard question.\nWe stumble upon different interpretations of probability (frequentists vs bayesians), when trying to clarify that “perceptible regularity”, despite the mathematics of probability theory being exactly the same. These are not “theorems” to prove, but rather axioms and philosophies often taken in practice as the starting point.\nThe frequency theory defines the probability of an outcome as the limiting frequency with which that outcome appears in a long series of similar events. If our experiment or investigation is such that those relative frequencies converge, then we can prove the LLN and the CLT. Basically, that we should view probability and predictions based on historical data. 4\n4 “To a frequentist, the probability of an event is intrinsic to that event’s nature and not something perceived, only discovered.”Formally, we can represent this statement by Bernoulli’s theorem, which is a special case of Law of Large Numbers, where \\(m_n\\) is the number of times an event \\(A\\) occurs in \\(n\\) trials.\n\\[\n\\lim_{n \\to \\infty} \\frac{m_n}{n} = p\n\\]\n\n\n\n\n\n\nFrequency Convergence as Statistical Stability\n\n\n\nSince we don’t have an infinite number of trials, the best we can do is to say that “experimental evidence strongly suggests statistical stability”. This applies really well in gambling (by design) and physics, but is it plausible in human behavior? I don’t think so, but it might be unreasonably effective, even if not true.\n\n\n\n\nSo, which one do you recommend? It depends, learn both! But I would agree with Richard McElreath’s meme: it’s the scientific process that has the last laugh\n\nBayesians view probability as the degree of confidence in a belief, an apriori guess or knowledge, that is, before seeing the data. Then, using inverse probability, you update your beliefs in the face of evidence and data. Often, you do have knowledge and can define your prior probabilities by a process of ellicitation – it should’t be too vague, neither dogmatic. Other Bayesians put in great effort to pick non-informative priors, as the data should quickly overwhelm it.\nTODO: Ellicitation of probability -&gt; we’ll do more later with inference\n\n\nThank you for bearing with me through the theory you have probably seen before, but we’re not done. We’re still in the land of set theory, and it is very hard to operate that way in practice – so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.\n\n\n\n\n\n\nThe breakthrough idea of a Random Variable\n\n\n\nWe started from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantifiable characteristics of the objects. Meaning, we start working with numbers instead of some qualitative properties. Now, we’re in business!\n\n\n\n\n\n\n\n\nRandom Variable is not a variable, nor random\n\n\n\nA random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can’t be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of measurability, meaning we can assign a meaningful “volume”.\n\\[\\begin{align}\nX(\\omega):\\Omega \\rightarrow \\mathbb{R} \\\\\ns.t. ~~ \\{\\omega \\in \\Omega | X(\\omega) \\leq r, \\forall r \\in \\mathbb{R} \\} \\in \\mathcal{F}\n\\end{align}\\]\n\n\nLet’s figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest.\nThe idea is that the preimage defined above \\(X^{-1}((-\\infty,r]) = E \\in \\mathcal{F}\\) on the following interval corresponds to an event E which should be in the event space \\(\\mathcal{F}\\). Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is \\(F_X(X \\le r)\\).\n\n\nCDF, one of the most important constructs in probability is a direct consequence of the definition of the random variable:\n\\[\nP(A ≤ r) = F_X(X ≤ r)\n\\]\nIn the practice of modeling, we often work with probability density functions, because it is more convenient in many cases. Then, in order to translate to probabilities, we would think in terms of areas under that curve. For sure, you remember the following duality between CFD and PDF:\n\\[\nF'(x) = p(x)\n\\]\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some relevant aspect of it to us)!\n\n\n\nAs a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of compositional data analysis5 and time series analysis. What I want to say, is that for more “exotic” applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.\n5 \n\n\nSource: Dumuid: Data in a Simplex, which is later translated to \\(R^n\\) by a carefully constructed basis expansion\n\n\n\n\n\n\n\n\nThe curious case of Compositional Data Analysis\n\n\n\nSometimes, the data doesn’t “live” in our normal, intuitive, euclidian space \\(\\mathbb{R}^n\\). There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.\nWe don’t necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods we have make sense for compositions.\nCompositional data analysis solves those issues by defining a probability triple over the simplex (instead of \\(\\mathbb{R}^n\\)): \\((\\mathcal{S}^n, \\mathcal{F}, \\mathbb{P})\\). This leads to a different definition of the event space \\(\\mathcal{F}\\), which is also a sigma-algebra and a different definition of the probability measure \\(\\mathbb{P}\\).\n\n\nRemember our exaple of pigeon superstition in the context of learning? It is not surprising to me that measure theory becomes important in Learning Theory,6 which is exactly those carefully formulated principles that will prevent our automated learners to become supersitious.\n6 Even though most courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr), according to Mikio’s Brown answer it’s essential in the idea of uniform convergence and its bounds, where “you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”.For the next example, you don’t have to understand what Gaussian Processes are or are used for. However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results, in contrast with choosing a wrong model or distribution.\nIt’s not that these methods don’t have parameters, but the parametrization varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don’t know what is a reasonable distribution or parametric functional form for the relationship that we model.\n\n\n\n\n\n\nNonparametrics and Gaussian Processes\n\n\n\n\nIf we’re thinking about a regression from the nonparametric perspective: that is, over a set of abstract functions: \\(f(x) \\in \\mathscr{C}^2:X \\rightarrow \\mathbb{R}\\), 7 we might want to know how a draw of samples from an infinite set of continuous differentiable functions might look like.\n\\[\nf(x) \\sim GP(\\mu(x); K(x,x'))\n\\]\nThe questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.\n\\[\np(f(x) \\, |\\left \\{ x\\right \\})=\\frac{p(\\left \\{ x\\right \\}| \\, f) \\, \\mathbf{p(f)}}{p(\\left \\{ x\\right \\})}\n\\]\n\n\n7 \n\n\nSource: Bizovi: A posterior distribution of the Gaussian Processes, when conditioned on data\n\n\n\nIn the case of stochastic processes, we work with a sequence of random variables \\(\\{X_t, t \\in T \\}\\) and start asking questions:\n\nWhat kind of time dependency is there? (autocorrelation)\nWhat is the long-run behavior?\nCan we say something about extreme events?\n\n\nA lot of important applications in economics and finance are dynamic, so we have to work with time series very often. It gets worse when data is correlated not only in time, but also geographically – which is why the field of spatio-temporal data analysis is in such demand right now for policy-making.\nThus, a natural extension of this probability machinery we discussed so far is stochastic processes, underlying these dynamical systems. We can look at our time series as a manifestation, a particular instantiation of this latent process. Depending on which one we choose, we can model a wide range of phenomena.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics",
      "1. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/2_performance_strategy.html",
    "href": "01_fundamentals/2_performance_strategy.html",
    "title": "Business Performance and Strategy",
    "section": "",
    "text": "Arguably, we live in a volatile, uncertain, complex, and ambiguous world that we have to understand in order to navigate it well. By “world”, I mean the economy, society, environment, or any other complex system – especially when human behavior is involved. It is reasonable to ask: wasn’t it always the case, depending on how we define the terms, level of analysis, and point of view?\nI think it’s a matter of scale and magnitude in VUCA dimensions, where an accelerating rate of change poses a great challenge to our capacity to adapt. Instead of an evolutionary or philosophical comment on why we need this field of data/decision science, I have two questions on my mind when it comes to living well or improving business outcomes: what is true and how should I act? 1\n1 This implies that we already framed the problem and figured out what we want to achieve, and that indeed we chose the right goals and objectives. Moreover, the questions of epistemiology and ethics are a never ending topic for discussion and enquiry.\n\n\n\n\n\nDecision Science: (not only) truth and action\n\n\n\nLet’s start from a business setting of an e-commerce, where we want to increase sales, customer satisfaction, and reduce costs. Imagine three scenarios, which neatly fall into the SWOT framework:\n\n\n\nSource: Kim Warren - Status Quo, Desired and Feared Trajectories\n\n\n\nWe keep the status quo, doing everyting as before. What is the most likely trajectory of profits? Can we come up with an educated guess? If the trajectory looks good, that is our strenghts and compentencies contributing to it, if not, our weakness.\nA feared trajectory, that is, if our business is hit by a shock in supply chain, inflation, by competition or customer demand. It’s the threats.\nA desired or aspirational trajectory. Is it reasonable and realistically achievable? If yes, what strategy and tactics should we implement, how sould we act? This is our opportunity.\n\n\n\nAfter this exercise, we defined more precisely where we stand, that is, quantify the current state of the firm. We framed the problem in terms of most relevant outcomes and we’re in the process of figuring out what are the optimal goals to aim for. Obviously, we need a mechanism, measurements to know that we’re on track and to recognize when we get there. Now, let’s go back to our two questions and unpack them:\n\nWhat is true? In the most general sense, we’re not asking for a mathematical and logical truth, but if it’s plausible, probable, deserves serious consideration, is backed by evidence. I also mean that we understand the underlying causal mechanisms. Not least, an assessment of the current situation. The metaphor which I suggest for this is “seeing clearly”, through the fog, illusions, and biases. 2\nHow should I act? What is plausible doesn’t entirely answer this question, we can’t derive an ought from is.3 In business settings, I would think about action in terms of strategic alignment and optimisation.\n\n2 Recommended reading: Scout Mindset by Julia Galef3 This is one of the most important and applicable philosophical ideas, introduced by David Hume: when one makes claims about what ought to be that are based solely on statements about what is.The only missing pieces from this mental model that I argue for is tremendously important: iteration and feedback. Due to VUCA, we can’t be sure our actions are optimal, or even that we’re solving the right problem, therefore fast iteration and feedback ensures we’re not taking too much risk, that we find out early about problems in our thinking and action, that we can change course to steer the ship back on track.\n\n\n\n\n\n\nWait, this sounds familiar\n\n\n\nSounds an awflul lot like Cybernetics, doesn’t it? Especially if we have the idea of a firm as a complex adaptive system as a pressuposition for this discussion.\n\n\n\nNow that it’s more clear what I meant in the course introduction by improving business outcomes and bringing value to organizations, I didn’t yet explain what does analytics, data science, machine learning, and AI do or are, and how to they fit in the picture we painted so far.\n\n\nSounds like a naive question, but bad strategy is prevalent – which comes from a misunderstanding about what is a strategy. I highly recommend this article published by McKinsey – we learn better from other’s mistakes.\nIn short, it is not just aspiration towards goal or having a vision or setting a target. By analogy, remember the SMART criteria when you’re setting goals. As true consultants, we can summarize the steps involved in developing a strategy in a matrix.\n\n\n\n\n\n\n\n\nStep\nOutcome\nCharacteristics\n\n\n\n\nHonest diagnosis\nIdentify obstacles\nFew critical, relevant aspects\n\n\nGuiding policy\nGeneral approach to overcome\nFocused on key aspects\n\n\nCoherent actions\nSupport policy with action plan\nCoordinated and focused\n\n\n\nSince strategy informs so much of decision-making, know your firm’s strategy – ask around, understand it, contribute to it. Call out bad strategy.\n\n\n\nThis is a lot to take in! But there is one more thing to explore – a brilliant idea from a course by Dr. Adam Fleischhacker 4, which has a very similar philosophy, but is much more established and thought out, with many practical examples. Here is what he has to say in the course intro:\n4 Adam Fleischhacker - Introduction to Business Analytics: Intro to Bayesian Business Analytics in the R Ecosystem\n“You will translate real-world scenarios into both mathematical and computational representations that yield actionable insight. You will then take that insight back to the real-world to persuade stakeholders to alter and improve their real-world decisions.”\n\nDr. Fleischhacker makes an illuminating distinction between the business analyst’s workflow and a machine learning workflow, and sets up the normative criteria which make it successful. In our course, his workflow falls under the discussions related to causal inference. One interesting thing to note, is the convergence in the approach of an extremely diverse set of people: Cassie Kozyrkov, Vincent Warmerdam, Adam Fleischhacker, Richard McElreath, Andrew Ng – all coming from different backgrounds and activating in different environments and domains.\n\n\n\nSource: causact.com; “(The workflow) starts with strategy, expertise, and data as inputs and results in the business analyst leading their firms to act on value-adding insights”\n\n\nLet’s briefly review those normative criteria of this workflow. It might be a confirmation bias on my part, but the fact that these are present in the current course in one way or another, means I stumbled upon them by trial-and-error and painful mistakes:\n\nOutcome-focused: What is the point of fancy models, if we don’t achieve good or desired outcomes? If I was implying it so far, for the rest of the course we’ll ask this explicitly every time we tackle a business problem.\nStrategically-aligned: “Not all outcomes are created equal. Companies seeking to capture market share might increase expenses to aid market capture. Companies seeking to be cost leaders might leave some customers unsatisifed to keep expenses low. So a one-size-fits-all approach to defining good outcomes is ill-advised.”\nAction-oriented: We insisted so much on insights influencing, driving actions and decisions that there is little to add here. The remaining question is how can we communicate and articulate it well to convince decision-makers and stakeholders.\nComputationally Rigorous: Refers to the know-how, the engineering in the trenches. Even though we’ll spend most of the time in the frequentist land – I think the future is at the intersection of Causality and Bayesian Inference.\n\nTaking it one step further, this kind of workflow should be reproducible and (mostly) automated. This is why we’ll explore an ecosystem of software engineering tools and practices in the labs.\nIdeally, given in the hands of our clients/users in form of a full stack data app. This is where we take off our consulting hat and start building software products.\n\n\nThis is in contrast with a predictive, machine learning workflow, which we called before “workhorse models”, a “hammer” for which everything is a nail. We got a taste of its power and limitations, and tried to articulate which are appropriate applications for ML. This course gives equal attention to ML and Causality, due to the prevalence of use-cases from which we can learn from data to make tons of decisions at scale and high frequency.\n\n\n\nSource: causact.com; “The machine learning analyst transforms historical data with known outcomes into future outcome predictions.”",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "2. Performance and Strategy"
    ]
  },
  {
    "objectID": "01_fundamentals/2_performance_strategy.html#what-is-strategy",
    "href": "01_fundamentals/2_performance_strategy.html#what-is-strategy",
    "title": "Business Performance and Strategy",
    "section": "",
    "text": "Sounds like a naive question, but bad strategy is prevalent – which comes from a misunderstanding about what is a strategy. I highly recommend this article published by McKinsey – we learn better from other’s mistakes.\nIn short, it is not just aspiration towards goal or having a vision or setting a target. By analogy, remember the SMART criteria when you’re setting goals. As true consultants, we can summarize the steps involved in developing a strategy in a matrix.\n\n\n\n\n\n\n\n\nStep\nOutcome\nCharacteristics\n\n\n\n\nHonest diagnosis\nIdentify obstacles\nFew critical, relevant aspects\n\n\nGuiding policy\nGeneral approach to overcome\nFocused on key aspects\n\n\nCoherent actions\nSupport policy with action plan\nCoordinated and focused\n\n\n\nSince strategy informs so much of decision-making, know your firm’s strategy – ask around, understand it, contribute to it. Call out bad strategy.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "2. Performance and Strategy"
    ]
  },
  {
    "objectID": "01_fundamentals/2_performance_strategy.html#business-analysts-workflow",
    "href": "01_fundamentals/2_performance_strategy.html#business-analysts-workflow",
    "title": "Business Performance and Strategy",
    "section": "",
    "text": "This is a lot to take in! But there is one more thing to explore – a brilliant idea from a course by Dr. Adam Fleischhacker 4, which has a very similar philosophy, but is much more established and thought out, with many practical examples. Here is what he has to say in the course intro:\n4 Adam Fleischhacker - Introduction to Business Analytics: Intro to Bayesian Business Analytics in the R Ecosystem\n“You will translate real-world scenarios into both mathematical and computational representations that yield actionable insight. You will then take that insight back to the real-world to persuade stakeholders to alter and improve their real-world decisions.”\n\nDr. Fleischhacker makes an illuminating distinction between the business analyst’s workflow and a machine learning workflow, and sets up the normative criteria which make it successful. In our course, his workflow falls under the discussions related to causal inference. One interesting thing to note, is the convergence in the approach of an extremely diverse set of people: Cassie Kozyrkov, Vincent Warmerdam, Adam Fleischhacker, Richard McElreath, Andrew Ng – all coming from different backgrounds and activating in different environments and domains.\n\n\n\nSource: causact.com; “(The workflow) starts with strategy, expertise, and data as inputs and results in the business analyst leading their firms to act on value-adding insights”\n\n\nLet’s briefly review those normative criteria of this workflow. It might be a confirmation bias on my part, but the fact that these are present in the current course in one way or another, means I stumbled upon them by trial-and-error and painful mistakes:\n\nOutcome-focused: What is the point of fancy models, if we don’t achieve good or desired outcomes? If I was implying it so far, for the rest of the course we’ll ask this explicitly every time we tackle a business problem.\nStrategically-aligned: “Not all outcomes are created equal. Companies seeking to capture market share might increase expenses to aid market capture. Companies seeking to be cost leaders might leave some customers unsatisifed to keep expenses low. So a one-size-fits-all approach to defining good outcomes is ill-advised.”\nAction-oriented: We insisted so much on insights influencing, driving actions and decisions that there is little to add here. The remaining question is how can we communicate and articulate it well to convince decision-makers and stakeholders.\nComputationally Rigorous: Refers to the know-how, the engineering in the trenches. Even though we’ll spend most of the time in the frequentist land – I think the future is at the intersection of Causality and Bayesian Inference.\n\nTaking it one step further, this kind of workflow should be reproducible and (mostly) automated. This is why we’ll explore an ecosystem of software engineering tools and practices in the labs.\nIdeally, given in the hands of our clients/users in form of a full stack data app. This is where we take off our consulting hat and start building software products.\n\n\nThis is in contrast with a predictive, machine learning workflow, which we called before “workhorse models”, a “hammer” for which everything is a nail. We got a taste of its power and limitations, and tried to articulate which are appropriate applications for ML. This course gives equal attention to ML and Causality, due to the prevalence of use-cases from which we can learn from data to make tons of decisions at scale and high frequency.\n\n\n\nSource: causact.com; “The machine learning analyst transforms historical data with known outcomes into future outcome predictions.”",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "2. Performance and Strategy"
    ]
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#three-critical-questions",
    "href": "01_fundamentals/slides/lecture2/index.html#three-critical-questions",
    "title": "Decision Science",
    "section": "Three critical questions",
    "text": "Three critical questions\n\n\n\n\n\nStatus Quo, Desired and Feared Trajectories in systems’ dynamics. Source: Kim Warren\n\n\n\n\nWhat happened (data / facts) and Why (inference)?\nWhere are we likely going if we do things as before?\n\nIs it a feared scenario?\nWhat contributes to it?\n\nHow to achieve the desired trajectory? Is it realistic?\n\n\n\n\nThe topic of how to measure performance is complicated and deserves a deep-dive."
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#trajectories-and-swot-diagnosis",
    "href": "01_fundamentals/slides/lecture2/index.html#trajectories-and-swot-diagnosis",
    "title": "Decision Science",
    "section": "Trajectories and SWOT diagnosis",
    "text": "Trajectories and SWOT diagnosis\n\n\n\n\n\n\nWhat would an apparel e-commerce want?\n\n\n(\\(\\max\\)) Top line (revenue) | bottom line (EBITDA) | customer satisfaction | LTV 1\n\nStatus Quo: what is most likely trajectory? What contributes to it?\n\nlooks good \\(\\implies\\) strengths\nlooks bad or unsatisfactory \\(\\implies\\) weaknesses\n\nFeared trajectory (shocks, risks, macro environment, competition):\n\nscenario looks bad \\(\\implies\\) threats\n\nDesired trajectory. Is it reasonable and realistically achievable?\n\nif yes \\(\\implies\\) opportunities\n\n\n\n\n\n\n\nDiscussion: There are better tools than SWOT, coming from management consulting. 2"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#we-cant-avoid-tradeoffs",
    "href": "01_fundamentals/slides/lecture2/index.html#we-cant-avoid-tradeoffs",
    "title": "Decision Science",
    "section": "We can’t avoid tradeoffs",
    "text": "We can’t avoid tradeoffs\n\nWhat if it’s a startup that received big funding?\nWhat if it wants to capture market share?\nWhat if the goal is to have sustainable profitability?\nWhat if they position themselves as luxury?\n\n\nThe question we asked is too generic. We need a strategy and possible decisions, constraints in their value chain\n\n\n\nWhat is the optimal tradeoff between objectives? In which part of value chain should they take action? How to make the most out of limited resources it has?"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#business-analysts-workflow",
    "href": "01_fundamentals/slides/lecture2/index.html#business-analysts-workflow",
    "title": "Decision Science",
    "section": "Business Analysts’ Workflow",
    "text": "Business Analysts’ Workflow\n\nSource: Adam Fleischhacker; This process is highly iterative and depends on having good feedback and collaboration"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#characteristics-of-this-process",
    "href": "01_fundamentals/slides/lecture2/index.html#characteristics-of-this-process",
    "title": "Decision Science",
    "section": "Characteristics of this process",
    "text": "Characteristics of this process\n\nOutcome-focused: What’s the point otherwise?\nStrategically-aligned: Not all outcomes are equal!\nAction-oriented: Biggest pitfall of any AI/ML initiative – when it’s not actionable!\n\nNeeds clear and persuasive communication\n\nComputationally rigorous:\n\nCorrectness, reproducibility and maintainability\nAccesible: idealy in an app which users explore\n\n\n\n\nData scientists often forget about outcomes and actionability. Students get stuck in EDA"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#what-is-a-strategy-anyways",
    "href": "01_fundamentals/slides/lecture2/index.html#what-is-a-strategy-anyways",
    "title": "Decision Science",
    "section": "What is a strategy anyways?",
    "text": "What is a strategy anyways?\nNOT just aspiration towards goal or a vision or a target.\n\n\n\n\n\n\n\n\nStep\nOutcome\nCharacteristics\n\n\n\n\nHonest diagnosis\nIdentify obstacles\nFew critical, relevant aspects\n\n\nGuiding policy\nGeneral approach to overcome obstacles\nFocus on most promising\n\n\nCoherent actions\nSupport policy with action plan\nCoordinated and focused\n\n\n\n\n\nWhen it comes to setting good goals, you might’ve heard of SMART criteria"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#other-methodologies-to-be-aware-of",
    "href": "01_fundamentals/slides/lecture2/index.html#other-methodologies-to-be-aware-of",
    "title": "Decision Science",
    "section": "Other methodologies to be aware of",
    "text": "Other methodologies to be aware of\n\nStatistics and experiment design (12 steps) 3\n\nThe scientific process is much larger than this\n\nCausal inference and probabilistic graphs 4\nCRISP-DM, Tuckey’s Exploratory Data Analysis 5\nMachine Learning (12 steps) 6\nSoftware Development: Agile, DDD, TDD, XP, Design Thinking\nAI Products: People+AI, AI Governance, Event Storming\n\n\n\nMethodologies are NOT recipes, NOT step-by-step instructions, NOR bureaucracy. Ways of thinking which help you avoid pitfalls, be systematic and effective in problem-solving."
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#value-chain-meets-decision-science",
    "href": "01_fundamentals/slides/lecture2/index.html#value-chain-meets-decision-science",
    "title": "Decision Science",
    "section": "Value Chain meets Decision Science",
    "text": "Value Chain meets Decision Science\n\nSource: bayesianquest – Data Science Strategy Safari."
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#roles-in-firms-stuff-data-people-do",
    "href": "01_fundamentals/slides/lecture2/index.html#roles-in-firms-stuff-data-people-do",
    "title": "Decision Science",
    "section": "Roles in firms: stuff data people do",
    "text": "Roles in firms: stuff data people do\n\nData Engineering – pipelines and infrastructure\nData Analysts – detectives, decision support\nBI – infrastructure for reporting, clean, modeled data\nML Engineer – builds ML models and deploys them\nData Scientist – jack of all trades, often lots of stats\nProduct Analyst – cares about experiments\nDecision Makers & Domain Experts are usually the clients\n\n\n\nEveryone deals with data, even BE, FE, Decision-Makers, QA, DevOPS, domain experts"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#conversation-fields-use-cases",
    "href": "01_fundamentals/slides/lecture2/index.html#conversation-fields-use-cases",
    "title": "Decision Science",
    "section": "Conversation: fields & use-cases",
    "text": "Conversation: fields & use-cases\n\nWhat are the fields in which data science methods are extensively used? e.g. finance, genomics, psychology, …\nWhat are some products that use AI, data science, data-driven systems? What are their use-cases? e.g. uber …\n\n\n\n\n\n\n\nThink in terms of reverse engineering\n\n\nWhen using those products, how do you think those systems were designed?\n\nWhat were the goals and user/client needs? What were the firm’s objective?\nWhat constraints did they hit? Why is it a difficult problem?\nWhat are some potential approaches they settled on? What is a naive solution?"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#remark-on-course-philosophy",
    "href": "01_fundamentals/slides/lecture2/index.html#remark-on-course-philosophy",
    "title": "Decision Science",
    "section": "Remark on course philosophy",
    "text": "Remark on course philosophy\n\nWhy is something important (method, idea, model …)\nDevelop conceptual understanding and intuition\n\nTheoretical rigor only where necessary\n\nUse simulations as a safe playground\nPractical and realistic applications\n\nproblem formulation: focus on decision-making\nstart with simplest models\ndeal with messy data and introduce more realism\n\n\n\n\nThere is no shame in going back to the fundamentals to master them"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#the-danger-of-thinking-in-buckets",
    "href": "01_fundamentals/slides/lecture2/index.html#the-danger-of-thinking-in-buckets",
    "title": "Decision Science",
    "section": "The danger of thinking in buckets",
    "text": "The danger of thinking in buckets\nHere is R. Sapolsky’s argument about studying different aspects of human behavior:\n\nOur brains think about stuff in buckets / boundaries\nThese buckets influence our memory, language, behavior\nWe stop seeing the big picture:\n\nBad at differentiating facts within buckets\nExagerrate differences between buckets\n\nTempting to claim that a bucket is the only, true explanation\nSome of the most influential scientists fell into this trap"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#well-walk-across-many-buckets",
    "href": "01_fundamentals/slides/lecture2/index.html#well-walk-across-many-buckets",
    "title": "Decision Science",
    "section": "We’ll walk across many buckets",
    "text": "We’ll walk across many buckets\n\nProblem space: the CAS of a firm, but not only\nCognitive science: intelligence, rationality, foolishness\nProbability Theory: Reason under uncertainty, DAGs, DGPs\nStatistics: formulating hypotheses, experiment design\nMachine Learning: next year we focus on predictions\nComputer Science: how to make the stuff usable\nPhilosophy: ethics, epistemiology, phil. science\nMathematics: elegant abstractions and tools"
  },
  {
    "objectID": "01_fundamentals/slides/lecture2/index.html#footnotes",
    "href": "01_fundamentals/slides/lecture2/index.html#footnotes",
    "title": "Decision Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsome industries have specific metrics like daily users, revenue per mile per sq.m\ne.g. McKinsey MECE + hypotheses, root cause analysis, modeling processes to find bottlenecks, impact analysis, design thinking, event storming, etc\nWe will discuss research methods and experiment design in the following lectures\nWe sketched out the process in the previous lecture\nYou might be familiar with this\nWe will dedicate two lectures on this, since you will need it until year two."
  },
  {
    "objectID": "01_fundamentals/1_applications.html",
    "href": "01_fundamentals/1_applications.html",
    "title": "Data Science in the Wild",
    "section": "",
    "text": "Data Science in the Wild\nAt this point, you might get tired of me emphasizing the decision-making aspect of data science as the main point of why it is important. It’s time we move from the general and abstact towards particular examples and applications in various industries. This will lead us to acknowledge how prevalent is AI (that we haven’t fully defined yet) in firms, services and technologies we use every day.\n\n\n\n\n\n\nDuring a lecture, I usually ask students\n\n\n\nCan you give some examples of businesses, sevices, technologies, problems, and domains which you suspect do have AI/ML algorithms and models behind the scenes?\nSee below some very good answers and argumentations provided by the students last year, then we’ll examine in more details one of them. Of course, there is always that one person, very passionate about sports or blockchain.\n\n\n\n\n\n\n\n\nDo some reverse engineering!\n\n\n\nWhen reading this section, I want to get you into a mindset of the reverse engineer: step back and think deeply about products and services you use every day:\n\nPut yourself in the shoes of the business making those decisions and building those systems\nWhat were the goals, user/client needs?\nWhat was the firm’s objective?\nWhat constraints did they hit? Why it was a difficult problem?\nWhat made it an appropriate use-case for ML, Statistics, and AI?\nWhat are some potential approaches they settled on?\nWhat would be a baseline, simple, naive solution?\n\n\n\n\nDynamic pricing in Bolt and Uber, which takes into account the weather, especially if it rains, peak hours: balancing demand and supply. It is at the intersection of ML and economic theory, as they are a platform or marketplace. Prices also change with respect to competitors, so we see aspects of an oligopoly behavior. 1\n\nStock markets and trading bots: at the intersection of economics, finance and AI. I would add the “good old” and boring portfolio management and venture capital enterprises.\nManagement consulting: what market to enter, whether and how to build a new product (product development). Lots of use-cases in marketing and market research firms.\nMedicine Applications: developing new vaccines and drugs, aided by AI and designing clinical trials for novel treatments.\nBanks and insurance: risk management, predicting credit defaults on mortgages and business loans. Chatbots for customer support, for most frequent and simple questions.\nAutomotive: routine tasks like automated parking, the race towards self-driving, autonomous or semi-autonomous cars, safety warnings. Predictive maintainance is tackling a problem where they leverage predictions to replace risky parts before they go out of function.\nLiverpool F.C. won a title, and a key part of their success was leveraging AI and ML to discover new tactis on the field with the highest payoffs. 2\nNBA teams invested a lot in the data infrastructure and decision-making capabilities: LA Lakers found the best player at the moment for a particular position they were lacking and would play well along with the team. Rockets won the regular championship divison by going all in on the 3-point shot.3 Golden State Warriors simply revolutionised basketball with data, before everyone else was doing it – giving them a competitive edge. 4\n\n1 When it doesn’t work out – I’m pretty upset at their data scientists and domain experts. Here is where ethical issues creep up: jacking up prices, monopolies, drivers struggling to make a living wage.2 TODO: Reference article and maybe dataset for more details3 Moreyball: The Houston Rockets and Analytics – an article in Harvard’s MBA Digital Innovation4 Check out the following video by The Economist on how data transformed the NBA. For more details on the statistical methodology, I enjoyed an youtube channel called Thinking Basketball and their playlist about the statistical methodology.In all of the examples above, those businesses and systems do have to make decisions, under uncertainty from multiple sources, trying to solve complex problems at a large scale, which would be impossible to do manually even with an army of employees.\nI would like to add a few more examples, from an insider and practitioner’s perspective, which might not be as impressive and a bit routine, but no less important. Keep in mind, that if at a closer look, the service seems to do something relatively intelligent very fast, specialized AI might be involved behind the scenes.\n\nDemand Planning: How many SKUs (items) should I order for manufacturing, to satisfy the demand (that last item on the shelf, minimizing lost sales) and to minimize excess inventory.\nLogistics and Supply Chain: routing, distribution center operations and automations for order fulfillment, return management\nRecommender systems for music, videos, books, products, news in social media, services, platforms, and e-commerces like facebook, instagram, tiktok, youtube, spotify, amazon, emag. You can find recommendations in surprising places, like google maps.\nProgrammatic Advertisement: finding best placement for ads on various platforms, right now dominated by meta and google\n\n\n\n\n\n\n\nClassroom Case Studies for a Deeper Understanding\n\n\n\nWe will explore some challenges and applications from this list in a series of case studies and labs. The idea is to improve our ability to identify opportunities and formulate problems from the point of view of an organisation, such that we can match those with the methods, models and algorithms discussed in the course.\nI’ll have to introduce a lot of new concepts when the language we developed so far will turn out to be insufficient to talk about and understand what’s happening inside these firms. Thus, each case study is an opportunity to play around with a novel idea. 5\n\nIn the first deep-dive, we will look at Uber and Bolt, with publicly available information, trying to figure out what do their data scientists do.\nThen, we will look at the lingerie e-commerce where I work at, AdoreMe and a different set of problems we’re facing.\n\n\n\n\n\n5 If you have a pretty good idea about what is AI, Analytics, ML, Deep Learning, Big Data, Causal Inference and when to use one approach or another, feel free to skip the history and terminology and go straight to the case studies.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "1. Application Domains & Use-Cases"
    ]
  },
  {
    "objectID": "01_fundamentals/learning.html",
    "href": "01_fundamentals/learning.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "In the last two sections of this lecture I felt it is necessary to raise two more warning tales, precisely because we will work with such a powerful machinery and attempt to tackle complex problems in our jobs and lives.\nA good metaphor for what we’re doing in this course is the way people learn. In a way we’re pattern recognition machines, with a powerful capacity for implicit learning, meaning we can’t articulate or explain how we did it. For example, riding a bike, catching a baseball, speaking, reading and writing, behaving in social situations and so on.\n\n\n\n\n\n\nImplicit Learning of Artificial Grammars\n\n\n\nThere is a famous experiment, in which researchers invented words from two languages, meaning two set of rules,1 let’s say between 3 and 8 characters, with appropriate vowels and so on. Total gibberish, but here we are, with two sets of words.\nParticipants saw the lists and they had to say from what “language” does a word come from. It is a good example of experiment design coming from cognitive science research.\n\n\n1 For the curious, they used a Markov Chain artificial grammar, which would be pretty straightforward to implement in R, and is a way of looking at the language which brought the first breakthroughs in NLP (natural language processing)\n\n\nSource: John Kihlstrom; They found that subjects differentiated them much better than chance and the results were statistically significant. During the interviews, when asked to explain how they did it, the responses were either “no idea”, or giving some rules, which when implemented on a computer, were not able to perform better than a coin flip.\n\n\nFollowing the experiment, this means that those rules articulated by the people in the experiment were NOT how they were thinking. There is something going on which can’t really be articulated. This means that we have a capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning.\nAnother hypothesis about how animals learn is the idea that some prior 2 knowledge or mechanism – which is there due to evolution optimizing for fittedness, is necessary to kickstart the process. The discussion about the fascinating interaction between nature and nurture is outside the scope of the point I’m trying to make right now. So, here we go with two more experiments:\n2 We will go into more technical details when discussing Statistical Learning and ML models, of why is it the case that biasing a learning process with prior information is essential to successful learning.\n\n\n\n\n\nBait Shyness – Rats Learning to Avoid Poisonous Baits\n\n\n\nWhen rats stumble upon food with new smell or look, they first eat very small amounts. If they got sick, that novel food is likely to be associated with illness, and in the future the rat will avoid it. Quoting Dr. Shai-Ben David:\n\nClearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative effect when encountered in the future. 3\n\n\n\n3 Shai-Ben David - Understanding Machine Learning, 2014The bait shyness is an example of learning, not just rote memorization. More exactly of a basic inductive reasoning, or in a more statistical language, of generalization from a sample. But we intuitively know, that when generalizing a pattern, regularity is sometimes error prone: we pick up on noise, spurious correlations instead of signal, we’re fooled by randomness.\n\n\n\n\n\n\nB.F. Skinner: Pigeon Superstition\n\n\n\nIn an experiment, B.F. Skinner put a bunch of hungry pigeons in a cage and gave them food at random intervals via an automated mechanism. They were doing something when the food was first delivered, like turning around or pecking – which reinforced that behavior.\nTherefore, they spent more time doing that exact same thing, without regard with the chances of those actions getting them more food. That is a minimal example of superstition, a topic on which philosophers spilled so much ink.\n\n\nShai-Ben David goes on to argue – what stands at the foundations of Machine Learning, are carefully formulated principles which will prevent our automated learners, who don’t have a common sense like us, to reach these foolish and superstitious conclusions. 4\n4 \n\n\nSource: skewsme; Here’s Skinner’s pigeon chambers\n\n\nWhat is the common thread among these three stories about learining? In a nutshell, it’s about cultivating the wisdom and ablility to differentiate between correlational and causal patterns.\n\nWhen all goes well we call it intelligence, intuition, a business knack. It’s our pattern recognition picking up on some real, causal regularities. It’s the common sense working as it is supposed to.\nWhen learning goes awry, it’s a bias and in worst cases – bigoted prejudice.\nI am not a fan of how behavioral economics treats biases, 5 but here are a few so prevalent and harmful outside their intended evolutionary purpose, that we have to mention them: confirmation bias, recency, selection, various discounting biases.\nWe attribute a causal explanation to a phenomena when it’s not. For example, size of the wedding to a long-lasting marriage, extroversion and attractiveness with competence, how religious are people vs years of life.\nThere are numerous examples, and I don’t have to repeat that correlation doesn’t imply causation, due to common causes, hidden variables, mediators, reverse causes and confounders.\n\n5 The reasons come from cutting-edge Cognitive Science research, which challenge the normative position of economic rationality. Moreover, they challenge the economic orthodoxy when it comes to rationality – it is a much more complex beast than a maximisation of expected utility.So, what can we do as individuals and professionals? I think one way to get wiser is to cultivate a kind of active open-mindedness, which tries to scan for those biases and bring them into our awareness, such that we can correct our beliefs, behavior and decisions. Another thing we can do is to update our beliefs often, in the face of new evidence, keeping a scout mindset, trying to see clearly, instead of getting too attached and invested in our beliefs and positions.\nI think we’re extremely lucky to be in a field like data science, where we can use formal tools from probability, causal inference, machine learning, optimization, combined with large amounts of data and domain expertise – in order to practice that kind of a mindset. However, let’s keep in mind how easy researchers are getting fooled, not only by randomness, and that we’ll never be immune to misjudgement.\n\n\n\n\n\n\nThe double edged sword of our intelligence\n\n\n\nThe same machinery which makes us intelligent general problem solvers and extraordinarily adaptable, makes us prone, vulnerable to bullshit and self-deceptive, self-destructive behavior.\nIt is the same, in a more techincal sense, with overfitting ML models and drawing wrong inferences. In business settings, I believe that firms will realize and appreciate that a decison scientist has this exact role – to help others rationally pursue their goals and strategy.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "5. Learning from Data: Intuition and Bias"
    ]
  },
  {
    "objectID": "01_fundamentals/learning.html#implicit-learning-intuition-and-bias",
    "href": "01_fundamentals/learning.html#implicit-learning-intuition-and-bias",
    "title": "Decision Science Course",
    "section": "",
    "text": "In the last two sections of this lecture I felt it is necessary to raise two more warning tales, precisely because we will work with such a powerful machinery and attempt to tackle complex problems in our jobs and lives.\nA good metaphor for what we’re doing in this course is the way people learn. In a way we’re pattern recognition machines, with a powerful capacity for implicit learning, meaning we can’t articulate or explain how we did it. For example, riding a bike, catching a baseball, speaking, reading and writing, behaving in social situations and so on.\n\n\n\n\n\n\nImplicit Learning of Artificial Grammars\n\n\n\nThere is a famous experiment, in which researchers invented words from two languages, meaning two set of rules,1 let’s say between 3 and 8 characters, with appropriate vowels and so on. Total gibberish, but here we are, with two sets of words.\nParticipants saw the lists and they had to say from what “language” does a word come from. It is a good example of experiment design coming from cognitive science research.\n\n\n1 For the curious, they used a Markov Chain artificial grammar, which would be pretty straightforward to implement in R, and is a way of looking at the language which brought the first breakthroughs in NLP (natural language processing)\n\n\nSource: John Kihlstrom; They found that subjects differentiated them much better than chance and the results were statistically significant. During the interviews, when asked to explain how they did it, the responses were either “no idea”, or giving some rules, which when implemented on a computer, were not able to perform better than a coin flip.\n\n\nFollowing the experiment, this means that those rules articulated by the people in the experiment were NOT how they were thinking. There is something going on which can’t really be articulated. This means that we have a capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning.\nAnother hypothesis about how animals learn is the idea that some prior 2 knowledge or mechanism – which is there due to evolution optimizing for fittedness, is necessary to kickstart the process. The discussion about the fascinating interaction between nature and nurture is outside the scope of the point I’m trying to make right now. So, here we go with two more experiments:\n2 We will go into more technical details when discussing Statistical Learning and ML models, of why is it the case that biasing a learning process with prior information is essential to successful learning.\n\n\n\n\n\nBait Shyness – Rats Learning to Avoid Poisonous Baits\n\n\n\nWhen rats stumble upon food with new smell or look, they first eat very small amounts. If they got sick, that novel food is likely to be associated with illness, and in the future the rat will avoid it. Quoting Dr. Shai-Ben David:\n\nClearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative effect when encountered in the future. 3\n\n\n\n3 Shai-Ben David - Understanding Machine Learning, 2014The bait shyness is an example of learning, not just rote memorization. More exactly of a basic inductive reasoning, or in a more statistical language, of generalization from a sample. But we intuitively know, that when generalizing a pattern, regularity is sometimes error prone: we pick up on noise, spurious correlations instead of signal, we’re fooled by randomness.\n\n\n\n\n\n\nB.F. Skinner: Pigeon Superstition\n\n\n\nIn an experiment, B.F. Skinner put a bunch of hungry pigeons in a cage and gave them food at random intervals via an automated mechanism. They were doing something when the food was first delivered, like turning around or pecking – which reinforced that behavior.\nTherefore, they spent more time doing that exact same thing, without regard with the chances of those actions getting them more food. That is a minimal example of superstition, a topic on which philosophers spilled so much ink.\n\n\nShai-Ben David goes on to argue – what stands at the foundations of Machine Learning, are carefully formulated principles which will prevent our automated learners, who don’t have a common sense like us, to reach these foolish and superstitious conclusions. 4\n4 \n\n\nSource: skewsme; Here’s Skinner’s pigeon chambers\n\n\nWhat is the common thread among these three stories about learining? In a nutshell, it’s about cultivating the wisdom and ablility to differentiate between correlational and causal patterns.\n\nWhen all goes well we call it intelligence, intuition, a business knack. It’s our pattern recognition picking up on some real, causal regularities. It’s the common sense working as it is supposed to.\nWhen learning goes awry, it’s a bias and in worst cases – bigoted prejudice.\nI am not a fan of how behavioral economics treats biases, 5 but here are a few so prevalent and harmful outside their intended evolutionary purpose, that we have to mention them: confirmation bias, recency, selection, various discounting biases.\nWe attribute a causal explanation to a phenomena when it’s not. For example, size of the wedding to a long-lasting marriage, extroversion and attractiveness with competence, how religious are people vs years of life.\nThere are numerous examples, and I don’t have to repeat that correlation doesn’t imply causation, due to common causes, hidden variables, mediators, reverse causes and confounders.\n\n5 The reasons come from cutting-edge Cognitive Science research, which challenge the normative position of economic rationality. Moreover, they challenge the economic orthodoxy when it comes to rationality – it is a much more complex beast than a maximisation of expected utility.So, what can we do as individuals and professionals? I think one way to get wiser is to cultivate a kind of active open-mindedness, which tries to scan for those biases and bring them into our awareness, such that we can correct our beliefs, behavior and decisions. Another thing we can do is to update our beliefs often, in the face of new evidence, keeping a scout mindset, trying to see clearly, instead of getting too attached and invested in our beliefs and positions.\nI think we’re extremely lucky to be in a field like data science, where we can use formal tools from probability, causal inference, machine learning, optimization, combined with large amounts of data and domain expertise – in order to practice that kind of a mindset. However, let’s keep in mind how easy researchers are getting fooled, not only by randomness, and that we’ll never be immune to misjudgement.\n\n\n\n\n\n\nThe double edged sword of our intelligence\n\n\n\nThe same machinery which makes us intelligent general problem solvers and extraordinarily adaptable, makes us prone, vulnerable to bullshit and self-deceptive, self-destructive behavior.\nIt is the same, in a more techincal sense, with overfitting ML models and drawing wrong inferences. In business settings, I believe that firms will realize and appreciate that a decison scientist has this exact role – to help others rationally pursue their goals and strategy.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "5. Learning from Data: Intuition and Bias"
    ]
  },
  {
    "objectID": "01_fundamentals/learning.html#calling-bullshit-in-the-age-of-big-data",
    "href": "01_fundamentals/learning.html#calling-bullshit-in-the-age-of-big-data",
    "title": "Decision Science Course",
    "section": "Calling Bullshit in the age of Big Data",
    "text": "Calling Bullshit in the age of Big Data\nYou probably noticed that I used the word bullshit a few times in this lecture. It is not a pejorative, but a technical/formal term introduced by Harry Frankfurt in his essay “on Bullshit”. In romanian, the closest equivalent would be “vrăjeală”, a kind of sophistry.\n\n\n\n\n\n\nThe critical difference between Lie and Bullshit\n\n\n\nA liar functions on respect for the truth, as he inverts parts of a partially true story to convince you of a different conclusion. It is interesting that we can’t really lie to ouselfs, we kind of know it’s a lie – so we do the other, we distract our attention away from it.\nIn Bullshit, you try to convince someone of something without regard for the truth. You distract their attention, drown them in irrelevant, but supersalient stuff.\n\n\nIn our age, BS is much more sophisticated than the “good old” advertisement trying to manipulate you to buy something. I can’t recommend enough that you watch the lectures by Carl T. Bergstrom and Javin West,6 where they explain at length numerous ways we’re being convinced by bullshit arguments, but which are quantitative and have lots of jargon and pretty charts in them.\n6 Carl T. Bergstrom and Javin West - Calling Bullshit: The art of Skepticism in a Data-Driven WorldThe kind of intimidating jargon comes from finance people, economists, when explaining why interest rates were risen, what happened in 2007-2008 great financial crisis. My “favorite” is cryptocurrency-related sophistry and some particular CEOs expertly making things more ambiguous, mysterious, and complicated with their corporate claptrap.\nThese lectures are short, fun, informative and useful for developing the critical thinking necessary when assesing the quality of the evidence or reasoning which led to a particular conclusion. I will try to incorporate here and there some examples from their course, where it fits naturally with our use-cases and theoretical topics, especially in causality.\nThere are tempting arguments which boil down to this: due to ever increasing volumes and richness of data, together with computing power and innovations in AI – it will lead to the end of theory. I couldn’t disagree more!\n\n\n\n\n\n\nSmall Data problems in Big Data\n\n\n\nIn huge datasets of clients and web events, there are lots of observations and many features/attributes being collected, which theoretically should be excellent for a powerful ML model.\nHowever, at the level of each observation, when we go to a very granular aggregation level, the information can be extremely sparse, with high cardinality, inconsistent (all data quality issues). For example, in an e-commerce, for a customer, you might have no information about their purchases, and just a few basic facts about their website navigation.\nSo, you have the cold start problem, data missing not at random, censoring/survival challenges, selection biases. The data at the lowest level becomes discrete, noisy, heteroskedastic. You know the saying: garbage in garbage out.\nEven in ML when there is a big, clean and rich dataset, we can’t escape theory (which is our understanding of the world), in one way or anover. For example, in demand forecasting, we need to provide the model relevant data, factors which are related, plausibly influencing that demand: like weather, holidays, promotions, competition, and so on.\nWe can’t just pour all this data into a ML model and expect the best. It isn’t clear that feeding irrelevant data doesn’t break or bias our model, such that it picks up on noise and spurious correlation, especially in very powerful DL models. That definitely doesn’t help with better decisions. From an engineering standpoint, I recommend you watch this talk “Big Data is Dead” by the creator of DuckDB.\n\n\nIn conclusion, there is no magic to AI, no silver bullet: more data and better models are often necessary, but not sufficient to improve outcomes. We have to ask the right questions. We have to set objectives aligned with business strategy. We have to frame and formulate a problem well, understanding it in its context. We have to collect relevant data, clean it, understand the processes of missingness. If we let AI decide who enters into a quarantine during the pandemic, what it would optimize for? It’s just a machinery combining statistics and optimization.\nTherefore, critical thinking becomes that much more important when we have these powerful quantitative tools at out fingerprints. A part of a data scientist’s job is to constrain artificial stupidity (more exactly, foolishness, because it does perfectly fine what you instructed it to do) and making sure we’re solving the right problem (sounds trivial, but ofter we solve the wrong problem, without being aware of it).",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "5. Learning from Data: Intuition and Bias"
    ]
  },
  {
    "objectID": "01_fundamentals/background.html",
    "href": "01_fundamentals/background.html",
    "title": "What is AI, Data Science, Cybernetics?",
    "section": "",
    "text": "What is AI, Data Science, Cybernetics?\nIt’s undeniable that there is a lot of excitement when it comes to AI, ML, and data science – to the point of calling it the sexiest job of 21st century. Data science is an umbrella term, with interdisciplinary at its core, drawing inspiration from multiple fields, sets of tools, practices, methods and it continuously evolves. 1 It is designed to help us tackle increasingly more complicated problems at a large scale. There is also a reasonable worry about ways in which these systems can go wrong or awry.\n1 There are many questions still unanswered: How does this landscape of Data Science look like? What are the roles and jobs? What is the process for building smarter, data-driven software systems; drawing more reliable inferences and conclusions from data and theory? How does a day in data scientist’s life look like?You will often see a Venn diagram where data science sits at the intersection of mathematics – statistics, computer science – software engineering, and domain knowledge. I think this is not sufficient to characterise data science, therefore, will try to elaborate what it does, and how (which is as important).\n\n\n\n\n\n\nAI in a Nutshell\n\n\n\nFor all pragmatic intents and purposes, especially in businesses, AI is about Decision-Making under Uncertainty at Scale 2.\nOne important keyword here is uncertainty, as there is no point in building AI solutions based on complex models if we don’t have uncertainty. We have to be able to change our mind and actions in the face of evidence.\nOn the other hand, scale is the reason ML and Deep Learning is so powerful, because you can take lots of small decisions in an automated way, with little curation or guidance from humans. This is why many traditionally “paperwork” industries like legal and accounting embrace digitalisation and automation now.\n\n\n2 C. Kozyrkov - AI is decision-making at scaleUltimately, why would I build a system which predicts demand for products in a direct-to-consumer ecommerce like Allbirds, Macy’s, or AdoreMe? Either in a marketplace like Emag or Amazon? Why would I try to find out the factors which contribute to a successful advertisement?\n\n\n\n\n\n\nHere are some answers from students\n\n\n\n\nIn order to allocate resources to the stuff which generates growth and profit. Avoid being scattered around (which I would call bad strategy), resulting in costs over targets and inefficiency.\nIn short, we attempt to allocate resources and efforts efficiently.\nWe can view information as a competitive advantage, anticipating and predicting so that we can plan and prepare, outperform competitors.\n\n\n\nWhen we talk about uncertainty, it’s important to recognize its sources: 3 one coming from incomplete information, that we always work with samples in one way or another. For example, even if at a certain point in time we might have real-time data, everything evolves and quickly becomes outdated. Everything is in a state of flux and change. Even in the current state, we don’t know for sure where we stand – sometimes, in economics, this problem is called nowcasting. When talking about the future, making a good prediction is one of the most difficult things.\n3 We will talk more formally about sources of uncertainty in the next lecture, while reviewing the fundamentals of statistics.For example, who would’ve predicted the pandemic and all its implications on the supply chain and society? It’s important to note the difference between this kind of black swan events and the irreducible, fundamental uncertainty, which can’t be captured by any explanatory factors.\nIn a happy case, we can quantify and reduce it by conditioning the model, that is, joint distribution of random variables with a given structure, on data. That would result in inferences and evidence with respect to our hypothesis and model of the world.4 At the very least we can try to quantify how uncertain are we.\n4 We want to say something intelligent about the population, technically, to generalize. However, there is ambiguity, as objectives and the meaning/semantics of data fields are not always mathematically clear or without conflict.So, we still have to make decisions. Those have to have a level of robustness and resilience to shocks, in the face of uncertainty. I would go even further, to suggest that we should aim for antifragility, meaning, the system improves after a shock or negative event, but that is very hard to implement and operationalize, therefore, it falls outside the scope of this course – to the realm of systems’ design.\n\n\n\n\n\n\nWhen you don’t need AI and Statistics\n\n\n\nAs a though experiment, imagine we have an equation or program, with well-defined rules, which perfectly predicts the price on stock markets, or perfectly predicts how many items will a client buy and how she will respond if we change the price (an intervention). We won’t need machine learning, causal inference, or AI there.\nOf course, we don’t have that kind of program. It’s only somewhat true in cases when we have a well-tested theory, which stood the test of time and went through the scientific process to become the best theory with respect to all others. For example newtonian physics, relativity, quantum mechanics, evolution.\n\n\nHowever, when we talk about human behavior, we should resist the temptation and arrogance to say that we have a well-defined theory, be it normative or positive. Our preferences change, and we can “decide” in which direction they change or persist.\nRegardless of the business we work at or own, the place in the value chain, we’ll have to deal with human behavior: customers, employees, decision-makers, engineers. We need other kind of tools to infer perceptible regularities and patterns in their behavior. We will be forced in one way or another to learn from data and observation.\n\n\n\n\n\n\nA model is a simplified representation of reality\n\n\n\nWe need models to make sense of the world around us, because it is so complex and uncomprehensible if we are to represent it faithfully in a simulation. Therefore, we focus on relevant, interestig, essential aspects to us, we simplify by baking in domain knowledge, assumptions, and data into the models and algorithms.\nSo, we can collect data, apply algorithms to train models, in order to make inferences about some relevant quantities. That will help us in making evidence-based decisions which gets us closer to our objective in an efficient way.\n\n\n\n\n\n\n\n\nWeak AI is Domain-Specific\n\n\n\nBy now, you probably figured out that we’re not talking about General AI, trying to surpass human intelligence in general reasoning and problem-solving. Thus, we’re talking about weak or specialized AI, which depends very much on the domain.\nAI in an a fashion e-commerce, like AdoreMe, where we sell lingerie, will have a very different flavor from the tools and methods used in genomics, medicine, social science or psychology.\nDespite the fact that there are a lot of shared fundamentals, when it comes to the principles of building models, it is not straightforward to take something which works in one domain and apply it in another. Significant tweaks and adaptations are needed, which are dependent on the specificities of that domain.\nThe good news is that when these transdisciplinary groups of people work together and successfully adapt a method, it is often a breakthrough in the field borrowing the theory and technology.\n\n\n\nCybernetics is what we call AI\nAt this point we have a working definition of Weak AI. At a first glance it might be hard to see what does it have in common with Cybernetics and its study of Complex Adaptive Systems.\nI’m not trying to equivocate those two, but argue that weak AI is how Cybernetics evolved and is mostly used in practice now. I will give a definition from P. Novikov, which I found tremendously useful, then explain it. Can you spot the parallels of “decision-making under uncertainty at scale” in this definition?\n\n\n\n\n\n\nA better definition of Cybernetics\n\n\n\nThe science of general regularities of control and information processing in animal, machine and human (socitey)\n\n\n\n\n\n\n\n\nUnpacking this dense/abstract definition\n\n\n\n\nControl means goal-directedness, the ability to reach the goals and objectives by taking action and stirring the system towards a trajectory. The objective can also be perserving the structural-functional organization of the system itself, an autopoesis.\nInformation Processing could be pattern recoginition, perception, how you understand and model the world, what inferences do you draw, what “data inputs” are used\nGeneral regularities means what is true and plausible of control and information processing across fields and a variety of complex systems, not only in particular cases.\nAnimal refers to applications in biology, machine – in engineering, and human – in our society and behavior.\n\nIn economic cybernetics, we’re concerned with economics, society and human behavior, rather than engineering, biology, or natural science applications.\n\n\nTo explain how Cybernetics evolved into Weak AI, there is a conglomeration of fields which went a bit out of fashion and favor: Game Theory, General Systems’ Theory, Agent-Based Modeling, Systems’ Dynamics, Complexity and Chaos, Evolutionary Algorithms. This stuff is fascinating and inspired many other breakthroughs, but it is extremely difficult to implement in practice.\nSo, we kind of settled on a more pragmatic set of tools, which is dominated by pattern recognition and optimisation, in one form or another trying to learn from data (ML, DL, Causality) and act optimally (Dynamic Programming, Reinforcement Learning). .\nWait. What’s going on here? Am I saying that we did a bachelor’s degree in AI under the term of Economic Cybernetics? For me, personally, after having this epiphany – everything I studied makes so much more sense in retrospective.\n\n\n\n\n\n\nThe meaning of AI changed in the meanwhile\n\n\n\nYou can make sense of the terminology and general confusion of terms, by reading M. I. Jordan’s brilliant article 5, which tells the history of “AI” and how this confusion arose. He also points out how many of the claims in the field, as of today are a stretch (i.e. the revolution hasn’t happened yet) 6.\nI highly encourage you to read the articles by M. Jordan, but until then, here are a few ways people understand AI:\n\nCybernetics and Weak AI, which we discussed before\nGeneral AI is a titanic project. It interweaves with Philosophy, Cognitive Science, in order to understand what makes us intelligent and conscious. On the other hand, trying to build general-purpose problem solving machines.\nSymbolic AI, is still relevant in a few niches, especially in automated proofs and logical reasoning.\nAugmentative AI, like VR, augmenting human capabilities, human-machine interactions\n\n\n\n6 M. Jordan - Artificial Intelligence: The Revolution Hasn’t Happened Yet5 K. Pretz - Stop Calling Everything AI, Machine-Learning Pioneer SaysIn practice, if you’re a data/business analyst, ML/data engineer, data scientist, statistician, product manager – Cybernetics is a way of thinking in systems and formulate problems well. When it comes to implementation, we mostly use data and the tools, models, methods discussed in this course.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "3. What is AI and Cybernetics?"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html",
    "href": "sim/1L_bday_problem.html",
    "title": "Introduction to simulation in R",
    "section": "",
    "text": "In the introduction, I tried to convince you that simulation can be used to understand fundamental concepts in probability and statistics in order to appreciate their practical relevance. By implementing the stories, toy examples, and case-studies in code, we will bridge the gap between theory and practice. Hopefully, this will help you gain confidence and get over the fear of mathematical abstraction in probability and hone your programming skills for data analysis. 1\nWhen having trouble understanding something, do not despair and remember this quote from the great physicist. Do not resort under any circumstance to rote memorization,2 as you will forget it in a few weeks, but instead think like an engineer. Once you build it by yourself, you will understand how a method works and can do it again in the future. At last, I want to tell you two important lessons from Patrick Winston’s talk at MIT, “How to speak”:\nIt might sound obvious, but we practice in a very specific way – with your reference book(s), code editor, and pen / paper at all times. Learn actively, by doing and problem solving, by testing yourself, by trying to come up with your own examples. We cannot afford to rush and take shortcuts when studying mathematics and computer science.3",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html#the-birthday-problem",
    "href": "sim/1L_bday_problem.html#the-birthday-problem",
    "title": "Introduction to simulation in R",
    "section": "The Birthday Problem",
    "text": "The Birthday Problem\nThis classic probability problem about matching and coincidences will remind you of a few important ideas in combinatorics and will teach you some of the most important elements of R programming.\n\nWhat is the probability that in a room of \\(n=35\\) people, at least two have their birthday on the same day of year? We will assume that every day is equally likely, ignore leap year (\\(k = 365\\)), there are no twins (observations are independent with respect to birthday date).\n\n\n\n\n\n\nLet’s check your intuitions\n\n\n\nFind a colleague near you, discuss for two minutes how many people you need in a room so that the probability of any two people having a matching birthday is close to 0.5.\nThen, the other way around, for \\(n = 35\\), what is the probability of a matching birthday? In what range is your answer? Raise your hand for the appropriate bucket. Abstain if you know the exact answer from before.4\n\\[[0, 0.2), [0.20, 0.4), [0.4, 0.6), [0.6, 0.8), [0.8, 1]\\]\nThen, answer how appropriate are the assumptions we’re making – is it good enough for an approximation?\nAt last, think of boundary conditions: what is the probability for 365 and 400 people? Can you figure out the answer for 2 people?\n\n\n4 It’s important to realize that knowing the answer is not important, but the reasoning behind itWe solve this problem via simulation first and will check our answers versus the analytical solution. This is an important lesson – when you’re stuck on a problem, it’s a good idea to simulate it first and find the numerical answers, which will give us ideas about how to solve it mathematically. In reality, we need both, as a careful mathematical analysis will result in more insights about the properties of a problem or construct and will show us how to generalize to more complicated cases.\n\n\n\n\n\n\nSurprising applications\n\n\n\nSometimes, a simple problem like this can have surprising applications and provide valuable insights / tools in different, totally unrelated domains.\nThe birthday “paradox” is the simplest way to compute probability of coincidences. This is useful in cryptography (where matching hashes are a bad thing), satellite collisions with space junk, false DNA / genomic matches. A related problem is about partitions, where you try to balance weights on two scales – which is relevant in matchmaking systems, for example, in games like Dota2.\nSimilarly, simple urn models resulting from basic combinatorics and probability, were successfully applied by physicists like Bose, Einstein, Fermi, Botzmann to describe the large-scale behavior of different particles and physical phenomena. This is the power and elegance of choosing the right model for the right task.\n\n\n\nSimulation in base R\nWithout further ado, let’s simulate and solve the problem in the simplest possible way. Consult with your professor about how to arrive at this solution step-by-step. We will use the replicate function a lot in this course. I suggest you learn about vectorized operations like sapply or purrr::map as an elegant and performant way to replace for loops.\n\nset.seed(15317)  # has to be a larger odd number. why?\nnr_people &lt;- 35  # even integers and floats are vectors of size one!\n\nsimulate_birthdays &lt;- function(nr_people, nr_sim = 10000) {\n    birthday_events &lt;- replicate(n = nr_sim, {\n        birthdays &lt;- sample(x = 1:365, size = nr_people, replace = TRUE)\n        anyDuplicated(birthdays) &gt; 0  # returns for each sim\n    })\n    mean(birthday_events)  # this returns implicitly!\n}\n\npr_same_bday &lt;- simulate_birthdays(nr_people)\nbday_match_size &lt;- sapply(2:90, simulate_birthdays, nr_sim = 10000)\n\n\n\nThis little example teaches you about data types, random number generation, variable assignment, functions and their arguments, implicit returns, sampling with replacement, iteration without using for lops, operations and applying functions on vectors.\nA quick check of the result shows that the probability of n = 35 people having a matching birthday is 0.81. Is this close to your intuitive answer? Now, let’s visualize the results for a range of relevant n’s. How would you expect the curve to look like?\n\n\nShow the visualization code\nplot(\n    x = 2:90, y = bday_match_size, \n    type = \"l\", lty = 1, lwd = 5,\n    xlab = \"number of people\", ylab = \"probability\",\n    main = \"Probability of matching birthdays\",\n    xlim = c(0, 80), ylim = c(0, 1), # Limits\n    panel.first = abline(h = seq(0, 1, 0.2), col = \"grey80\")\n)\nsegments(\n    x0 = nr_people, y0 = bday_match_size[nr_people - 1], \n    x1 = nr_people, y1 = 0, \n    lty = \"dashed\"\n)\nsegments(\n    x0 = 2, y0 = bday_match_size[nr_people - 1], \n    x1 = nr_people, y1 = bday_match_size[nr_people - 1], \n    lty = \"dashed\"\n)\n\n\n\n\n\nIt takes a lot of effort to make a nice visualization in base R. This is why we will switch very soon to the tidyverse and ggplot grammar of graphics\n\n\n\n\n\n\nTry to keep the visualization as simple as possible. Don’t get lost into all the graphical parameters at the beginning, it will take time and practice to make beautiful graphs\nOne reason for this counterintuitive result is that probability grows relative to the number of possible pairings of people, not just the group’s size. For a group of 23, we’ll have \\(23 \\cdot 22 / 2 = 253\\) unique pairs of people. As in other applications of probability, having an intuition helps, but we can’t afford to be clever – we’ll get to the correct answer only by ruthless application of the rules of probability.\n\n\nAnalytical solution\n\n\nJoseph K. Blitzstein, Jessica Hwang - Introduction to probability, 2nd ed, 2019. (pg. 19-20)\nBy the multiplication rule, there are \\(365^n\\) ways to assign birthdays to the people in the room (sampling with replacement). First, you have to recognize that it is unfeasible to solve this problem directly via the inclusion-exclusion principle. Therefore, a useful trick in many problems is to focus on the complement: \\(P(A) = 1 - P(A^C)\\)\nCounting the number of ways in which we can assign birthdays to n people so that they don’t share a birthday is equivalent to sampling without replacement. Think of the metaphor of putting k flags on n poles. 5\n5 You might know the numerator as “Aranjamente”, but I like the idea of falling factorial and just remembering that we speak of ordered sampling without replacement\\[\n\\mathbb{P}(A^C) = \\frac{365 \\cdot 364 \\cdot ... \\cdot (365 - n + 1)}{365^n}\n\\]\nIn the original formulation of the problem, the question is how many people do we need for the probability to be close to 0.5. Let’s compute the analytical solution, visualize it, and use a simple base R mechanism for selecting / filtering the rows of interest.\n\nn_grid &lt;- seq(2, 70, by = 1)  # alternative to 2:70\n\n# notice the anonymous function notation \\(x) \nprob &lt;- sapply(n_grid, \\(x) 1 - prod(366 - 1:x) / 365^x)\nsim  &lt;- data.frame(nr_people = n_grid, prob = prob)\n\nsim[\"diff\"] &lt;- abs(0.5 - sim$prob)\nprob_half   &lt;- sim[which.min(sim$diff), ]\nprob_half\n\n   nr_people      prob        diff\n22        23 0.5072972 0.007297234\n\n\nIgnore the code for the following visualization, as it is unnecessarily complicated – I’m just showing off. 6 By this, I want to emphasize that a persuasive visualization is a very important practical skill and that it’s not easy to do in base R. So, in order not to suffer and achieve the same result much easier, we will have to use tidyverse and ggplot.\n6 treat it as a documentation of the things that can be customized in base R plots\n\nShow the visualization code\npar(# mar = c(3, 3, 3, 3),  # Dist' from plot to side of page\n    mgp = c(3, 1, 0),       # Dist' plot to label\n    las = 1,                # Rotate y-axis text\n    tck = -.01,             # Reduce tick length\n    xaxs = \"i\", yaxs = \"i\"  # Remove plot padding\n) \n\nplot(\n    x = sim$nr_people, y = sim$prob, type = \"l\", \n    ylab = \"probability\", xlab = \"number of people\", lwd=3,\n    main = paste0(\n        \"Only \", prob_half$nr_people, \" people for a \", \n        round(prob_half$prob * 100, 1), \"% chance of matching bday\"\n    ),\n    axes = FALSE, # Don't plot the axes\n    frame.plot = FALSE, # Remove the frame\n    xlim = c(0, 70), ylim = c(0, 1), # Limits\n    panel.first = abline(h = seq(0, 1, 0.25), col = \"grey80\")\n)\n\nsegments(\n    x0 = prob_half$nr_people, y0 = 0, \n    x1 = prob_half$nr_people, y1 = prob_half$prob,\n    lty=\"dashed\"\n)\nsegments(\n    x0 = 0, y0 = prob_half$prob, \n    x1 = prob_half$nr_people, y1 = prob_half$prob, \n    lty=\"dashed\"\n)\nat &lt;- pretty(sim$nr_people)\naxis(side = 1, at = at, col = \"grey40\", line = 1, cex = 0.7)\nat &lt;- seq(0, 1, 0.25)\nmtext(side = 2, text = at, at = at, col = \"grey40\", line = 1, cex = 0.9)\n\n\n\n\n\n\n\n\n\n\n\nSimulation with tidyverse\nFinally, let’s see another way of solving the same problem using the tidyverse. It takes some time to get used to the pipelines and working within data frames, but it is a much nicer way of working in R. It almost looks like SQL in its semantics.7\n7 I highly recommend Hadley Wickham’s R for Data Science free and open-source book. It will take a week or so of intensive study to go through it, but you can benefit a decade ahead from the very practical skills you will acquire.\nlibrary(tidyverse)\nlibrary(glue)\n\nsim_tidy &lt;- tidyr::crossing(\n        people = seq(2, 70, by=1), \n        trial  = 1:10000,\n    ) |&gt;\n    dplyr::mutate(\n        birthday = purrr::map(people, \\(x) sample(365, x, replace = TRUE)), \n        multiple = purrr::map_lgl(birthday, \\(x) any(duplicated(x)))\n    ) |&gt;\n    dplyr::group_by(people) |&gt;\n    dplyr::summarise(chance = mean(multiple)) |&gt;\n    dplyr::mutate(analytical_solution = \n        purrr::map_dbl(people, \\(x) 1 - prod(366 - 1:x) / 365^x)\n    )\n\nprob_half_tidy &lt;- sim_tidy |&gt; mutate(diff = abs(0.5 - analytical_solution)) |&gt;\n    arrange(diff) |&gt;\n    head(1)\nprob_half_ppl &lt;- prob_half_tidy |&gt; pull(people)\nprob_half_tidy\n\n# A tibble: 1 × 4\n  people chance analytical_solution    diff\n   &lt;dbl&gt;  &lt;dbl&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1     23  0.518               0.507 0.00730\n\n\n\nggplot(sim_tidy) +\n    geom_line(aes(people, chance)) +\n    geom_line(aes(people, analytical_solution), lty=\"dashed\", color = \"darkred\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        title = glue(\"Only {prob_half_ppl} people for a 50% chance of matching bday\"),\n        subtitle = \"Analytical vs simulated solution\",\n        y = \"probability\", x = \"number of people\"\n    ) + \n    theme_minimal()\n\n\n\n\nNotice how we got a visualization of a similar quality with much less code",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html#homework-and-study-resources",
    "href": "sim/1L_bday_problem.html#homework-and-study-resources",
    "title": "Introduction to simulation in R",
    "section": "Homework and study resources",
    "text": "Homework and study resources\nThe homework in this course has reading and coding assignments. Your readings will mostly be stories, case-studies, and papers published in academic journals – there won’t be much theory.\n\nIntroduction and background. Understand why simulation and numerical methods are important and how this class fits in a larger context of decision science. ~30min\nWhy did you study all of that? You will find an explanation of how the subjects you studied before are helpful in practice and what are their main idea. ~15min\nRead this short explanation on the differences between probability and statistics ~5min\nWatch this video of Santosh S. Venkatesh showing the vast array of practical applications of probability theory ~5min\n\nBesides this one hour of light readings, you should spend some time to install R, RStudio, learn how to run commands, where to look for output, documentation, and errors; how to create and save a script, how to install packages, load them and check if they installed successfully. 8\n8 Optional! You can attempt to generalize the birthday paradox to an arbitrary k and 3 or 4 matches. You can check out this post in SAS, but think for yourself how would you implement it in RIn order to make sure you’re comfortable with RStudio, try to reproduce the analysis of the 2022 Australian elections, in section 2.2 of “Telling stories with data”. You will not understand what each line of code does, but will get a sense of what we will do for the remainder of the course.",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "roadmap_prob.html",
    "href": "roadmap_prob.html",
    "title": "Module II: Probability and Statistics",
    "section": "",
    "text": "There are so many pitfalls in statistics and many of them come from a misunderstanding about the nature of statistical inference, a mechanical application of methods, and an insufficient grasp of fundamentals. I prefer to fill in the gaps with stories and simulations, however, in some cases the mathematical formalism and abstractions can’t be avoided and actually helps understanding.\nTherefore, fundamental – doesn’t mean easy, nor basic, nor trivial. Most courses make you solve puzzles, but I ask you to appropriately define, justify, and apply the choice of method and tool in the context of business applications.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics",
      "Schedule & Roadmap"
    ]
  },
  {
    "objectID": "roadmap_prob.html#pythonr-computational-toolbox",
    "href": "roadmap_prob.html#pythonr-computational-toolbox",
    "title": "Module II: Probability and Statistics",
    "section": "Python/R Computational Toolbox",
    "text": "Python/R Computational Toolbox\nIn order to make the most out of the three modules, we need to be skilled (or at least competent) programmers, from the perspective of data science. It is a skill which can be developed independently – I give a few recommendations in the introduction (prerequisites section).\n\n\nIn this course I heavily rely on the principles of reproducible research, which have 3 key aspects: data, computational environment, and code.\nA second aspect that we need for the programming part not to slow us down and stand in the way, is to have a good, pleasant, and reliable development environment setup. Here is a starting, non-exhaustive list of tools you will need.\nFor Windows 11+ users, I strongly recommend you use WSLII – and get familiar with the command line & linux. Between R and Python, I choose technologies which make it easy to draw parallels and transition from one to another.\n\n\n\n\n\n\nFor R (v4.4.2)\n\n\n\n\nRStudio as our main IDE\nQuarto for literate programming\ngit & github\nrenv for managing environments\ndatabase: duckdb (analytical, in-process)\ndata wrangling: tidyverse, data.table (bigger data), arrow\ndata visualization: ggplot, plotly\ninteractive applications: shiny\nAPIs: plumbr\n\nR has many gotchas, which is the main reason that makes the language hard. Therefore, we need to some additional concepts in R, especially functional programming and understanding the S3 object system. Tidyverse is an important ecosystem which tries to solve a lot of the issues in base R.\n\n\n\n\n\n\n\n\nFor Python (v3.10.x)\n\n\n\n\nVScode as our main IDE\nJupyter Notebooks or Quarto for literate programming\ngit & github\nuv (recommended) or conda for managing environments\ndatabase: duckdb (analytical, in-process), sqlite (transactional, in-process)\ndata wrangling: pandas 2.0, pypolars (bigger data), arrow\ninteractive applications: streamlit, shiny, or dash\nAPIs: fastapi",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics",
      "Schedule & Roadmap"
    ]
  },
  {
    "objectID": "04_engineering/roadmap.html",
    "href": "04_engineering/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n  DSc(Data Apps)  --&gt; Data(Data Wrangling)  --&gt; EDA(EDA) --&gt; DV(Visualization)\n  EDA --&gt; LP(Literate Programming)\n  \n  DSc --&gt; Repr[Reproducibility] --&gt; DP[Data Pipelines]\n  DSc --&gt; M[Modeling] --&gt; Pymc(PyMC) --&gt; Tr[Torch]\n  DSc --&gt; MLP[ML Pipes] --&gt; Srv(Serving Models)\n  \n  DV --&gt; FS[/Full Stack Apps/]\n  Tr --&gt; FS\n  Srv --&gt; FS\n\n  Repr --&gt; LP\n  DP --&gt; FS\n  FS --&gt; Dep[Deploy]\n\n\n\n\nFigure 1: We will need to learn a lot of engineering and new tools, so that we’re able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients."
  },
  {
    "objectID": "04_engineering/roadmap.html#module-iv-full-stack-data-apps",
    "href": "04_engineering/roadmap.html#module-iv-full-stack-data-apps",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n  DSc(Data Apps)  --&gt; Data(Data Wrangling)  --&gt; EDA(EDA) --&gt; DV(Visualization)\n  EDA --&gt; LP(Literate Programming)\n  \n  DSc --&gt; Repr[Reproducibility] --&gt; DP[Data Pipelines]\n  DSc --&gt; M[Modeling] --&gt; Pymc(PyMC) --&gt; Tr[Torch]\n  DSc --&gt; MLP[ML Pipes] --&gt; Srv(Serving Models)\n  \n  DV --&gt; FS[/Full Stack Apps/]\n  Tr --&gt; FS\n  Srv --&gt; FS\n\n  Repr --&gt; LP\n  DP --&gt; FS\n  FS --&gt; Dep[Deploy]\n\n\n\n\nFigure 1: We will need to learn a lot of engineering and new tools, so that we’re able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Are you ready for an adventure?",
    "section": "",
    "text": "There are three questions I lose sleep over: What is data science? Why is it important? How should it be taught? The standard “intersection of statistics, economics, and computer science”, answers none. Instead, I find it helpful to think of this applied, quantitative, and interdisciplinary field as decision science.\nIn the context of businesses, we want to improve financial and non-financial outcomes like revenue, profitability (EBITDA), market share, unit economics, production efficiency, customer acquisition, user experience, customer lifetime value, etc. A firm will increase its chances to improve performance if it has an accurate diagnosis of the current state and formulated a good strategy, but it still has to test their ideas and make many decisions under uncertainty.\nA decision scientist should collaborate with domain experts, decision-makers, clients, and stakeholders to understand their domain and challenges, ask the right questions, and frame the problem space (formulate a problem). Then, they will collect data, build statistical models, and apply optimization algorithms which bring insights into consequences of actions, interventions, and policies. 1 These insights, inferences, and predictions will inform the firm which decisions are more promising and whether their hypotheses hold up to evidence.\n\\[Question \\longrightarrow Model \\longrightarrow Insight \\longrightarrow Action \\longrightarrow Outcome \\]\nNot gonna lie, this task and endeavor is not easy at all. In this website, I will explain how to study and master the fundamentals of statistical modeling, business economics, and programming for data analysis which will help you become more effective at problem-solving. These skills will be valuable in the majority of career paths you might choose.\nLast, but not least, I aspire to teach people how to navigate this interdisciplinary landscape and gain an understanding of how statistics, machine learning, econometrics, operations research, AI, cognitive science, and scientific method fit together. 2\nThis website is not a self-contained course or book, but a vision, philosophy, and concrete roadmap for learning and teaching decision science. It emerges out of:\nA lot has been written about the journey from novice to expert and the process of mastery in programming, chess, tennis, painting, music, etc. Decision science is challenging due to its breadth across disciplines and depth (starting from fundamentals). It will require two kinds of intentional practice and active learning over a long period of time:\nIn contemplation, we try to see how fundamental theoretical ideas translate into real-world stories and solutions to practical problems. In service of understanding, it becomes conceptual at times, but I hope you bear with me until you see the benefits of those abstractions. We also focus on understanding the problem space in various domains, our clients’ needs and challenges – which enables us to ask the right questions, formulate the problem correctly, then pick the right model and methodology for the task. Last, but not least, we’ll gain insights into pitfalls and common mistakes to avoid.\nOn the other hand, engineering in the trenches is a skill. The only way to become better at modeling and data analysis is hands-on practice. It is not sufficient to know about the modeling workflow and methodology. In practice, we have to develop pragmatic solutions and test, implement our ideas in code. If we are to increase our chances of success, we’ll also have to follow best practices of reproducible research, automation, and model operationalization.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#why-is-this-journey-rewarding",
    "href": "index.html#why-is-this-journey-rewarding",
    "title": "Are you ready for an adventure?",
    "section": "Why is this journey rewarding?",
    "text": "Why is this journey rewarding?\nYou might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, deep reinforcement learning models are beating people at go, Dota, chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week. There are so many cool applications of AI and modeling.\n\n\n\n\n\n\n\n\n\nHow do I keep up?\n\n\n\nYou don’t, at least if you want to have a balanced life. That’s why I choose to focus on fundamentals which stood the test of time. You will be surprised how many problems that businesses encounter can be solved well with simple, even linear models.\nThese (statistical) fundamentals are anyways a prerequisite before diving into understanding the technicalities of those cutting-edge models and systems. I have nothing against deep learning, but we need to develop the wisdom to use the right tool for the right job.\n\n\nI want you to realize, that despite all the (justified) hype around deep learning and generative AI – well established fields of statistics, econometrics, causal inference, operations research, control theory, dynamical systems, cognitive science, and computer science have been evolving as well. These are not just prerequisites and intellectual forerunners for AI, but tools routinely and successfully used in a large problem space.\n\n\n\n\n\n\nWhy study all of this?\n\n\n\nWe live in a volatile, uncertain, complex and ambiguous world,4 but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence, and robust predictions.\nPeople study the fields outlined above in order to be well equipped for such challenges. For a more in-depth explanation of prerequisites, purposes of each subject, and its relevance for decision science, read section iv.\n\n\n4 VUCA: a mental model to better understand the worldIn businesses, data science and AI can have a function of decision-making support, process improvement, or be an essential part of the system/product itself, like in the case of Uber, Amazon, Netflix, Spotify, Google and many others. To get a better sense of what I mean by decision-making support, I will present a few challenges many data scientists are working on.\nThese problems generally can’t be solved in an “optimal” way by simple rule-based business logic, classical algorithms, or exploratory data analysis. Notice the common thread of optimization at a large scale and that many of these applications are related to important decisions in the value chain of a firm.\n\nDemand forecasting and inventory optimization\nProduction planning and quality control\nRevenue management, pricing optimization, and personalized promotions\nEstimating the impact of advertisement, marketing mix modeling, and conversion optimization\nCustomer churn, repurchase, engagement, LTV\nFraud detection, credit default prediction, insurance risk\nChoice modeling, recommender systems, targeting and uplift models\nImproving products, assortment, and merchandising\n\nDon’t worry if you’re not familiar with some of the applications or that it’s not clear yet which models and fundamentals will be used. We’ll discuss it in great detail in the first module called “Business School for Data Scientists”.\n\nI want you to take away one thing, that is “AI” and Data Science in Businesses boils down to: Decision-Making under Uncertainty at Scale\n\nEven if you are not a decision scientist, you will work with them in one form or another (Data Scientist, Statistician, Researcher, Quant, Data Analyst, Business Analyst, ML Engineer, Data/BI Engineer, Domain Expert). Therefore, you have to understand their language, what are they doing, how to ask and make sure they solve the right problem for you.\n\n\n\n\n\n\nThink of youself as a business person with superpowers\n\n\n\nThis is the best advice I ever got as a novice data scientist and it will be really helpful for people who care a lot about the technical aspects of statistical and machine learning models.\nYour superpower is the ability to solve problems at scale and answer difficult questions by building models and programming. However, you should always put yourself in the shoes of the client and deeply understand their domain, data, objectives, tradeoffs, and decisions. Communicate persuasively, in their language.\nThis mindset will ensure that your solution is valuable and used. Otherwise there is a risk of it being interesting, but not understood, not trusted, not actionable and thus, not adopted.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#dont-travel-without-a-map",
    "href": "index.html#dont-travel-without-a-map",
    "title": "Are you ready for an adventure?",
    "section": "Don’t travel without a map",
    "text": "Don’t travel without a map\nWhen you travel to an unknown place, you need a map to know your location and where are you going. Consider this guide a conceptual frame which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea years later, with greater wisdom and skill – to unlock its real power. We should embrace the fact that learning is not linear.\nAt the risk of annoying you with my metaphors, let me explain what is wrong with our existing maps. Some are too simplistic and low resolution, resulting in too much wandering around. Others are not maps at all, but a sequence of subjects to study – which misses the interdisciplinary nature of decision science. At last, many are too detailed, overwhelming, and unclear about what is most important.\n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it fits together or how can it be implemented in practice to bring value to organisations, that is: be useful\n\n\n\n\n\n\n\n\nOverwhelming amount of content\n\n\n\nIf you go to the data science section in Coursera, you will see a hundred pages of courses and specializations. The situation is even worse with the amount of available tutorials.\nA student is guaranteed to get lost in details and be overwhelmed by all those 800 page hardcore textbooks required by some classes. On the other hand, seemingly pragmatic, cookbook and bootcamp-style approaches miss depth, nuance, key theoretical ideas, and methodological aspects.\nInevitably, you have to use the recommendations provided by other people. I really hope you will like the references I provide, which were carefully curated during the past 10 years. Almost all are free, open-source, and supplement the lectures / theory with code examples.\n\n\nThis is the course I wish I had when starting my journey in data science, which would prepare me for the realities of industry, often very different from the academic world. In the following sections and chapters, I will outline a powerful set of fundamentals, the common threads and connections between them, where to read or watch to gain understanding, and how to practice in order to develop your skills.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#what-is-a-model",
    "href": "index.html#what-is-a-model",
    "title": "Are you ready for an adventure?",
    "section": "What is a model?",
    "text": "What is a model?\nBy now you’ve heard the word “model” a lot of times. In the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. Reality is overwhelming and combinatorially explosive in its possibilities. 5\n5 Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etcWe build models, because we want to understand a system and capture / explain the essential aspects of the phenomena of interest, in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen.\nA model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions. I know this sounds very abstract, but be patient, I’ll formalize what a statistical model means and give plenty of practical applications and examples. Until then, keep the following metaphors in mind:\n\n\n\n\n\nWhat does Pollock and Picasso have to do with statistical modeling and quantitative questions?\n\n\n\n\n\n\n\n\nPhilosophical detour on latent processes\n\n\n\nOne of the most important ideas in statistics is that the causes are not found in data, but in scientific theories. In other words, the answer to our quantitative question is an unobserved, latent quantity or process. We collect data and perform experiments in order to infer those parameters. Hence, the data is the phenomenon, at the surface (Pollock’s canvas) and we would like to know what mechanism could plausibly generate it.\nOn the other hand, models are useful because if successful, they separate the noise from the signal and synthesize all the relevant information we have in our sample. This generalization (abstracting away from idiosyncrasies of each observation) allows us to make predictions and gain insights. Think of the way Picasso drew a camel – lots of artists will agree that simplification is also beautiful.\n\n\nWe collect data, perform experiments, and build models in order to minimize the effect of our biases and foolishness. It’s also important to remember that all models have assumptions, pressupositions, and limitations. They are little machines, golemns of prague which follow instructions precisely, and can backfire if used outside their range of validity. All models are wrong, but some are useful. They can be powerful, but lack wisdom, which is found in your domain / problem understanding and scientific thinking.\n\n\n\n\n\n\nThree challenges in statisical modeling\n\n\n\nA. Gelman highlights three different aspects of statistical inference. Always remember this when designing your study or learning about a new statistical tool! We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.\n\\[Sample \\longrightarrow Population\\]\n\\[Treatment \\longrightarrow Control\\]\n\\[Measurement \\longrightarrow Construct\\]\nThe holy grail is to build statistical models based on the causal processes informed by theories and hypotheses. If we take into account how we measured,6 and collected data, we’ll increase our chances to generalize our conclusions and will have stronger evidence.\n\n\n6 We’ll dedicate one lecture on measurement, but you might have whole specialized classes on this topic",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites-and-background",
    "href": "index.html#prerequisites-and-background",
    "title": "Are you ready for an adventure?",
    "section": "Prerequisites and background",
    "text": "Prerequisites and background\nI think this roadmap will bring the most value to masters’ students, professionals, and students in their last year of BSc. If you never had a linear algebra, calculus, statistics, probability, and programming classes (or are very rusty in them) – read section iv. for a guide on how to integrate these foundational subjects into your study plan.\nMost people I know studied statistics and probability at some point, but the presentation was highly theoretical and without any programming or data analysis. I propose we go back to the most important ideas, discuss their practical relevance, and really understand them by coding up the simulations for our real-world stories. We will see that statistics is not just useful, but can be a lot of fun!\n\n\n\n\n\nPractice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!\n\n\n\nFor Linear Algebra and Calculus – only exposure is needed, but competence and mathematical maturity will help a lot. It is a personal choice how deep to dive, but generally, it will make your journey much easier.\nFor Probability Theory – competence is needed, even though we start from the very beginning with a review of combinatorics. I suggest you read along and practice with a more comprehensive resource, like Joe Blitzstein’s “Probability 110”. As your understanding of probability grows, you can tackle more challenges.\nFor Mathematical Statistics the story is the same as for Probability. You will need at least to be familiar with estimators, sampling distributions, LLN/CLT, hypothesis testing and regression. This course attempts to clear up misunderstandings so that you don’t fall into common pitfalls of statistical practice.\nPython or R programming for data science is mandatory in order to do anything even remotely sophisticated in practice. 7 Besides the basics of the language, we need to develop competence in data wrangling, visualization, SQL / databases, and reproducible data analysis. I recommend two free books:\n\n“R for Data Science” by Hadley Wickham\n“Python for Data Analysis” by Wes McKinney.\nOf course, the more experience you have in one or both, the better\n\nIf you have to deal with financial statements at your job or have an interest in finance, I strongly suggest you check out A. Damodaran’s crash course on accounting and corporate finance. Depending on your job, knowledge of microeconomics, marketing, management, operations, and logistics might be essential.\n\n7 Most universities introduce it far too late, but things are changing. I would argue that one has to start coding and analyzing data from stats 101 and linear algebraBesides technical prerequisites, there is a list of concepts and ideas from research design, which can be helpful in practice, but are often not part of statistical curriculum. I highly recommend A. Zand Scholten’s introductory course on Quantitative Methods. It is worth to invest a bit of time to get familiar with the following:\n\n\nTaken in isolation, topics like measurement and sampling are very abstract. Therefore, they have to be integrated into the case-studies whenever they bring added value. This is not easy, so .. work in progress\n\nMeasurement is especially relevant and tricky in social science. Sometimes, we do not measure what we think we do (underlying construct). For measuring abstract things like happiness or personality, we need to dive into scales, instrumentation, operationalization, validity, and reliability.\nSampling isn’t limited to simple random samples, as these rarely occur in practice. We need to be aware what tools exist so that we can generalize to the population of interest. For example, weighting and post-stratification.\nThreats to validity, including confounds, artifacts, and biases. On the one hand we use statistics as a guardrail against foolishness, on the other hand there a lot of the same problems that can compromise our study.\nReproducible research and literate programming has become a de-facto standard in the data science world. We have amazing tools right now, but it’s still not easy to achieve end-to-end reproducibility and automation.\nAcademic writing is an important skill, even in businesses. Writing is both a way of thinking, problem-solving, and communication. In my opinion, the structure of a scientific paper is very helpful to crystalize your ideas.\n\nLast, but not least, there is value in understanding the scientific process, in its broad sense. In section iii., I adapt it to the context of decision science in businesses and show how most processes and workflows like CRISP-DM, Tuckey’s EDA are a part of it.\n\n\n\nThis poorly drawn diagram is a way to visualize the prerequisites (top and bottom rows) for the three modeling and problem-solving approaches which we will develop in the following modules.\n\n\nWhat can you expect from the three modules? You probably heard the distinction between descriptive, predictive, and prescriptive analytics. I want to propose a better way of thinking about the problem-solving approaches available in our toolkit.\nWe leverage these prerequisites to deeply understand statistical inference applied to generalized linear models and complement it with causal thinking. This foundation will enable us to respond to the 3 challenges in statistics: generalizing from sample to population, from treatment to control group, and from measurement to the underlying construct / causal effect.\nIn other words, we will learn how to design experiments to test our hypotheses and the effects of interventions, make predictions to optimize processes, and make inferences about the (potential) causes of the observed performance. 8 Each of these approaches have their own concepts, ideas, tools, methodologies, processes, and workflows – with some particularities, depending on the target domain.\n8 I’m leaving out optimization algorithms, which are very important, but more are more of an operations research and computer science topic",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#module-1-business-school-and-methodology",
    "href": "index.html#module-1-business-school-and-methodology",
    "title": "Are you ready for an adventure?",
    "section": "Module 1: Business School and Methodology",
    "text": "Module 1: Business School and Methodology\nRemember the advice “think of yourself as a business person with superpowers”? It means that as a decision scientist, it’s helpful to think question, action, and outcome first – and only then about models. In other words, we have to deeply understand our client’s business domain and problem space. The best way to achieve that, is experience and collaboration on the job. Therefore, my role is to tell as many stories from the trenches, so that you know what to expect in practice.\nA good starting place is thinking about the purpose of a business from different perspectives. Then, identify key decisions that firms have to make across many industries and domains. This exercise will show us how widespread are data science applications and use-cases, even in places we don’t suspect.\nAt this point it is important to clarify what we mean by AI, ML, analytics, data mining, statistics and when is it appropriate to choose one approach over another. You’ll be surprised, but statistics is the hardest to define and often misunderstood, because of its inferential, experimental, and causal facets.\nNext, we’ll learn a few frameworks for understanding a firm’s performance evolution, value chain, strategy and tie them all together in an analyst’s workflow. These tools will help us ask good (statistical) questions and increase the chances that our modeling efforts result in actionable insights.\n\n\nIn time, I’ll add many more case-studies from advertisement, promotion, merchandising, engagement, CRM, conversion rate optimization, demand planning, supply chain, etc.\nAn essential aid in this pursuit of improving decision-making at scale, are different processes and methodologies for machine learning, experiment design, causal inference, and exploratory data analysis. It is important to mention that methodology is not a recipe, but way of structuring our work, a set of guiding principles and constraints over the space of all the things that we could do in analysis. Don’t think of these constraints as limiting your freedom, but as helpful allies in effective problem problem solving.\nThese methodolgical fundamentals are not “just theory”, it is what will make or break projects in practice. There are so many pitfalls in ML and statistics that we cannot afford to do it ad-hoc. In my teaching, I try very hard to bring back the scientific process and methodology into “data science”. Understanding and applying a statistical tool or model by itself is not sufficient. We need case-studies in which we put the business context, decisions, domain understanding, and working software at the forefront. As R. McElreath sais, science before statistics!\n\n\n\n\n\n\nWhy most AI/ML projects fail?\n\n\n\nA lot of attention has been paid in the industry to the software engineering aspects of machine learning as one of the causes of failed projects. In the last 5 years, a lot of tools and best practices have emerged which can help us mitigate those risks and increase our chances of successful operationalization.\nIn my opinion, a bigger problem is methodological and organizational: a mismatch between business objectives, actions, constraints, tradeoffs, domain specificities vs modeling. This is why the ability to ask good questions, to formulate a problem, and think scientifically is critical. Therefore, an important goal of my writing is to recognize and avoid adhockery. 9\nThere is one more problem you have to be aware of, which I call the “Kaggle phenomenon”. Despite Kaggle competitions being an excellent platform for honing ML skills and developing better models, I think it gave too many people the impression that this is what data science is about. Kaggle misses the most important aspects of problem-solving and it is devoid of most of the original business context and decision-making.\n\n\n9 Adhockery in modeling, in product-management, in data-mining, and software engineeringIf you worry that the business school is too conceptual – don’t despair, as we’ll have a few hands-on, technical lectures – involving math, modeling, and programming. We will come back to these problems after getting the required background in probability and statistical modeling.\n\nI use the newsvendor problem as a starting point in modeling and optimization for demand planning\nThe pricing optimization as a starting point towards revenue management\n\n\n\n\n\n\n\nGo to “Study Guide” pages for practice!\n\n\n\nConceptual understanding by itself is not enough. So, I curated a list of resources to practice on interesting case-studies, datasets, which directly apply the models, tools, and methodologies presented. These are written by experts in the field, are usually well thought, easy to follow, reproducible, and highlight important aspects of a problem and model.\nAlso, keep an eye on the course github repo, in which we’ll do a lot of simulations, some exciting projects (full stack data apps) and investigate common challenges with a fresh perspective.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#module-2-probability-and-statistics",
    "href": "index.html#module-2-probability-and-statistics",
    "title": "Are you ready for an adventure?",
    "section": "Module 2: Probability and Statistics",
    "text": "Module 2: Probability and Statistics\nI think that most students will benefit from re-framing the fundamentals of probability and statistics. A lot of mistakes in practice come from a fundamental misunderstanding of the nature of statistical inference. If we view statistics as changing our minds and actions in the face of evidence – the good ol’ t-test will shine in a new light. It will become clear why those models and procedures were invented in the first place.\n\n\nAnother benefit of revising these fundamentals is that we can develop an appreciation for the stories and ideas behind these methods. Also, it can be fun.\nWe start with combinatorics and urn models, probability trees, distributions, conditioning, laws of large numbers, central limit theorem, Bayes’ rule. For each one of these topics, I present the story, use-case, and hands-on simulations to show that even very simple tools can be useful.\nPerhaps the most important lecture is also very abstract, where I formally define what is probability, a random variable, what is the key difference between probability and statistics. Understanding what is collectivity, statistical population, sample; estimands, estimators, estimates; and formally, what is a statistical model – will give you the right terminology and set you on a good path towards more advanced models.\nThe module culminates in practical aspects of experiment design and A/B testing, sample size justification, power analysis, and a conversation about the causes and remedies to the replication crisis. After all this effort put into the study of fundamentals, you can go with ease and confidence into Bayesian Statistics, Machine Learning, and Causal Inference.\n\n\nWhen discussing hypothesis testing, it is very important to recognize that most statistical tests can be framed as linear models.\n\n\n\n\n\n\nThe art of formulating a hypothesis\n\n\n\nIn statistics’ classes, the problem is usually completely framed and students focus on computation / interpretation. Experiment design in practice is tricky and more complicated than it looks in introductory hypothesis testing. We will use Cassie Kozyrkov’s 12 steps of statistics to make sure that we don’t forget some important aspect or potential pitfall.\n\n\nMy preferred sequence is to go in parallel with the “business school” and statistics, since there is a great synergy. The prior focuses on the problem space and processes of analytics, ML, statistics. I also recommend in case of a very short and intensive course to study bayesian generalized linear models before the experiment design topics.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#module-3-applied-bayesian-statistics",
    "href": "index.html#module-3-applied-bayesian-statistics",
    "title": "Are you ready for an adventure?",
    "section": "Module 3: Applied Bayesian Statistics",
    "text": "Module 3: Applied Bayesian Statistics\nOnce we have a confident grasp of the fundamentals, we continue on the path of applied Bayesian statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. We will learn new tools to deal in a principled way with missing data, measurement error, sampling biases, nonlinearities, discreteness, dynamics, and heterogeneity.\n\n\n\n\n\n(Source: R. McElreath – Science before statistics)\n\n\nInstead of applying an out-of-the-box model, we will build custom ones for each individual application in an iterative way. Choosing appropriate probability distributions and modeling assumptions will be critical in this process, along with model critique and validation. There are many ways to teach bayesian statistics and generalized linear models, but I think R. McElreath has the best approach, by giving a lot of importance to DAGs of influence, simulation, and ideas from causal inference.\n\n\n\n\n\n\nHierarchical Generalized Linear Models\n\n\n\nMany experts in the field argue that this should be the default way we do regression analysis and that we need a strong justification to show that a simple linear regression is the appropriate model.\nThis modeling approach will enable us to overcome the limitations of simpler models encountered before. Handling correlated and multilevel data is essential to make efficient inferences and draw correct conclusions.\n\n\nOne notable difference between the Bayesian approach and the traditional way advanced econometrics is taught, is that we will focus on computation instead of proofs and heavy-duty mathematical statistics. In a sense, it means that we work with a single estimator which updates our prior beliefs about parameter distributions. We declare the model and the sampler does the job for us! If it explodes, we probably mis-specified the model or priors.\n\nNotes on Causal Inference and ML\nDid you get comfortable with building custom statistical models for inference and prediction? What about decisions with high stakes, where we sometimes want to do randomized experiments?\nOften, A/B tests and randomized controlled trials are unfeasible or unethical. Also realize that we cannot reach a causal conclusion from observational data alone – we can talk just about associations. We need a theory, which is our understanding of how the “world” works – translated into a statistical model, plus data, which will give us new insight into the causal processes (the evidence).\nThis motivates a deep-dive into the field of causal inference. Think of it as a link between the theoretical models and observational data, where we sometimes can take advantage of certain “natural experiments”. Causal inference requires deep thinking and understanding, which is truly challenging – an art and science, in contrast with the “auto-magic”, unreasonably effective pattern recognition of ML.\nCausal thinking is useful in description, inference, and prediction, as we need to know the causes of what makes our sample different from target population (for example, differential non-response, selection bias, or non-probability sampling). It also gives us a principled approach to deal with missing data, measurement error, and prevents us from drawing wrong conclusions when exploring data.\nOne of the most important lessons is to be very careful when using ML to make predictions, when we actually want to intervene in a system. It can be shown that a model optimized for prediction can suggest nonsensical interventions and that a causal model can make subpar predictions.\n\n\n\n\n\n\nWhat if I care only about ML?\n\n\n\nEven if you’re interested only in machine learning, most practitioners will emphasize the importance of mastering regression (often starting with generalized linear models) and doing A/B tests resulting in evidence that our new model brings an improvement (or not).\nThis is how we jump through various buckets, highlighting the golden thread linking them all: decisions and uncertainty. Moreover, the tools we learned in Bayesian Statistics are directly applicable in ML – the lines between these two fields are blurry. This is especially the case when considering flexible models like Gaussian Processes and Bayesian Additive Regression Trees.\n\n\nOther times, we care not just about a single decision or developing better policies, but we have to make tons of little decisions at scale. This is when we switch to a predictive, Machine Learning perspective and walk through our workhorse models, which should serve us decades ahead in a wide range of applications: both predictive and exploratory.\nThere are other miscellaneous topics dear to me and usually not covered in such courses: demand forecasting, recommender systems, and natural language processing. All extremely useful in business contexts, but significant tweaks are needed to the models discussed before.\n\n\n\n\n\n\nProject: 5-page applied research paper\n\n\n\nAfter finishing the first three modules, a good chance to put the knowledge to practice, is to write a 5-page research paper on a question / problem you’re passionate about.10 This will test your study design, modeling, programming, and writing skills.\nWe will see how easy is it to publish your article / paper / report with quarto and github pages.\n\n\n10 In the labs, I give detailed instructions on how a good project should look like, starting from structure and ending in presentation\n\nFull-Stack Data Apps\nI mentioned before that a key prerequisite is competence in Python/R, SQL, data wrangling, EDA, visualization, and literate programming. There are many wonderful resources to learn and practice these skills. During the labs we’ll build from the ground up a tech stack for reproducible data analysis, model and data pipelines.\nIf you got to this point and want to continue, you will probably move onto Machine Learning and Causal Inference. There is, however, one huge problem when it comes to bringing added value in businesses – operationalizing models.\nI have been speaking at conferences about this topic for 6 years: from data, MLOps, product management, and organizational perspectives – and it’s still very relevant. Of course, the way we operationalize a model and build a system depends on the use-case and many factors – however, the most general case to me is via a “Full Stack Data-Driven App” (with user interface, backend, database; data, training, and serving pipelines).\n\n\n\n\n\n\nBuild an impressive project for your portfolio\n\n\n\nA full-stack data-driven app is your final project and something you can brag about in your portfolio and github profile. It sounds complicated, but we have the tools and frameworks to make it easy for us. This will force you to think about the user need and how your software product solves their problem.\nDon’t worry about getting everything perfect, but focus on a problem and single area from the course you’re passionate about: it could be statistical modeling, ML pipelines, or even frontend development and data visualization.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Are you ready for an adventure?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI mentioned that my teaching and industry success stands on the shoulder of giants. This is a metaphor scientists use to emphasize that we incrementally build on the work of others.\nWhen it comes to statistical modeling and machine learning, the following people had a large influence on the way I think and in turn, I will refer you to their excellent books and courses: Trevor Hastie, Bradley Efron, Yaser Abu-Mostafa, Shai Ben-David, Richard McElreath, Andrew Gelman, Jennifer Hill, Aki Vehtari, Stephen Boyd, Gilbert Strang, Steven Burton, Nathan Kutz, Cassie Kozyrkov.\nAt last, I have to explain why I’m not writing a book yet and linking you instead to the best resouces I know of. I think we have more than enough excellent books to learn the theory and develop skills in the fields within decision science.\nFrom the point of view of teaching, the only reason to write my own book is the following: we need engaging, high-quality, in-depth, hands-on, realistic case-studies. 11 We have more than enough tutorials and toy examples. Often, the only criticism I can address to some of the books is that the code examples are applied to boring and overused problems and datasets. It is not a fair criticism, since their main goal is teaching how the models work. Moreover, no one will publish a thousand page textbook.\n11 Ok, enough hyphenated adjectives, you get the point12 A cheeky reference to the no free lunch theoremNo one has time to study it all and there is no single book which does everything best.12 So, by cherry-picking lectures, chapters, examples, and applications; then arranging them into a coherent roadmap, we get different perspectives and will start developing preferences for one way of teaching and one approach over another.\nThe cleanest code associated to applications is not always written by the authors. Often, the open-source community significantly improves it and we’ll take advantage of this fact. Sometimes, I don’t like any code and will do it myself, but this is really, polishing an existing idea rather than an original contribution.\n\nThe best kind of applications are relatively simple, but interesting and pose real modeling challenges. They capture the essence of the problem. This is why I heavily rely on Richard McElreath’s “Statistical Rethinking, 2nd ed.” and “Bayesian Data Analysis, 3rd ed.” by Andrew Gelman et. al. \n\nIn my opinion, writing such case-studies takes more time than actually solving the problem inside a firm. I will do it, someday. In general, after you studied from the linked materials, tried to write the code without reference, and understood the examples in depth – it is your responsibility to come up with a project or case-study. It can be on openly available or private data, on messy, real data, or your own, clean simulations. It’s a good idea to choose a domain, problem, and question you’re passionate about.",
    "crumbs": [
      "| 1. Business |",
      "i. Introduction"
    ]
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "Course philosophy",
    "section": "",
    "text": "Decision science is by definition interdisciplinary, which makes studying it challenging, interesting, and rewarding. We will need to borrow ideas, models, and techniques from probability, statistics, causal inference, machine learning, optimization, economics, cognitive science, etc. This means jumping through many fields, which we’ll call aspects or “buckets”.\n\n\n\n\n\n\nThe danger of thinking in buckets\n\n\n\nOur brains think about stuff by creating boundaries, i.e. ‘buckets’ around ideas. These buckets can influence our memory, language, behavior, and ability to see the ‘big picture’. 1\n\nAn implication of our bucketing minds is that we are bad at differentiating facts that fall within the same category. Two shades of red are labelled ‘red’.\nA larger consequence is that when we focus on categories while talking about behavior, we tend to exagerrate differences and lose out on the big picture\nIt’s easy to see a single one of these categories as providing The Explanation. But they are merely various “behavior buckets”. They are all a part of the big picture explanation.\nIt is an easy trap to fall into. Flawed bucket thinking has been done by many of the most influential scientists in history, sometimes, with catastrophic consequences.\n\nA major goal is not to fall for bucket thinking – we must resist the temptation to find “The Explanation” in one bucket. Robert Sapolsky explains behavior in terms of evolution, neuroscience, molecular genetics, ethology, etc.\n\n\n1 I first encountered this idea in one of R. Sapolsky’s lectures / notes and it had a huge impact on the way I look at science.Imagine we’re exploring a map by travelling through various islands: familiar or unknown places. Sometimes, we’ll take a little detour for a fun fact, other times we need to settle for a while and master a craft. Maybe, it’s not enough to talk to locals, but we have to live among them in order to understand their challenges. If we don’t have a map and guide, we might not be even aware that we’re missing important knowledge and understanding from an unexplored place.\nProblem space is the land of challenges. Our goal here is to understand the domain where we have to make decisions, figure out how to improve relevant outcomes for clients and stakeholders. There is much uncertainty here, questions about what will happen and how should we act. It’s the real world, seen as a Complex Adaptive System (I will explain what this means in the next chapter). This is where we get our data from and who we build software for!\n\n\n\n\n\nWe will take short trips in the fields outlined below. (Source: Generated with DALL-E)\n\n\nScience, especially cognitive science, which will give us insights about our intelligence, rationality, wisdom, foolishness, and biases. This is the place where we’ll get the process, method, learn how to observe, formulate scientific hypotheses, use theories, and causal models to make predictions and perform experiments.\nIn probability, we reason about uncertainty in the real world, build narratives and tell stories with DAGs (directed acyclic graphs) of random variables, which are little machines generating data. In the happy case they match theoretical models and result in plausible patterns. We’ll spend much time simulating (economic) phenomena, being the masters of these alternative multiverses.\nIn statistics, we change our minds and actions in the face of evidence. We learn the skills of exploratory data analysis, experiment design, and causal inference. Why build models? To make better decisions, of course.\nMachine learning and deep learning, the younger tribes of statistics are the future: they learn from data and when things go well, make reliable and robust predictions, in order to optimize the heck out of any process. Think of them as shamans or oracles, who sometimes overfit by seeing patterns which are not real, therefore are prone to acting foolishly.\nWe come back to the homeland of many of you: computer science and software engineering, the place where nowadays everything on this map becomes reality. We will learn how to build full-stack, data-driven software, good practices of the guild. While spending time here, we will develop an appreciation for the contribution of CS to all other places we already visited.\nAh! We forgot about mathematics. It is an essential prerequisite for everything we do, however, it is hard to do rigorous mathematics in the setup we outlined, as it will take a decade. The good news is, we will be fine with the starter pack!\nLast, but not least, do not underestimate philosophy. It will help us reason about the ethical aspects of AI and ML, teach us how to evaluate an argument, understand the limitations of our methods, the nature of evidence, and more importantly force us to make our assumptions explicit.",
    "crumbs": [
      "| 1. Business |",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#on-interdisciplinarity",
    "href": "philosophy.html#on-interdisciplinarity",
    "title": "Course philosophy",
    "section": "",
    "text": "Decision science is by definition interdisciplinary, which makes studying it challenging, interesting, and rewarding. We will need to borrow ideas, models, and techniques from probability, statistics, causal inference, machine learning, optimization, economics, cognitive science, etc. This means jumping through many fields, which we’ll call aspects or “buckets”.\n\n\n\n\n\n\nThe danger of thinking in buckets\n\n\n\nOur brains think about stuff by creating boundaries, i.e. ‘buckets’ around ideas. These buckets can influence our memory, language, behavior, and ability to see the ‘big picture’. 1\n\nAn implication of our bucketing minds is that we are bad at differentiating facts that fall within the same category. Two shades of red are labelled ‘red’.\nA larger consequence is that when we focus on categories while talking about behavior, we tend to exagerrate differences and lose out on the big picture\nIt’s easy to see a single one of these categories as providing The Explanation. But they are merely various “behavior buckets”. They are all a part of the big picture explanation.\nIt is an easy trap to fall into. Flawed bucket thinking has been done by many of the most influential scientists in history, sometimes, with catastrophic consequences.\n\nA major goal is not to fall for bucket thinking – we must resist the temptation to find “The Explanation” in one bucket. Robert Sapolsky explains behavior in terms of evolution, neuroscience, molecular genetics, ethology, etc.\n\n\n1 I first encountered this idea in one of R. Sapolsky’s lectures / notes and it had a huge impact on the way I look at science.Imagine we’re exploring a map by travelling through various islands: familiar or unknown places. Sometimes, we’ll take a little detour for a fun fact, other times we need to settle for a while and master a craft. Maybe, it’s not enough to talk to locals, but we have to live among them in order to understand their challenges. If we don’t have a map and guide, we might not be even aware that we’re missing important knowledge and understanding from an unexplored place.\nProblem space is the land of challenges. Our goal here is to understand the domain where we have to make decisions, figure out how to improve relevant outcomes for clients and stakeholders. There is much uncertainty here, questions about what will happen and how should we act. It’s the real world, seen as a Complex Adaptive System (I will explain what this means in the next chapter). This is where we get our data from and who we build software for!\n\n\n\n\n\nWe will take short trips in the fields outlined below. (Source: Generated with DALL-E)\n\n\nScience, especially cognitive science, which will give us insights about our intelligence, rationality, wisdom, foolishness, and biases. This is the place where we’ll get the process, method, learn how to observe, formulate scientific hypotheses, use theories, and causal models to make predictions and perform experiments.\nIn probability, we reason about uncertainty in the real world, build narratives and tell stories with DAGs (directed acyclic graphs) of random variables, which are little machines generating data. In the happy case they match theoretical models and result in plausible patterns. We’ll spend much time simulating (economic) phenomena, being the masters of these alternative multiverses.\nIn statistics, we change our minds and actions in the face of evidence. We learn the skills of exploratory data analysis, experiment design, and causal inference. Why build models? To make better decisions, of course.\nMachine learning and deep learning, the younger tribes of statistics are the future: they learn from data and when things go well, make reliable and robust predictions, in order to optimize the heck out of any process. Think of them as shamans or oracles, who sometimes overfit by seeing patterns which are not real, therefore are prone to acting foolishly.\nWe come back to the homeland of many of you: computer science and software engineering, the place where nowadays everything on this map becomes reality. We will learn how to build full-stack, data-driven software, good practices of the guild. While spending time here, we will develop an appreciation for the contribution of CS to all other places we already visited.\nAh! We forgot about mathematics. It is an essential prerequisite for everything we do, however, it is hard to do rigorous mathematics in the setup we outlined, as it will take a decade. The good news is, we will be fine with the starter pack!\nLast, but not least, do not underestimate philosophy. It will help us reason about the ethical aspects of AI and ML, teach us how to evaluate an argument, understand the limitations of our methods, the nature of evidence, and more importantly force us to make our assumptions explicit.",
    "crumbs": [
      "| 1. Business |",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#cybernetics-and-ai",
    "href": "philosophy.html#cybernetics-and-ai",
    "title": "Course philosophy",
    "section": "Cybernetics and AI",
    "text": "Cybernetics and AI\nWhat is cybernetics and how is it relevant to the course philosophy? Does it have something to do with robotics or cybersecurity? Not really. It is studying complex adaptive systems (CAS) and is very close to what we call today weak or specialized AI. Does the idea of “decision-making under uncertainty at scale” ring a bell?\nThe term comes from greek (kybernetes) – to steer towards a desired outcome or trajectory. It highlights the aspects of agency, action, pattern recognition, and prediction.\n\n\n\n\n\n\nUnpacking the definition of Cybernetics\n\n\n\nThe science of general regularities of control and information processing in animals, machines, and humans 2\n\nControl means goal-directedness, the ability to solve problems and achieve objectives by taking action and stirring the system towards a trajectory. The goal can also be perserving the structural-functional organization of the system itself, an autopoesis.\nInformation processing can be interpreted as pattern recoginition, perception, how you understand and model the world, what inferences do you draw based on “data inputs”\nGeneral regularities means what is true and plausible of control and information processing across fields and a variety of complex systems, not only in particular cases.\nAnimal refers to applications in biology, machine – in engineering, and human – in our society and behavior.\n\nWhen applied to economics, cybernetics is ultimately concerned with human behavior. It views consumers, households, firms, institutions, markets, and countries as complex adaptive systems.\n\n\n2 There are many definitions of Cybernetics, but I find this one by P. Novikov to be the most useful for usIf we are to put decision science on a philosophical foundation, ideas from cybernetics, cognitive science, scientific process (philosophy of science), and ethics should be included. I don’t do this for the sake of rigor, but because it is an useful way of thinking in practice. These four aspects appear again and again in my teaching and applications.\nPerhaps, the most valuable skill from studying cybernetics is the ability to think in systems and bring clarity in messy, dynamic problems. I will not use any of its theory or tools, but it justifies and informs the way I put decision science in a larger context.\n\n\n\n\n\n\nCybernetics and the history of AI\n\n\n\nI highly recommend that you stop here and read two brilliant interviews by Michael I. Jordan, in which he tells the story of how the meaning of AI changed throughout the decades and what role did Cybernetics play:\n\nStop Calling Everything AI, Machine-Learning Pioneer Says\nArtificial Intelligence: The Revolution Hasn’t Happened Yet",
    "crumbs": [
      "| 1. Business |",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#four-principles-for-teaching",
    "href": "philosophy.html#four-principles-for-teaching",
    "title": "Course philosophy",
    "section": "Four principles for teaching",
    "text": "Four principles for teaching\nLet’s go back to the burning question: what I wish I had in a course? It is hard to teach these topics in a way which is both theoretically sound and can be immediately applied. After recognizing that there is no silver bullet, my conclusion is that following the principles outlined below consistently, dramatically increases the chances of preparing a new generation ready to tackle the messy, ill-defined problems we encounter.\n\nProvide motivation for why something is important (a field, theory, model, method, technology), then showcase relatable expamples and the story behind the idea\nDevelop a conceptual undestanding, an intuition about the problem and the “tool” we think is appropriate in tackling it, and where does it fit in the methodology:\n\nPresent the tool theoretically rigorous and sound, but only where it matters.\nMore geometry and graphs over equations, more simulations and stories over proofs. Low-level details need careful, individual, hands-on study\nLeverage previous mathematical, statistical, and domain knowledge\nFor the mathematically inclined, add some elements of abstract math to understand the underlying foundations of these methods and models\n\nHeavily use simulations as a safe playground that we control, to get a feel for the behavior of models and algorithms.\n\nIt forces us to declare our assumptions about the problem and investigate the implications\nBefore we commit to a costly real-life experiment or modeling project, it is essential to know that a model works in principle for our application: is able to recover the parameters, causal structure, and generalize beyond the sample?\n\nDiscuss practical applications and challenges the firms face, from an insider’s perspective. Those are our case-studies, outlined systematically in the following way:\n\nFirst, we try to deeply understand the problem and frame it in “our” language of decision-making. We clarify our assumptions and reduce ambiguity\nThen, we strip the problem to the simplest model, to illuminate some key aspect, like uncertainty in pricing and demand planning decisions.\nThen, and only then, we bring back all the realism, nuances, and complexities. Applications are based on realistic or real data, which can be messy, hard to access, biased and incomplete\nIn projects, we implement software solutions, with the main focus on the real-world challenges of deploying models and improving decision making. Interactive data visualization will help a lot in communicating persuasively.\nKeep in mind best practices from reproducible research and software engineering, so that our apps and research code is easy to maintain and extend\n\n\n\n\nIn teaching and learning, I heavily rely on the Dreyfus model of skill acquisition, which describes the following stages: Novice, Competent, Proficient, Expert, Master. This 10 minutes presentation is well worth your time.\n\n\n\n\n\n\nUnderstand by building it (Feynman)\n\n\n\nI often find myself truly understanding something, only after I code it up and get a sense of the behavior and mechanics of a model. Then, I try to think of how I would apply it in practice in different contexts.\nThis hints to the idea that we need a complementary computational background and set of tools (R/Python/SQL), right from the beginning of higher education.\n\n\nSpeaking of prerequisites, there are some tools we have to dust off the shelves and cultivate an appreciation of their importance: linear algebra, calculus, probability theory and mathematical statistics.\n\n\nThose prerequisites are placed in context of the business practice, with swift reviews and references to resources which should fill in all the gaps.\nAt the same time, we have to gradually get rid of the bad habits that were accumulated: analyses which can’t be reproduced, mechanically following a statistical procedure (because the flowchart of statistical tests said so, for example), jumping to a conclusion (as if there is only one, correct, textbook answer), rushing the learning process, handwaving to cover for a poorly articulated argument. On the other side of the spectrum, perfectionism doesn’t help either, as this field is inherently iterative and experimental.\nTherefore, we need to develop a set of processes and methodologies to iterate and improve effectively. We focus and pay close attention to the process of problem-solving: from formulation to modeling and operationalization.",
    "crumbs": [
      "| 1. Business |",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#lessons-in-humility",
    "href": "philosophy.html#lessons-in-humility",
    "title": "Course philosophy",
    "section": "10 Lessons in Humility",
    "text": "10 Lessons in Humility\nIf you found yourself on this long journey towards mastery, I hope you will find these lessons helpful:\n\nThere is no shortcut to deep understanding\n\nOf a domain, especially in an interdisciplinary setting\nWith communities engaged in an evolving dialogue\n\nThere is no shortcut to being skillful at something\nThe journey from novice to expert is not linear, however, the “interest compounds”\nThe journey need not be painful, but it can be seriously playful, a source of wonder and meaning\nWithout skin in the game, we can’t claim we truly get something\nWithout a vision which is flexible enough, but at the same time long-lived:\n\nIn the case of rigidity - there is a risk of being stuck, pursue obsessively, counterproductively the wrong thing\nIn the case of everything goes - there is a risk of wandering aimlessly and not finding a home\n\nFixating on beliefs and propositional knowing (the facts!) is counterproductive. Which should put into question all written above\nFixating on skills makes you lose the grasp of the big picture",
    "crumbs": [
      "| 1. Business |",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "At my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale. I really believe in the idea of “skin in the game”\n\n\n\n\n\nMihai Bizovi | VP of Decision Science @AdoreMe (acquired by Victoria’s Secret)\n\n\n\nResponsible for AI strategy, decision science, generative AI governance, and systems design. Enabling teams to be more effective and efficient\nHistorically had many hats – statistics, ML and data engineering, AI product management, developing data apps, teaching / mentoring\nCritical applications along the value chain:\n\ndemand forecasting and inventory optimization systems\nrecommender systems and NLP/LLMs for try-at-home businesses\nmarketing, acquisition, CRO experiment design, pricing, CRM"
  },
  {
    "objectID": "about_me.html#whoami-as-engineer-and-leader",
    "href": "about_me.html#whoami-as-engineer-and-leader",
    "title": "Decision Science Course",
    "section": "",
    "text": "At my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale. I really believe in the idea of “skin in the game”\n\n\n\n\n\nMihai Bizovi | VP of Decision Science @AdoreMe (acquired by Victoria’s Secret)\n\n\n\nResponsible for AI strategy, decision science, generative AI governance, and systems design. Enabling teams to be more effective and efficient\nHistorically had many hats – statistics, ML and data engineering, AI product management, developing data apps, teaching / mentoring\nCritical applications along the value chain:\n\ndemand forecasting and inventory optimization systems\nrecommender systems and NLP/LLMs for try-at-home businesses\nmarketing, acquisition, CRO experiment design, pricing, CRM"
  },
  {
    "objectID": "about_me.html#whoami-as-teacher-and-researcher",
    "href": "about_me.html#whoami-as-teacher-and-researcher",
    "title": "Decision Science Course",
    "section": "~whoami: as teacher and researcher",
    "text": "~whoami: as teacher and researcher\nAs a teacher, I aspire to contribute to the understanding of AI’s complex landscape; how to navigate it, develop valuable skills, and become more effective at problem-solving.\n\n\n\n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\nThink of me as a maven (yid: meyvn), i.e. someone experienced, who has been there before and will help you find a good path.\n\nGraduate of Cybernetics (BSc) and Quantitative Economics (MSc)\n\nThesis & Dissertation on Bayesian Microeconometrics\nResearch in Probabilistic Methods for Time Series\nStarted in Systems’ Dynamics and Economic Complexity\n\nSpeaking at conferences, meetups, and panels since 2018:\n\nBDW, DevFest, Google Cloud, CNCF, Romanian AI Days, DigitalShift\n\nTeaching at ASE’s business analytics masters since 2021"
  },
  {
    "objectID": "about_me.html#whoami-as-a-person",
    "href": "about_me.html#whoami-as-a-person",
    "title": "Decision Science Course",
    "section": "~whoami: as a person",
    "text": "~whoami: as a person\nAs a person, I aspire to a life of wisdom and meaning, towards the good, true, and beautiful. Learning how to be present and deeply understanding reality.\n\nCultivating wisdom – commitment to philosophy as a way of life\nPainting as a craft, creative expression, and therapy. Learning about art history and art appreciation in multiple media (music, film, painting)\nReading about cognitive science, evolution, history, and economics\nHiking, hipster coffee, 14 years of pro-ish chess"
  },
  {
    "objectID": "about_me.html#how-does-it-fit-together",
    "href": "about_me.html#how-does-it-fit-together",
    "title": "Decision Science Course",
    "section": "How does it fit together?",
    "text": "How does it fit together?\n\n\n\n\n\nsynoptic is to take the best of multiple perspectives, reduce conflict and contradiction – then see where the evidence converges\n\n\nOn another note, my interests in AI, art, cognitive science, and philosophy have an aspect of science, craft, worldview, and deep participation, engagement. Moreover, they are deeply interconnected and I would even say, synoptically integrated. It is important to me that these conceptual frames fit into a coherent whole and contribute in a practical way towards a good life.\nI can’t emphasise enough the importance of different levels of “knowing”: propositional, procedural, perspectival, and participatory – as it is not enough to know the facts (or have beliefs), but to know how to do something, to have a perspective of the “world” and a sense of participation in whatever you’re engaged in.\nI mean that we’re agents in different arenas of life and the sense of meaning comes from an attunement to those arenas. We participate in a course of something, which has impact on the environment, which changes us and how we view and relate to the world, self and others.\nUnfortunately, I have none of the answers of how does Intelligence, Rationality, Wisdom contribute to meaning in life – at best, some plausible hypotheses\nUnsurprisingly, there will be lots of painting metaphors when it comes to simplicity, and cognitive science references when talking about ways in which we’re biased and foolish. Chess, of course, inspires analogies of competition, strategy, and tactics to its service."
  },
  {
    "objectID": "sci_process.html",
    "href": "sci_process.html",
    "title": "Scientific process is all you need",
    "section": "",
    "text": "People doing data mining, analytics, ML, and applied statistics recognize the importance of methodology. They design processes and workflows for their domain of expertise, which is indubitably helpful. However, it is confusing for a beginner to see so many of them – as I show on the margin. Even worse, it is hard to decide which one to use.\nFor a very long time, I chose a process depending on the type of question and problem at hand. I still do, but I’m increasingly unease about it, as every well-intentioned author seems to make their own slight variations. You know the joke: “I decided to unify 10 existing standards – now we have 11”.",
    "crumbs": [
      "| 1. Business |",
      "iii. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#action-and-agent-arena",
    "href": "sci_process.html#action-and-agent-arena",
    "title": "Scientific process is all you need",
    "section": "Action and Agent-Arena",
    "text": "Action and Agent-Arena\nIn order to make the diagram more specific, let’s take the example of a firm and think of it as an agent. At this level of analysis, we don’t need to consider the full implications of a complex adaptive system. Hopefully, the firm has formulated its mission, vision, has articulated a rough strategy to overcome obstacles and has set reasonable objectives.\nEmployees and leadership of the firm will try to make good decisions, develop capabilities in order to increase performance. In a happy and lucky case, these actions are coordinated, aligned with strategy and generate desired outcomes. Besides performance, we can also consider an increase in value added brought by improvements in its processes, products, and services.\nThe firm gathers data about its current and past state \\(\\{\\mathcal{S}_t\\}_{1:t}\\), observes the environment \\(\\mathbfcal{O}_t\\), and its actions \\(\\mathcal{A}_t\\) influence the environment, and vice-versa. It also interacts with other agents in the network (be it customers, suppliers, manufacturers, government, the board, stakeholders, etc).\n\n\nThis perspective is pretty standard in AI research. We’ll see later in the course how we could also use it to tease out different sources of uncertainty.\nI called this “element” in the bigger process an “agent-arena” relation, because we usually don’t consider the whole firm, but our framing depends on what point of view in the value chain we take. For example, the POW of a team responsible for conversion-rate optimization will have its own objectives, decisions, and data they care about \\((\\mathcal{S}, \\mathcal{O}, \\mathcal{A} | {POW} )\\). Of course, there is a concern of alignment, of local optimization which results in suboptimal global outcomes – but this topic is outside the scope of our course.\n\n\n\n\n\n\nSources of uncertainty\n\n\n\nIn their book Algorithms for Decision Making, MIT Press - 2022, M. Kochenderfer, T. Wheeler, and K. Wray make a helpful classification of the sources of uncertainty, based on agent-environment interactions:\n\nOutcome uncertainty, as the consequences and effects of actions are uncertain. Can we be sure that an intervention works out? We can’t take everything into account when making a decision – it becomes combinatorialy explosive.\nModel uncertainty suggests we can’t be sure that our understanding, model as a simplification of reality is the right one. In decision-making, we often mis-frame problems and in statistics, well, choose the wrong model.\nState uncertainty means that the true state of the environment is uncertain, everything changes and is in flux. This is why statisticians argue we always work with samples\nInteraction uncertainty, due to the behavior of the other agents interacting in the environment. For example, competitive firms, social network effects.\n\nWe will focus very little the last aspect of uncertainty, but you have some tools to reason about it: game-theoretic arguments, ideas from multi-agent systems, and graph theory. I think it is an important dimension of reality, however, taking it into account in this course would make it incomparably more complicated and advanced.",
    "crumbs": [
      "| 1. Business |",
      "iii. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#building-and-testing-ideas",
    "href": "sci_process.html#building-and-testing-ideas",
    "title": "Scientific process is all you need",
    "section": "Building and testing ideas",
    "text": "Building and testing ideas\nAt the core of scientific process is experimentation, data collection, and testing of ideas – which results in evidence with respect to a hypothesis or theoretical prediction. For our purposes, we consider experimentation in its large sense, not just randomized control trials and highly-controlled experiments. By including statistical models, simulation, ML models; software systems, prototypes, and features – we can benefit from this self-correcting feedback loop not just in science, but in decision-making.\n\n\nLet’s consider Agile principles and Shape-Up methodology for software development. I think that they fit well into our framework when we’ll sketch out the two remaining pieces\nThis means that inferential and causal statistical models are equally as important ways to test ideas. Moreover, the ultimate test is whether the recommended decisions based on modeling insights work well in practice. Meaning, we get better outcomes in the agent-arena, our problem space. Let’s see how testing of ideas interacts with other elements of our diagram:\n\nAgent-Arena\n\nour insights and recommendations can help inform actions\nafter operationalization, we get feedback on how well it worked\n\nCollaborators\n\nwe have to communicate our insights clearly and persuasively\nother people give feedback about the analysis / results and best-practices about how should we test our ideas\n\nDiscovery and Explanation\n\nideally, we don’t test a vague hypothesis, but a well-thought causal model\nour test results in stronger or weaker evidence for/against the explanation\nwe can use simulation to validate that our statistical models work in principle, meaning we recover the estimand (unobservable quantity of interest)\n\n\nSince the data comes from the real world, we will have to consider very carefully how it was collected and measured. This means that when we build models, we might need to learn new tools to correct for biased samples, non-response, missing data, measurement error, etc.\n\n\nFor some research questions, measurement and sampling is a complicated problem. This is why I’m trying to integrate them in this course.",
    "crumbs": [
      "| 1. Business |",
      "iii. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#thinking-discovery-and-explainations",
    "href": "sci_process.html#thinking-discovery-and-explainations",
    "title": "Scientific process is all you need",
    "section": "Thinking, discovery and explainations",
    "text": "Thinking, discovery and explainations\nOne can make a case that wisdom starts in wonder, and science with curiosity. For a long time, statistics classes were taking hypotheses and theories as given. Our field has caught up to scientists, in the sense of openness in how we gather inspiration.\n\n\nMany books have been written about how theories are being developed, the role of curiosity, inspiration, observation, previous theories, evidence, etc.\nModel-driven exploratory data analysis an pattern recognition have become acceptable ways to find inspiration and formulate hypotheses. I would argue that for most problems I encountered, this process was essential and this is the way in which a good analyst is extremely valuable. Note that with this power comes a lot of responsibility to not fool ourselfs, hence we’ll need discipline and guardrails.\nYou will probably agree that asking a good question and formulating, framing a problem well gets us “halfway” towards a solution. We can approach a research design data-first or question-first, but in practice it’s almost always a mix.\nSpeaking about business decisions, what I mean by theory is our current knowledge and understanding of the problem and domain. Thus, don’t get fooled by claims of “theory-free” AI and ML. The mere fact of selecting what to measure and what features to include in the model, involves human understanding and a judgement of relevance.\nIn the course we will try our best to ensure that statistical models are answering the right quantitative question, which we call an estimand. It might not be obvious why we worry about the underlying, unobservable, theoretical constructs and ways we could measure them. After all, we’re not doing quantitative psychology.\nIn my experience, sometimes it comes up explicitly and thinking this way is very helpful, for example: willingness to pay, customer satisfaction, engagement, brand loyalty and awareness, who can be convinced to buy via promotions, email fatigure, etc.",
    "crumbs": [
      "| 1. Business |",
      "iii. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#steering-and-collaboration",
    "href": "sci_process.html#steering-and-collaboration",
    "title": "Scientific process is all you need",
    "section": "Steering and collaboration",
    "text": "Steering and collaboration\nModern science, and science in general doesn’t happen in isolation. The same is true in firms and almost any other domain. I will not spill much ink in trying to convince you of the power of collaboration, since almost any problem which is complex enough can’t be tackled by a single individual.\nIn the context of this course, we’re thinking of collaboration with clients, stakeholders, experts, engineers, and decision-makers. A few outcomes of this collaboration are the strategy we mentioned at the beginning, policies, interventions, best-practices, feedback, and review. Collaboration informs what should we do, prioritize, build and test, and hypothesise / think about.\n\n\n\n\n\n\nExperiment design, causal inference, and machine learning\n\n\n\nIn the course you will see processes and workflows for experiment design (C. Kozyrkov’s 12 steps of statistics), causal inference (R. McElreath’s owls and DAGs), and machine learning (C. Kozyrkov’s 12 steps of ML, Google’s People+AI).\nDon’t try to memorize them, but go back to this diagram of the scientific process and locate different steps of the workflow. For example, leaving aside a test dataset in machine learning is a way to assess and approximate how well a trained and chosen model generalizes beyond training data.\nSimulating synthetic data from a causal process model in order to validate if a statistical model can draw correct inferences about the estimand can be considered an experiment, although, one which lives in pure theory, until we apply it to real data. This iterative process lives at the intersection of asking questions, building theories and testing ideas.",
    "crumbs": [
      "| 1. Business |",
      "iii. On scientific process"
    ]
  },
  {
    "objectID": "roadmap_bayes.html",
    "href": "roadmap_bayes.html",
    "title": "Module III: Applied Bayesian Statistics",
    "section": "",
    "text": "After the first two modules, we should have a solid foundation for building more realistic models. As discussed in the introduction, randomized experiments and A/B tests have obvious limitations: might be unfeasible, unethical, or just not the right tool. Even when we design experiments or surveys, we still need to model non-response, correct for biases, and account for confounds – meaning, engage in modeling.\n\n\nIn my opinion, among many excellent resources, Richard McElreath’s 2023 lectures on statistical rethinking is the best way to learn the kind of modeling which fits our course and problem space.\nI think that the best way to study regression and generalized linear models is to think very carefully about the causal processes underlying the phenomenon we’re studying. The big advantage of switching to a Bayesian perspective and GLMs, is that we treat all models under an unified framework. This means, we can focus on science and much less on the mathematical intricacies of advanced econometrics.\n\n\n\n\n\n\n\n\n\n\n\nLecture Agenda\nKeywords\nCase Studies / Activities\n\n\n\n\n1\nIntroduction to Bayesian Statistics\nEstimating proportions with Beta-Binomial, counts with Gamma-Poisson, defining priors\nElections, Football Goals, Speed of Light\n\n\n2\nRegression and the Bayesian Workflow\nDAGs of influence, categorical variables, assumptions, nonlinearities, splines, simulation\nE2E example on Howell’s study on height and weight. Summarizing estimands\n\n\n3\nConfounds. Good and Bad controls\nDeep-dive on translating DAGs into statistical models. Adjustment set, backdoor criterion\nE2E example of Waffle houses and divorce rates. Spurious correlations\n\n\n~\nOverfitting and Validation\nPrior and posterior checks, LOO-CV, LOO-PIT, regularization, robust regression, simulation-based calibration\nTBD\n\n\n4\nMultiple Groups and Hierarchical Models\nComplete pooling, No pooling, Partial pooling. Post-stratification and non-representative samples\nRadon concentration, Starbucks wait times, Customer LTV, Elections polls\n\n\n5\nBinomial GLMs\nLink functions, Simpson’s paradox\n1973 Berkeley admissions\n\n\n6\nPoisson GLMs\nOverdispersion, zero-inflated, Negative Binomial\nCampus Crime, Student absence\n\n\n7\nHierarchical GLMs\nDeep-dive into model formulation. Studies on stage anxiety and plumonary fibrosis.\nBasketball Fouls, AirBnb reviews, Seat Belts\n\n\n8\nLongitudinal Data\nChallenges of multilevel time series\nTeenage drinking, sleep study\n\n\n~\nSpecial topics\nBART and Gaussian Processes\nTBD\n\n\n~\nSpecial topics\nMissing data and measurement error\nTBD",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics",
      "Schedule & Roadmap"
    ]
  },
  {
    "objectID": "03_ml/roadmap.html",
    "href": "03_ml/roadmap.html",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n\n  DSc --&gt; Unsup(Dim. Reduction) --&gt; Clust(Clustering) --&gt; MM(Mixtures) --&gt; HDB(HDBScan)\n  Unsup --&gt; PCA --&gt; CA --&gt; UMAP\n\n  DSc --&gt; Cl(Classification) --&gt; T(Tree-based Models) --&gt; BG(Bagging) --&gt; XG(Boosting)\n\n  Cl --&gt; Im(Imbalance) --&gt; F(Fraud Detection)\n\n  DSc(Decisions, Scale) --&gt; Text[/NLP/] --&gt; EM[Embeddings] --&gt; Attn[Attention] --&gt; ABSA[ABSA]\n  DSc --&gt; RS[/RecSys/] --&gt; Mtr[Metrics] --&gt; FM[Factorization Mach.] --&gt; HM[Hybrid Models]\n  DSc --&gt; CV(DL: Vision) --&gt; Conv[CNNs] --&gt; AK[Approx. kNN]\n\n  Conv --&gt; HM\n\n  DSc --&gt; TS(Time Series) --&gt; MTS[Metrics] --&gt; XGB[ML Approaches] --&gt; DL[DL Approaches]\n  EM --&gt; HM\n\n\n\n\n\nFigure 1: In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods – while keeping in mind the particularities of the domain and applications."
  },
  {
    "objectID": "03_ml/roadmap.html#module-iii-ml-and-deep-learning",
    "href": "03_ml/roadmap.html#module-iii-ml-and-deep-learning",
    "title": "Decision Science Course",
    "section": "",
    "text": "flowchart LR\n\n  DSc --&gt; Unsup(Dim. Reduction) --&gt; Clust(Clustering) --&gt; MM(Mixtures) --&gt; HDB(HDBScan)\n  Unsup --&gt; PCA --&gt; CA --&gt; UMAP\n\n  DSc --&gt; Cl(Classification) --&gt; T(Tree-based Models) --&gt; BG(Bagging) --&gt; XG(Boosting)\n\n  Cl --&gt; Im(Imbalance) --&gt; F(Fraud Detection)\n\n  DSc(Decisions, Scale) --&gt; Text[/NLP/] --&gt; EM[Embeddings] --&gt; Attn[Attention] --&gt; ABSA[ABSA]\n  DSc --&gt; RS[/RecSys/] --&gt; Mtr[Metrics] --&gt; FM[Factorization Mach.] --&gt; HM[Hybrid Models]\n  DSc --&gt; CV(DL: Vision) --&gt; Conv[CNNs] --&gt; AK[Approx. kNN]\n\n  Conv --&gt; HM\n\n  DSc --&gt; TS(Time Series) --&gt; MTS[Metrics] --&gt; XGB[ML Approaches] --&gt; DL[DL Approaches]\n  EM --&gt; HM\n\n\n\n\n\nFigure 1: In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods – while keeping in mind the particularities of the domain and applications."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "The study guide for the third module was incredibly hard to put together, since there is so much content on the web and it’s so hard to strike a balance between clarity, simplicity, and the use-case being interesting, realistic. The most important criteria was for the case-study or resource to have code, data, and explained theory or methodology in a freely available e-book.\n\n\nOften, you will have to combine various aspects of an use-case, model, method – from different sources, taking the best from each author.\nThere is little point in replicating well-executed examples from other authors, just for the sake of consistency of code and style. I will, however, migrate examples not available in our frameworks or language of choice. Other times we can benefit from a significant improvement over the original presentation or improving code quality.\n\n\n\n\n\n\nRight level of granularity\n\n\n\nInstead of recommending whole books, which in this field are huge: 500-900p of dense material – in the listings below, you have them grouped by tool, use-case, in manageable-sized chunks. They are ordered and organized in a way which facilitates a more linear, gradual, composable development of skills and understanding. Think of them as lego pieces.\n\n\nI made sure to include interesting examples and archetypal applications for each statistical tool and theoretical topic, so you can immediately apply the concepts you read about or saw during the lectures. However, the responsibility to practice falls entirely on the learner – I can just do my best to make your journey less frustrating.\n\n\n\n\n\n\nflowchart TD\n  DM[/ Probability & Stats /]\n  DM --&gt; CJ[Conjugate Priors] \n  DM --&gt; LR1[Regression Workflow] --&gt; LR2[Model Critique] --&gt; LR3[Model Selection. LOO-PIT] --&gt; LR4[Nonlinearities] --&gt; GLM[GLMs] \n  DM --&gt; G[Groups & Pooling] --&gt; MCMC(MCMC & HMC) --&gt; HM1[Hierarchial Models] --&gt; HM2[ Longitudinal Data] --&gt; HM3[Hierarchical GLMs]\n\n  GLM --&gt; HM1 \n  CJ --&gt; BB(Beta-Binomial) --&gt; GP(Gamma-Poisson) --&gt; NG(Normal-IG. Link fn)\n  NG --&gt; GLM\n  HM3 --&gt; BART(ML & BART)\n\n  style DM fill:#ffb6c1\n  style BB fill:#f7f5bc\n  style GP fill:#f7f5bc\n  style NG fill:#f7f5bc\n  style BART fill:#f7f5bc\n  style MCMC fill:#f7f5bc\n\n\n\n\n\nFigure 1: We develop a composable toolbox, capable to tackle the challenges of nonlinearity, heterogeneity, low level of aggregation, discreteness, missing data and heteroskedasticity. By putting together the right pieces in a right way, we can improve predictions and decisions in most aspects of a business.\n\n\n\n\n\n\n\nAfter the first two modules, we should have a solid foundation for building more realistic models. As discussed in the introduction, randomized experiments and A/B tests have obvious limitations: might be unfeasible, unethical, or just not the right tool.\n\n\nWe are not wasting time by switching to Bayes (7 lectures to get to GLMs), because we treat all models under the same framework.\nInstead of taking regression as the starting point, then going into advanced econometrics – I prefer we switch to a Bayesian perspective, so we can build custom models taking into account the particularities of the phenomena of interest. The course culminates in Hierarchical GLMs, which should be sufficient for the majority of problems you encounter in practice, or at least a good first attempt.\nWe start simple, by modeling a single random variable \\(Y\\), choosing the appropriate distribution for each phenomenon, a conjugate prior for the parameters, doing simulations – then sampling from the posterior with pymc, numpyro, or bambi.1\n1 The R equivalents would be stan and brms, rstanarm. The latter two are a high-level API for most common models.Limiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling of the data generating process, which will serve as building blocks in more complicated and realistic models.\nIn the second module, we applied Bayes rule and got some insightful results in three totally different domains. However, we weren’t doing neither statistics, nor inference – but got into the right mindset. Now it’s time for full-luxury Bayes!\n\n\n\n\n\n\nBeta-Binomial Model. Estimating proportions\n\n\n\nI know, I know, the coin-tossing – simple, yet fundamental and found anywhere there is a binary outcome \\(Y_i \\in \\{0, 1\\}\\). There are many ways to estimate the success probability \\(\\theta\\), when we observe \\(k\\) successes from \\(n\\) trials.\n\nIn Bayes’ Rules is a detailed exposition of the theory, with examples about Michelle’s election support and Milgram’s experiment.\nShare of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)\nA/B Testing for proportions in BMH. Just remember that experiment design is much more nuanced than performing such inference or a test.\nPolitical attitudes in 2008 Election data (BDA3, Ch2, Pr. 21)\nProportion of female births, given prior information. (BDA3, Ch2)\n\nThere are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.\n\nThe debate on the hot hand phenomenon is not yet over. Here are the bayesians weighting in and some new research.\nBasketball shot percentages and true shooting, brilliantly explained by thinking basketball in a playlist about NBA statistics.\nImportant problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in \\(Bin(\\theta, N)\\) both parameters are unknown. Here is an old paper.\nConfidence intervals and lower bound for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: reddit, hacker news. It is a good opportunity to work with the reddit API in order to collect data about posts and comments.\n\n\n\nThe next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.\nYou can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT\nNote that prior choice and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.\n\n\n\n\n\n\nPoisson Distribution. Gamma-Poisson Model\n\n\n\nCounts of independent events in a unit of (space/time/…), with a low probability. You can review the maths here. Below is a list of applications you can practice on:\n\nDeaths by horses in Prussian Army. Here is the historical data and a blog post if you need a refresher on Poisson distribution.\nAsthma mortality (BDA3, Ch2)\nAirplane fatal accidents and passenger deaths\nEstimating WWII production of German tanks based on samples captured\nComparing football teams and goals in football matches\nComparing birth rates in two hospitals\n\nCheck out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) – there are better ways. Also, you will see a poisson example of how to take into account the sample size\n\n\nThe next examples are a little detour, to appreciate the flexibility of the modeling approach we’re taking. We’re building upon previous models, by inferring which rate \\(\\lambda_1\\) or \\(\\lambda_2\\) is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we’re interested in.\nIdeally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.\n\n\n\n\n\n\nPoisson changepoint detection\n\n\n\nEstimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.\n\nCoal Mining disasters, pymc\nText Messages, pymc\nU.S. School Shootings, is there an increase in attempts and occurences?\n\n\n\nWe already worked with multiple parameters, even touching upon the relationship between two variables: counts and time \\(Y_t\\), but not really – it’s more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation \\(g(x)\\) which maps \\(X\\) to the correct support of \\(Y\\). I recommend to introduce this before jumping into gaussian linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.\n\n\n\n\n\n\nLink functions. Golf case-study\n\n\n\nThe domain/geometry inspired, custom model is presented in pymc version, stan by Andrew Gelman, and stan by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won’t work well for this case.\n\n\n\n\nThis is an appropriate point to introduce linear regression and logistic regression. It is not too early, given the importance of those tools, however the presentation should be practical and pretty high level, as there is much nuance to deep-dive later.\nOf course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you’re doing regression that it’s all about the conditional expectation \\(\\mathbb{E}_\\theta[Y|X=x]\\).\n\n\n\n\n\n\nInverseGamma-Normal\n\n\n\n\nYou can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions study\nSpeed of light experiment (BDA3, Ch3), data. You will find here and in any statistics textbook the cases of known and unknown variance, and how the \\(t_k\\) test is derived from the first principles.\nAs in the case of proportions, we can use the model above to model the difference between the means of two independent groups.\n\n\n\n\n\n\nWe already worked with groups in the case of difference in means (proportions and continuous variables) and making inferences for three and more groups, treating them as separate. We will see that such an approach is called “no pooling”.\nIn the traditional teaching of statistics, the above would be covered by t-test, tests for proportions, and when it comes to groups, by ANOVA. If you were lucky, these would’ve been treated as particular versions of linear regression, like in “Most common statistical tests are linear models”.\n\n\nOne more reason for this is that groups and categorical variables do not receive the deserved, nuanced exposition in linear regression. Also, comparing groups is so widespread, that having a tool to deal with the challenges which it poses is immediately useful in your practice inside any firm.\nIn this section, the goal is to show the idea of hierarchical models and partial pooling. I agree with BDA3 (Bayesian Data Analysis 3ed) approach to teach it before regression, as the latter needs a lot of nuance and a long time to learn to apply properly.\n\n\n\n\n\n\nBeta-Binomial for groups. Normal Approximation\n\n\n\nThere is no point in repeating all from the first section, as it is straightforward to apply for groups, as you’ll see when we compare it with a hierarchical approach.\nHowever, it is a good chance for a frequentist detour, to the Agresti-Coull confidence intervals, \\(\\chi^2\\) tests of independence, and nonparametric tests for proportions in multiple groups.\n\nErica Ma has a great talk for hypothesis testing for 2+ groups.\nI think the authoritative resource on Bayesian versions of the distribution-free methods for hypothesis testing is “Bayesian Statistics for Experimental Scientists”, found here. Unfortunately, it is expensive, so I suggest you look at the table of contents and search for the implementations elsewhere.\n\n\n\nI build upon the module 2 on A/B testing by introducing Multi-Armed Bandits. There are cases in which we are testing multiple groups, e.g. which images to show on the website, and we do it at scale. Moreover, we want to experiment countinuously and automatically, trading off between exploration and exploitation to maximize the payoff.\nMABs are a big topic in itself, and a very narrow, particular example of reinforcement learning. It can be very powerful when carefully designed and applied appropriately.\n\n\n\n\n\n\nA/B Testing and Multi-Armed Bandits\n\n\n\nI suggest you start from the didactic examples in Bayesian Methods for Hackers, Chapter 6. If you have an use-case in which this fits perfectly, you can check out the theory and more variants to implement it in more specialized resources.\n\n\nWe encountered the problem of sample size when ranking barbecue diners and reddit posts, but it deserves a few lectures in itself. Once you master a few methods of reasoning about \\(n\\), you can cut through so much bullshit in media, research, and literature.\nFor continuity with the previous “Building Blocks” section, here are a few examples for groups, where the dependent variable is following the Poisson distribution. In this case, the novelty is choosing a gamma prior based on the sample size information of each group. If you think we can do better than this trick – you’re totally right.\n\n\n\n\n\n\nGamma-Poisson for groups\n\n\n\nThe models become more complicated as we attempt to estimate parameters for each group of \\(n\\) observations:\n\nKidney Cancer rates, with priors chosen in accordance to the sample size (BDA3, Ch2). An R visualization\nGuns and suicides, with ideas from empirical and hierarchical Bayes.\n\n\n\n\n\nComplete pooling is when we ignore the fact that we have groups, and make one, global estimate. Of course, it would fall in the category of underfitting or model mis-specification, if the categories or groups are relevant.\nFinally, we’re ready to see how partial pooling and hierarchical (multilevel) models are such an important and powerful innovation, to the point where some practitioners argue (and I agree), that it should be the default way of (regression) modeling. Meaning, a strong justification is needed why your model doesn’t need that structure or modeling of heterogeneity. Keep in mind this advice, but always start with the most simple models when iterating towards the final one.\n\n\n\n\n\n\nHierarchical Models for Groups\n\n\n\nThe main example is a classic (gaussian): modeling the level of radon in houses on the first floor and basement, where the groups are counties (BDA3). We will see this example again in the Hierarchical GLMs section, where predictors at house-level and county-level are added.\n\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro, focuses on exactly the idea we need.\nPrimary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nBeta-Bionmial examples:\n\nEric Ma tutorial at pycon, about baseball batting, and the equivalent numpyro code. Another baseball example is a classic by Efron, implemented in pymc.\nPolice shooting training – detecting race bias. A full Bayesian workflow in bambi.\nAnother classic example is about Rat Tumors experiment, implemented in pymc, from BDA3, Ch5.\n\nA great Gamma-Poisson example is presented by Richard McElreath in Statistical Rethinking lectures: “Starbucks coffee-shops waiting time in queue”.\n\n\nOne of my favorite business applications of the ideas outlined in this section, is in the seemingly innocent problem of computing the lifetime value of a customer. Just to show how tricky this problem is, there is a niche, but extensive literature on the probem, starting from 2000-2005 by Rossi, Fader, and Hardie, which is entering into the mainstream only now, by 2020-2023.\n\n\nThe below is by far not the only model: there are versions for contractual and non-contractual settings with different assumptions. The culmination of this research, in my opinion, is Eva Ascarza’s 2013 paper.\n\n\n\n\n\n\nEstimating Customer Lifetime Value\n\n\n\nModel: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level. The math and code in pymc3 can be found here.\n\n\n\n\n\nI think most practitioners and educators will agree that Linear Regression is the most important tool to master in statistics. I mean it in the most general sense – not only knowing model details, assumptions, extensions and limitations, but how to use it effectively in practice in an end-to-end workflow.\nTo avoid naming confusions in the context of GLMs we’re going to study, and the fact that Regression can be done by many classes of models, I call the good old Linear Regression – “Gaussian Linear Regression”\n\n\n\n\n\n\nExamples. Introduction to the workflow\n\n\n\nI highly recommend the (freely available) book “Regression and Other Stories”, by Andrew Gelman, Jennifer Hill and Aki Vehtari. The latter has a course website for this book, with the examples coded in stan.\nAlso, the second resource, which I would say is even beter, takes on the perspective of causal inference from the very beginning of studying regression. In the course homepage you will find links to the pymc port, slides, and video lectures from 2023.\n\nEugene-Springfield community sample data: OCEAN personality traits as related to drug usage – no categorical variables, \\(Y\\) distribution normal-ish. Demo in bambi.\nEducational outcomes for hearing-impaired children, in pymc\nDangers of spurious correlation and accounting for them are displayed in the marriages and waffles example by McElreath – written in numpyro, but there is also a pymc version.\npymc moderation analysis: muscle mass and age\nThis case study about 100m runs from Calling Bullshit, shows the dangers of extrapolation beyond the range of \\(X\\) that the models were trained on.\n\n\n\nI have a third favorite book, which is freely available, called “Beyond Multiple Linear Regression”. It covers all that we discuss here, but from a frequentist perspective. Despite that, the examples are amazing and it has enough of theory to make it a self-contained resource – therefore, makes a perfect candidate for a port in pymc/numpyro. For a review of linear regression, take a look at the Kentucky derby horse race.\n\n\n\n\n\n\nSplines and Nonlinear Transformations\n\n\n\nThe linearity assumption in linear regression is often under-emphasized, as pointed out by the trio of RoS in a podcast episode. Linear Models are linear in parameters, but we should always think of the appropriate transformations of \\(y\\) and \\(X\\).\n\nSplines from Osvaldo Martin, on Bike Ridership (UCI data, bike sharing)\nSplines, from Statistical Rethinking, on Cherry Blossoms Data\nBe careful and recognize when you should be using a log-log model, expecially since power laws are so widespread in nature and human behavior.\n\n\n\nBy introducing splines and nonlinear transformations, we can easily increase the dimensionality of \\(X\\), with respect to \\(n\\) – which causes massive problems for inference. This is the appropriate moment to introduce a fundamental tradeoff between model complexity and out-of-sample predictive performance.\n\n\n\nUp until this point we took the sampling from the posterior provided by the probabilistic programming languages for granted, as magically converging to the true distribution. We also used very rudimentary methods of comparing models (via Bayes Factors).\nHowever, things go wrong in practice all the time: poorly specified and parametrized models lead to computational problems. It is important to be able to diagnose the chains, and even more, actively critique and interogate your models. At some point, you have to get an understanding of what MCMC, HMC does under the hood – and when you would trade off the accuracy for faster, approximate inference.\n\n\n\n\n\n\nModel Critique and Evaluation\n\n\n\nChecking domain and modeling assumptions, critiquing the models we build are one of the most difficult aspect of statistical practice. It is hard to give a tutorial for this, so I just suggest you read Richard McElreath’s Statistical Rethinking chapters on this topic, along with the more theoretical BDA3, Part II.\n\nFor an intuitive process which can structure your modeling and critique, I suggest McElreath’s “Drawing the Bayesian Owl”.\nChapter 2 of Bayesian Computation in Python, by Osvaldo Martin (BCP) is one of the best at exemplifying these concepts.\nPrior and posterior checks pymc docs\nAn end-to-end example in BCP, modeling the flight delays.\n\nThe frequentists and statistical learning approach have well-established tools for asessing the out-of-sample predictive performance. The fact that training is fast, they can easily use cross-validation and even bootstrap. A recent innovation in Bayes is LOO-PIT, an approximation to leave-one-out cross-validation, which leverages the fact that we have posterior sampling.\n\nA reminder of bootstrap, that is what the frequentists might do - chapter 10 or page 249. It is a powerful tool to have in your arsenal anyways.\nHere are the technical details of LOO-PIT, in the context of model comparison.\n\n\n\n\n\n\nOnce we are able to critique and evaluate a single model, it makes perfect sense to compare models of increasing complexity and with alternative assumptions. The following uses the same tools as before, but in a iterative workflow.\n\n\n\n\n\n\nModel Comparison and Selection\n\n\n\n\npymc. - Model selection with fake data and polynomials\nChapter 3 and 4 of BCP are beautiful, where linear and logistic regression are introduced, along with questions of interactions, nonlinear transformations, hierarchical models and model choice.\n\n\n\nThere are cases when the problem is in sampling and one common cause, besides mis-specification, are models with bad posterior geometry (from the perspective of samplers). As a rule of thumb – “round and smooth” is good. A prevalent solution in literature is to use better parametrizations, but in order to fix your problem, you first have to be aware that it might be the case.\n\n\n\n\n\n\nBad posterior geometry. Reparametrization\n\n\n\n\nTechnical: (mu, sigma) - Neal’s Funnel numpyro, there is equivalent pymc\nMore solutions in numpyro for bad posterior geometry\n\nThese examples in 2D are intuitive, however things become much more complicated and counterintuitive in multidimensional parameter spaces. The long story short, in practice, is to use heavier regularization (via stronger priors) in those cases.\n\n\n\n\n\n\n\n\nRegularization and Variable Selection\n\n\n\nA big topic in ML is regularization and encouraging sparse solutions (models). The point is that in highly-dimensional spaces, only a small subset of variables is relevant. Bayesians have sneaky methods of constructing priors which act as a mechanism for variable selection.\nThere is an additional topic of robuts methods (to outliers). Some machine learning models are robust(ish) out of the box: quantile regression, gradient boosted trees. Bayesians use heavier-tailed distribution to achieve a similar effect.\n\nSpike and Slab example on Kaggle\nHorseshoe prior implemented in pymc3\nRobust linear regression on simulated data, implemented here, in pymc/bambi.\n\n\n\n\n\n\nI can’t resist to suggest a case-study from finance: the good old portfolio returns optimization. The methods used in practice are much more sophisticated than the below, but it’s useful to be aware of the fundamentals. The Wishart distribution, used as a prior for positive semi-definite matrices (covariation) can be an useful building block in more complicated models to capture the covariation structure, e.g of different time series.\nAs a very advanced aside, I am fascinated by Michael I. Jordan’s approach to multidimensional time series modeling, where the covariation structure is evolving in time.\n\n\n\n\n\n\nPortfolio optimization a la Markowitz\n\n\n\nSee the example with Wishart prior and portfolios from BMH. We could do an optimization on point estimates, but this aprroach is more general, capturing the uncertainty in estimates.\n\n\nIf you got to this last topic on linear regression, months probably have passed. There is a good reason universities dedicate entire semesters for regression – there are so many nuances to get right: from both practical and theoretical perspective. One of these complexities is how to handle missing data, which deserves a course on its own.\n\n\n\n\n\n\nModeling how Data goes Missing\n\n\n\nIn Bayesian Statistics, we also have to be very explicit and declare our assumptions about how the data went missing – which is a hard, but beneficial thing to do. The good news – missingness is treated like an unobserved variable and is subject to the same inferences we’ve been doing before. We just need to pick the right model of missingness for each individual case.\n\nMissing data imputation, pymc, both for linear and hierarchical\nDiscrete missing data imputation numpyro, with simulated data and causal graphs\nPymcon - missing data tutorial in pymc3\nMissing Data Imputation\n\n\n\n\n\n\nNot every phenomena we model is continuous or appropriate for gaussian linear regression, but we can easily extend it with the building blocks we learned at the beginning. It’s all about those link functions and relaxing some of the assumptions of the linear regression. I want to point out how the following mirrors the simple, single-variable models we first practiced on.\nI do not want to make the transition from gaussian case to logistic, poisson and other variants sound trivial. Their behavior, evaluation, and interpretation is different and distinct. You can not equivocate them.\nAs a theoretical background of what connects these models into one single family, hence the name of GLMs, I suggest you read the section in the “Beyond MLR” about the exponential family. Richard McElreath has similar information-theoretic arguments in his lectures on Statistical Rethinking.\n\n\n\n\n\n\nLogistic Regression\n\n\n\nAs a fun aside about machine learning, logistic regression is the only model which is calibrated out-of-the-box, meaning the scores for the classification can be interpreted as probabilities, due to its specific loss function.\nFor the rest of the models (e.g. random forests), you would need to calibrate via an isotonic or piecewise-linear regression, on a separate, validation dataset. So, going back to the practice, here are some examples using the logistic regression:\n\nBeetles survival by concentration of chemical, via bambi – tries out different link functions and is a simple example to get started with.\nVote intention by age in the U.S., via bambi, 2016 ANES data, 1200 samples.\nSocio-economic influences on income bracket (\\(\\ge \\$50k\\)), also does model comparison, implementation in bambi\nMultinomial regression on the iris, the most boring dataset, but you’ll see that you have a solution for the multiclass classification without the multiple comparisons.\n\n\n\nWhen you have a numeric target variable, think twice if it is appropriate for the gaussian distribution. Sometimes, what we model is really, counts, or non-negative, or monotonically increasing with respect to a \\(X_i\\) (that alone deserves its own case-study). Sometimes, even poisson doesn’t work out due to overdispersion or zero-inflation, and it is not a rare exception, especially when modeling aspects of customer behavior.\n\n\n\n\n\n\nPoisson Regression\n\n\n\n\nNumber of laws regarding equality, which includes a discussion for the issue of overdispersion. Not sure at all that this would be a prototypical DGP story for a Poisson.\nStop and frisk data from BDA3 and RoS, the frequentist version\nCampus crime and estimating household size in Philippines from BeyondMLR.\nAlcohol and meds interaction, with simulated data.\n\n\n\n\n\n\n\n\n\nOverdispersion. Negative Binomial. Zero-Inflation\n\n\n\nThe negative binomial distribution is often found in customer behavior, in contractual or non-contractual settings, when it comes to purchases, subscription renewals.\n\nCockroaches and pest management, where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.\nFishing catches in a park, in numpyro\nStudents’ absence, UCLA data, application of negative binomial, written in bambi\n\n\n\nIn the probability fundamentals section, we discussed the use-case of estimating proportions and named the field of compositional data analysis. It is often found in practice, but disguises itself, which causes the wrong method to be applied.\nIf you encountered such a problem, when you care about proportions, mix of stuff, or compositions, not their absolute values or quantities, you can check out these lecture notes\n\n\n\n\n\n\nProportions. Compositional Data Analysis\n\n\n\nDirichlet regression, pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books\n\n\n\n\n\nWe finally reached the most exciting point in our journey so far, where we can properly model and explain sources of variation at multiple levels (of analysis). This is where we relax the assumption of \\(i.i.d.\\), replacing it with one of exchangeability.\nThe point is that correlated data causes problems in modeling when we don’t account properly for it: be it groups, clusters, categorical variables, time series, geography, etc. BeyondMLR has a very well thought, practical motivation for multilevel models.\n\n\n\n\n\n\nHierarchical Gaussian Regression\n\n\n\nWe come back to the radon example, by adding one covariate at each level: house and county. This explains a part of variation which was missed before and leverages pooling to take care of low sample size in some counties.\n\nRadon: Primary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nbambinos a higher level API, models the log-radon. Alternatively, McStanPy implementation\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro\n\nBayesian Multilevel Regression numpyro on OSIC Pulmonary Fibrosis Progression data, which assesses the risks of smoking and not only.\nBeyondMLR presents a really interesting psychological study about stage anxiety for music performers at a conservatory.\nA repeated analysis of Stack’s facial feedback hypothesis, in the context of replication crisis, via bambi – full workflow.\n\n\n\n\n\n\nLongitudinal data is a special case of multilevel models, but has the distinct feature, that at the group level, the data isn’t iid, but comes as a realization of the stochastic process. Often called Panel Data in economics and social sciences. You will see a different terminology in that literature: of mixed or multilevel models.\n\n\n\n\n\n\nLongitudinal Data and Studies\n\n\n\nThese examples model the trend with a covariate, as a function of time – which is a happy case when we can do that. However, in practice, things become more complicated if we have to model and identify a stochastic process like \\(AR(k)\\), or \\(MA(n)\\).\n\nLongitudinal data with drinking teens, alcohol consumption per cohorts, pymc.\nSleep study and reaction times in time by subject. The same modeling idea is in pig growth study, both implemented in bambi.\nBeyond MLR charter schools longitudinal study.\n\n\n\nIf in the previous sections, I didn’t have a strong preference for the Bayesian Approach, in the case of multilevel models, I strongly believe Bayesian Inference to be superior and less prone to overfitting and numerical instability.\nAnother aspect of this, is that most mixed models packages will make certain decisions for us, without our consent, which could influence our results and conclusions. As you’ll see in these tutorials, we are forced to construct the model and declare, justify every choice of prior and model structure.\n\n\n\nThis is the end of the Module 3 (Applied Bayesian Statistics). These tools are so general and powerful, that they will last you a lifetime. However, there is so much more to learn and so many more challenges to tackle – that there is no end to the mastery of Multilevel GLMs.\n\n\n\n\n\n\nHierarchical Logistic Regression\n\n\n\n\nBeyond MLR, college basketball referee foul differential here\nGraduate Admissions, from McElreath,  UC Berkeley in Fall 1973 (numpyro)\nRasch item-response model for Nba fouls in crunch time, pymc\n\n\n\n\n\n\n\n\n\nHierarchical Poisson Regression\n\n\n\n\nAirbnb number of reviews\nEstimating the strength of a rugby team\nPaper investigating seat-belt use rates, with data probably taken from the department of transportation crashes website\n\n\n\n\n\n\n\n\n\nModels with 3 Levels\n\n\n\nAt last, it is useful to see an example where we have an even more complicated multilevel structure, as it will be the case in some practical applications.\n\nBeyond MLR 3-level seed germination\n\n\n\nEven after you learned all the technical intricacies, we’re hit again with the reality that association doesn’t imply causation. So, after being competent with this amazing tool – we have to go back to ideas from causal inference, if we want to get a more than associative insight about our theory and understanding of phenomena.\n\n\n\nAs a bouns, I’m adding two flexible, general-purpose models, which you can treat as in the Machine Learning practices, but are Bayesian to the core. These are extremely useful if we care more about predictive accuracy, either for decision-making, or as a part of causal inference process, where a certain complicated relationship details aren’t important to us, but just the conditional average treatment effects.\n\n\n\n\n\n\nBayesian Additive Regression Trees\n\n\n\n\nBART from osvaldo, on bike shares\nPodcast Episode and an R package\nNonparametric methods - what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. This is the simplest and the most accessible explanation I know of so far.\n\n\n\n\n\n\n\n\n\nGaussian Processes\n\n\n\n\nGelman: Birthdays, hilbert space approximation repo, also in stan",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#introduction-to-bayesian-statistics",
    "href": "references.html#introduction-to-bayesian-statistics",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "After the first two modules, we should have a solid foundation for building more realistic models. As discussed in the introduction, randomized experiments and A/B tests have obvious limitations: might be unfeasible, unethical, or just not the right tool.\n\n\nWe are not wasting time by switching to Bayes (7 lectures to get to GLMs), because we treat all models under the same framework.\nInstead of taking regression as the starting point, then going into advanced econometrics – I prefer we switch to a Bayesian perspective, so we can build custom models taking into account the particularities of the phenomena of interest. The course culminates in Hierarchical GLMs, which should be sufficient for the majority of problems you encounter in practice, or at least a good first attempt.\nWe start simple, by modeling a single random variable \\(Y\\), choosing the appropriate distribution for each phenomenon, a conjugate prior for the parameters, doing simulations – then sampling from the posterior with pymc, numpyro, or bambi.1\n1 The R equivalents would be stan and brms, rstanarm. The latter two are a high-level API for most common models.Limiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling of the data generating process, which will serve as building blocks in more complicated and realistic models.\nIn the second module, we applied Bayes rule and got some insightful results in three totally different domains. However, we weren’t doing neither statistics, nor inference – but got into the right mindset. Now it’s time for full-luxury Bayes!\n\n\n\n\n\n\nBeta-Binomial Model. Estimating proportions\n\n\n\nI know, I know, the coin-tossing – simple, yet fundamental and found anywhere there is a binary outcome \\(Y_i \\in \\{0, 1\\}\\). There are many ways to estimate the success probability \\(\\theta\\), when we observe \\(k\\) successes from \\(n\\) trials.\n\nIn Bayes’ Rules is a detailed exposition of the theory, with examples about Michelle’s election support and Milgram’s experiment.\nShare of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)\nA/B Testing for proportions in BMH. Just remember that experiment design is much more nuanced than performing such inference or a test.\nPolitical attitudes in 2008 Election data (BDA3, Ch2, Pr. 21)\nProportion of female births, given prior information. (BDA3, Ch2)\n\nThere are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.\n\nThe debate on the hot hand phenomenon is not yet over. Here are the bayesians weighting in and some new research.\nBasketball shot percentages and true shooting, brilliantly explained by thinking basketball in a playlist about NBA statistics.\nImportant problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in \\(Bin(\\theta, N)\\) both parameters are unknown. Here is an old paper.\nConfidence intervals and lower bound for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: reddit, hacker news. It is a good opportunity to work with the reddit API in order to collect data about posts and comments.\n\n\n\nThe next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.\nYou can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT\nNote that prior choice and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.\n\n\n\n\n\n\nPoisson Distribution. Gamma-Poisson Model\n\n\n\nCounts of independent events in a unit of (space/time/…), with a low probability. You can review the maths here. Below is a list of applications you can practice on:\n\nDeaths by horses in Prussian Army. Here is the historical data and a blog post if you need a refresher on Poisson distribution.\nAsthma mortality (BDA3, Ch2)\nAirplane fatal accidents and passenger deaths\nEstimating WWII production of German tanks based on samples captured\nComparing football teams and goals in football matches\nComparing birth rates in two hospitals\n\nCheck out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) – there are better ways. Also, you will see a poisson example of how to take into account the sample size\n\n\nThe next examples are a little detour, to appreciate the flexibility of the modeling approach we’re taking. We’re building upon previous models, by inferring which rate \\(\\lambda_1\\) or \\(\\lambda_2\\) is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we’re interested in.\nIdeally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.\n\n\n\n\n\n\nPoisson changepoint detection\n\n\n\nEstimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.\n\nCoal Mining disasters, pymc\nText Messages, pymc\nU.S. School Shootings, is there an increase in attempts and occurences?\n\n\n\nWe already worked with multiple parameters, even touching upon the relationship between two variables: counts and time \\(Y_t\\), but not really – it’s more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation \\(g(x)\\) which maps \\(X\\) to the correct support of \\(Y\\). I recommend to introduce this before jumping into gaussian linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.\n\n\n\n\n\n\nLink functions. Golf case-study\n\n\n\nThe domain/geometry inspired, custom model is presented in pymc version, stan by Andrew Gelman, and stan by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won’t work well for this case.\n\n\n\n\nThis is an appropriate point to introduce linear regression and logistic regression. It is not too early, given the importance of those tools, however the presentation should be practical and pretty high level, as there is much nuance to deep-dive later.\nOf course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you’re doing regression that it’s all about the conditional expectation \\(\\mathbb{E}_\\theta[Y|X=x]\\).\n\n\n\n\n\n\nInverseGamma-Normal\n\n\n\n\nYou can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions study\nSpeed of light experiment (BDA3, Ch3), data. You will find here and in any statistics textbook the cases of known and unknown variance, and how the \\(t_k\\) test is derived from the first principles.\nAs in the case of proportions, we can use the model above to model the difference between the means of two independent groups.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#multiple-groups-and-hierarchical-models",
    "href": "references.html#multiple-groups-and-hierarchical-models",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "We already worked with groups in the case of difference in means (proportions and continuous variables) and making inferences for three and more groups, treating them as separate. We will see that such an approach is called “no pooling”.\nIn the traditional teaching of statistics, the above would be covered by t-test, tests for proportions, and when it comes to groups, by ANOVA. If you were lucky, these would’ve been treated as particular versions of linear regression, like in “Most common statistical tests are linear models”.\n\n\nOne more reason for this is that groups and categorical variables do not receive the deserved, nuanced exposition in linear regression. Also, comparing groups is so widespread, that having a tool to deal with the challenges which it poses is immediately useful in your practice inside any firm.\nIn this section, the goal is to show the idea of hierarchical models and partial pooling. I agree with BDA3 (Bayesian Data Analysis 3ed) approach to teach it before regression, as the latter needs a lot of nuance and a long time to learn to apply properly.\n\n\n\n\n\n\nBeta-Binomial for groups. Normal Approximation\n\n\n\nThere is no point in repeating all from the first section, as it is straightforward to apply for groups, as you’ll see when we compare it with a hierarchical approach.\nHowever, it is a good chance for a frequentist detour, to the Agresti-Coull confidence intervals, \\(\\chi^2\\) tests of independence, and nonparametric tests for proportions in multiple groups.\n\nErica Ma has a great talk for hypothesis testing for 2+ groups.\nI think the authoritative resource on Bayesian versions of the distribution-free methods for hypothesis testing is “Bayesian Statistics for Experimental Scientists”, found here. Unfortunately, it is expensive, so I suggest you look at the table of contents and search for the implementations elsewhere.\n\n\n\nI build upon the module 2 on A/B testing by introducing Multi-Armed Bandits. There are cases in which we are testing multiple groups, e.g. which images to show on the website, and we do it at scale. Moreover, we want to experiment countinuously and automatically, trading off between exploration and exploitation to maximize the payoff.\nMABs are a big topic in itself, and a very narrow, particular example of reinforcement learning. It can be very powerful when carefully designed and applied appropriately.\n\n\n\n\n\n\nA/B Testing and Multi-Armed Bandits\n\n\n\nI suggest you start from the didactic examples in Bayesian Methods for Hackers, Chapter 6. If you have an use-case in which this fits perfectly, you can check out the theory and more variants to implement it in more specialized resources.\n\n\nWe encountered the problem of sample size when ranking barbecue diners and reddit posts, but it deserves a few lectures in itself. Once you master a few methods of reasoning about \\(n\\), you can cut through so much bullshit in media, research, and literature.\nFor continuity with the previous “Building Blocks” section, here are a few examples for groups, where the dependent variable is following the Poisson distribution. In this case, the novelty is choosing a gamma prior based on the sample size information of each group. If you think we can do better than this trick – you’re totally right.\n\n\n\n\n\n\nGamma-Poisson for groups\n\n\n\nThe models become more complicated as we attempt to estimate parameters for each group of \\(n\\) observations:\n\nKidney Cancer rates, with priors chosen in accordance to the sample size (BDA3, Ch2). An R visualization\nGuns and suicides, with ideas from empirical and hierarchical Bayes.\n\n\n\n\n\nComplete pooling is when we ignore the fact that we have groups, and make one, global estimate. Of course, it would fall in the category of underfitting or model mis-specification, if the categories or groups are relevant.\nFinally, we’re ready to see how partial pooling and hierarchical (multilevel) models are such an important and powerful innovation, to the point where some practitioners argue (and I agree), that it should be the default way of (regression) modeling. Meaning, a strong justification is needed why your model doesn’t need that structure or modeling of heterogeneity. Keep in mind this advice, but always start with the most simple models when iterating towards the final one.\n\n\n\n\n\n\nHierarchical Models for Groups\n\n\n\nThe main example is a classic (gaussian): modeling the level of radon in houses on the first floor and basement, where the groups are counties (BDA3). We will see this example again in the Hierarchical GLMs section, where predictors at house-level and county-level are added.\n\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro, focuses on exactly the idea we need.\nPrimary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nBeta-Bionmial examples:\n\nEric Ma tutorial at pycon, about baseball batting, and the equivalent numpyro code. Another baseball example is a classic by Efron, implemented in pymc.\nPolice shooting training – detecting race bias. A full Bayesian workflow in bambi.\nAnother classic example is about Rat Tumors experiment, implemented in pymc, from BDA3, Ch5.\n\nA great Gamma-Poisson example is presented by Richard McElreath in Statistical Rethinking lectures: “Starbucks coffee-shops waiting time in queue”.\n\n\nOne of my favorite business applications of the ideas outlined in this section, is in the seemingly innocent problem of computing the lifetime value of a customer. Just to show how tricky this problem is, there is a niche, but extensive literature on the probem, starting from 2000-2005 by Rossi, Fader, and Hardie, which is entering into the mainstream only now, by 2020-2023.\n\n\nThe below is by far not the only model: there are versions for contractual and non-contractual settings with different assumptions. The culmination of this research, in my opinion, is Eva Ascarza’s 2013 paper.\n\n\n\n\n\n\nEstimating Customer Lifetime Value\n\n\n\nModel: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level. The math and code in pymc3 can be found here.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#regression-and-bayesian-workflow",
    "href": "references.html#regression-and-bayesian-workflow",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "I think most practitioners and educators will agree that Linear Regression is the most important tool to master in statistics. I mean it in the most general sense – not only knowing model details, assumptions, extensions and limitations, but how to use it effectively in practice in an end-to-end workflow.\nTo avoid naming confusions in the context of GLMs we’re going to study, and the fact that Regression can be done by many classes of models, I call the good old Linear Regression – “Gaussian Linear Regression”\n\n\n\n\n\n\nExamples. Introduction to the workflow\n\n\n\nI highly recommend the (freely available) book “Regression and Other Stories”, by Andrew Gelman, Jennifer Hill and Aki Vehtari. The latter has a course website for this book, with the examples coded in stan.\nAlso, the second resource, which I would say is even beter, takes on the perspective of causal inference from the very beginning of studying regression. In the course homepage you will find links to the pymc port, slides, and video lectures from 2023.\n\nEugene-Springfield community sample data: OCEAN personality traits as related to drug usage – no categorical variables, \\(Y\\) distribution normal-ish. Demo in bambi.\nEducational outcomes for hearing-impaired children, in pymc\nDangers of spurious correlation and accounting for them are displayed in the marriages and waffles example by McElreath – written in numpyro, but there is also a pymc version.\npymc moderation analysis: muscle mass and age\nThis case study about 100m runs from Calling Bullshit, shows the dangers of extrapolation beyond the range of \\(X\\) that the models were trained on.\n\n\n\nI have a third favorite book, which is freely available, called “Beyond Multiple Linear Regression”. It covers all that we discuss here, but from a frequentist perspective. Despite that, the examples are amazing and it has enough of theory to make it a self-contained resource – therefore, makes a perfect candidate for a port in pymc/numpyro. For a review of linear regression, take a look at the Kentucky derby horse race.\n\n\n\n\n\n\nSplines and Nonlinear Transformations\n\n\n\nThe linearity assumption in linear regression is often under-emphasized, as pointed out by the trio of RoS in a podcast episode. Linear Models are linear in parameters, but we should always think of the appropriate transformations of \\(y\\) and \\(X\\).\n\nSplines from Osvaldo Martin, on Bike Ridership (UCI data, bike sharing)\nSplines, from Statistical Rethinking, on Cherry Blossoms Data\nBe careful and recognize when you should be using a log-log model, expecially since power laws are so widespread in nature and human behavior.\n\n\n\nBy introducing splines and nonlinear transformations, we can easily increase the dimensionality of \\(X\\), with respect to \\(n\\) – which causes massive problems for inference. This is the appropriate moment to introduce a fundamental tradeoff between model complexity and out-of-sample predictive performance.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#regression-and-model-critique",
    "href": "references.html#regression-and-model-critique",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "Up until this point we took the sampling from the posterior provided by the probabilistic programming languages for granted, as magically converging to the true distribution. We also used very rudimentary methods of comparing models (via Bayes Factors).\nHowever, things go wrong in practice all the time: poorly specified and parametrized models lead to computational problems. It is important to be able to diagnose the chains, and even more, actively critique and interogate your models. At some point, you have to get an understanding of what MCMC, HMC does under the hood – and when you would trade off the accuracy for faster, approximate inference.\n\n\n\n\n\n\nModel Critique and Evaluation\n\n\n\nChecking domain and modeling assumptions, critiquing the models we build are one of the most difficult aspect of statistical practice. It is hard to give a tutorial for this, so I just suggest you read Richard McElreath’s Statistical Rethinking chapters on this topic, along with the more theoretical BDA3, Part II.\n\nFor an intuitive process which can structure your modeling and critique, I suggest McElreath’s “Drawing the Bayesian Owl”.\nChapter 2 of Bayesian Computation in Python, by Osvaldo Martin (BCP) is one of the best at exemplifying these concepts.\nPrior and posterior checks pymc docs\nAn end-to-end example in BCP, modeling the flight delays.\n\nThe frequentists and statistical learning approach have well-established tools for asessing the out-of-sample predictive performance. The fact that training is fast, they can easily use cross-validation and even bootstrap. A recent innovation in Bayes is LOO-PIT, an approximation to leave-one-out cross-validation, which leverages the fact that we have posterior sampling.\n\nA reminder of bootstrap, that is what the frequentists might do - chapter 10 or page 249. It is a powerful tool to have in your arsenal anyways.\nHere are the technical details of LOO-PIT, in the context of model comparison.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#regression-and-model-selection",
    "href": "references.html#regression-and-model-selection",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "Once we are able to critique and evaluate a single model, it makes perfect sense to compare models of increasing complexity and with alternative assumptions. The following uses the same tools as before, but in a iterative workflow.\n\n\n\n\n\n\nModel Comparison and Selection\n\n\n\n\npymc. - Model selection with fake data and polynomials\nChapter 3 and 4 of BCP are beautiful, where linear and logistic regression are introduced, along with questions of interactions, nonlinear transformations, hierarchical models and model choice.\n\n\n\nThere are cases when the problem is in sampling and one common cause, besides mis-specification, are models with bad posterior geometry (from the perspective of samplers). As a rule of thumb – “round and smooth” is good. A prevalent solution in literature is to use better parametrizations, but in order to fix your problem, you first have to be aware that it might be the case.\n\n\n\n\n\n\nBad posterior geometry. Reparametrization\n\n\n\n\nTechnical: (mu, sigma) - Neal’s Funnel numpyro, there is equivalent pymc\nMore solutions in numpyro for bad posterior geometry\n\nThese examples in 2D are intuitive, however things become much more complicated and counterintuitive in multidimensional parameter spaces. The long story short, in practice, is to use heavier regularization (via stronger priors) in those cases.\n\n\n\n\n\n\n\n\nRegularization and Variable Selection\n\n\n\nA big topic in ML is regularization and encouraging sparse solutions (models). The point is that in highly-dimensional spaces, only a small subset of variables is relevant. Bayesians have sneaky methods of constructing priors which act as a mechanism for variable selection.\nThere is an additional topic of robuts methods (to outliers). Some machine learning models are robust(ish) out of the box: quantile regression, gradient boosted trees. Bayesians use heavier-tailed distribution to achieve a similar effect.\n\nSpike and Slab example on Kaggle\nHorseshoe prior implemented in pymc3\nRobust linear regression on simulated data, implemented here, in pymc/bambi.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#regression-and-nonlinearities",
    "href": "references.html#regression-and-nonlinearities",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "I can’t resist to suggest a case-study from finance: the good old portfolio returns optimization. The methods used in practice are much more sophisticated than the below, but it’s useful to be aware of the fundamentals. The Wishart distribution, used as a prior for positive semi-definite matrices (covariation) can be an useful building block in more complicated models to capture the covariation structure, e.g of different time series.\nAs a very advanced aside, I am fascinated by Michael I. Jordan’s approach to multidimensional time series modeling, where the covariation structure is evolving in time.\n\n\n\n\n\n\nPortfolio optimization a la Markowitz\n\n\n\nSee the example with Wishart prior and portfolios from BMH. We could do an optimization on point estimates, but this aprroach is more general, capturing the uncertainty in estimates.\n\n\nIf you got to this last topic on linear regression, months probably have passed. There is a good reason universities dedicate entire semesters for regression – there are so many nuances to get right: from both practical and theoretical perspective. One of these complexities is how to handle missing data, which deserves a course on its own.\n\n\n\n\n\n\nModeling how Data goes Missing\n\n\n\nIn Bayesian Statistics, we also have to be very explicit and declare our assumptions about how the data went missing – which is a hard, but beneficial thing to do. The good news – missingness is treated like an unobserved variable and is subject to the same inferences we’ve been doing before. We just need to pick the right model of missingness for each individual case.\n\nMissing data imputation, pymc, both for linear and hierarchical\nDiscrete missing data imputation numpyro, with simulated data and causal graphs\nPymcon - missing data tutorial in pymc3\nMissing Data Imputation",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#generalized-linear-models",
    "href": "references.html#generalized-linear-models",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "Not every phenomena we model is continuous or appropriate for gaussian linear regression, but we can easily extend it with the building blocks we learned at the beginning. It’s all about those link functions and relaxing some of the assumptions of the linear regression. I want to point out how the following mirrors the simple, single-variable models we first practiced on.\nI do not want to make the transition from gaussian case to logistic, poisson and other variants sound trivial. Their behavior, evaluation, and interpretation is different and distinct. You can not equivocate them.\nAs a theoretical background of what connects these models into one single family, hence the name of GLMs, I suggest you read the section in the “Beyond MLR” about the exponential family. Richard McElreath has similar information-theoretic arguments in his lectures on Statistical Rethinking.\n\n\n\n\n\n\nLogistic Regression\n\n\n\nAs a fun aside about machine learning, logistic regression is the only model which is calibrated out-of-the-box, meaning the scores for the classification can be interpreted as probabilities, due to its specific loss function.\nFor the rest of the models (e.g. random forests), you would need to calibrate via an isotonic or piecewise-linear regression, on a separate, validation dataset. So, going back to the practice, here are some examples using the logistic regression:\n\nBeetles survival by concentration of chemical, via bambi – tries out different link functions and is a simple example to get started with.\nVote intention by age in the U.S., via bambi, 2016 ANES data, 1200 samples.\nSocio-economic influences on income bracket (\\(\\ge \\$50k\\)), also does model comparison, implementation in bambi\nMultinomial regression on the iris, the most boring dataset, but you’ll see that you have a solution for the multiclass classification without the multiple comparisons.\n\n\n\nWhen you have a numeric target variable, think twice if it is appropriate for the gaussian distribution. Sometimes, what we model is really, counts, or non-negative, or monotonically increasing with respect to a \\(X_i\\) (that alone deserves its own case-study). Sometimes, even poisson doesn’t work out due to overdispersion or zero-inflation, and it is not a rare exception, especially when modeling aspects of customer behavior.\n\n\n\n\n\n\nPoisson Regression\n\n\n\n\nNumber of laws regarding equality, which includes a discussion for the issue of overdispersion. Not sure at all that this would be a prototypical DGP story for a Poisson.\nStop and frisk data from BDA3 and RoS, the frequentist version\nCampus crime and estimating household size in Philippines from BeyondMLR.\nAlcohol and meds interaction, with simulated data.\n\n\n\n\n\n\n\n\n\nOverdispersion. Negative Binomial. Zero-Inflation\n\n\n\nThe negative binomial distribution is often found in customer behavior, in contractual or non-contractual settings, when it comes to purchases, subscription renewals.\n\nCockroaches and pest management, where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.\nFishing catches in a park, in numpyro\nStudents’ absence, UCLA data, application of negative binomial, written in bambi\n\n\n\nIn the probability fundamentals section, we discussed the use-case of estimating proportions and named the field of compositional data analysis. It is often found in practice, but disguises itself, which causes the wrong method to be applied.\nIf you encountered such a problem, when you care about proportions, mix of stuff, or compositions, not their absolute values or quantities, you can check out these lecture notes\n\n\n\n\n\n\nProportions. Compositional Data Analysis\n\n\n\nDirichlet regression, pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#introduction-to-hierarchical-models",
    "href": "references.html#introduction-to-hierarchical-models",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "We finally reached the most exciting point in our journey so far, where we can properly model and explain sources of variation at multiple levels (of analysis). This is where we relax the assumption of \\(i.i.d.\\), replacing it with one of exchangeability.\nThe point is that correlated data causes problems in modeling when we don’t account properly for it: be it groups, clusters, categorical variables, time series, geography, etc. BeyondMLR has a very well thought, practical motivation for multilevel models.\n\n\n\n\n\n\nHierarchical Gaussian Regression\n\n\n\nWe come back to the radon example, by adding one covariate at each level: house and county. This explains a part of variation which was missed before and leverages pooling to take care of low sample size in some counties.\n\nRadon: Primary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nbambinos a higher level API, models the log-radon. Alternatively, McStanPy implementation\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro\n\nBayesian Multilevel Regression numpyro on OSIC Pulmonary Fibrosis Progression data, which assesses the risks of smoking and not only.\nBeyondMLR presents a really interesting psychological study about stage anxiety for music performers at a conservatory.\nA repeated analysis of Stack’s facial feedback hypothesis, in the context of replication crisis, via bambi – full workflow.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#hierarchical-models-and-longitudinal-data",
    "href": "references.html#hierarchical-models-and-longitudinal-data",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "Longitudinal data is a special case of multilevel models, but has the distinct feature, that at the group level, the data isn’t iid, but comes as a realization of the stochastic process. Often called Panel Data in economics and social sciences. You will see a different terminology in that literature: of mixed or multilevel models.\n\n\n\n\n\n\nLongitudinal Data and Studies\n\n\n\nThese examples model the trend with a covariate, as a function of time – which is a happy case when we can do that. However, in practice, things become more complicated if we have to model and identify a stochastic process like \\(AR(k)\\), or \\(MA(n)\\).\n\nLongitudinal data with drinking teens, alcohol consumption per cohorts, pymc.\nSleep study and reaction times in time by subject. The same modeling idea is in pig growth study, both implemented in bambi.\nBeyond MLR charter schools longitudinal study.\n\n\n\nIf in the previous sections, I didn’t have a strong preference for the Bayesian Approach, in the case of multilevel models, I strongly believe Bayesian Inference to be superior and less prone to overfitting and numerical instability.\nAnother aspect of this, is that most mixed models packages will make certain decisions for us, without our consent, which could influence our results and conclusions. As you’ll see in these tutorials, we are forced to construct the model and declare, justify every choice of prior and model structure.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#hierarchical-models-case-studies",
    "href": "references.html#hierarchical-models-case-studies",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "This is the end of the Module 3 (Applied Bayesian Statistics). These tools are so general and powerful, that they will last you a lifetime. However, there is so much more to learn and so many more challenges to tackle – that there is no end to the mastery of Multilevel GLMs.\n\n\n\n\n\n\nHierarchical Logistic Regression\n\n\n\n\nBeyond MLR, college basketball referee foul differential here\nGraduate Admissions, from McElreath,  UC Berkeley in Fall 1973 (numpyro)\nRasch item-response model for Nba fouls in crunch time, pymc\n\n\n\n\n\n\n\n\n\nHierarchical Poisson Regression\n\n\n\n\nAirbnb number of reviews\nEstimating the strength of a rugby team\nPaper investigating seat-belt use rates, with data probably taken from the department of transportation crashes website\n\n\n\n\n\n\n\n\n\nModels with 3 Levels\n\n\n\nAt last, it is useful to see an example where we have an even more complicated multilevel structure, as it will be the case in some practical applications.\n\nBeyond MLR 3-level seed germination\n\n\n\nEven after you learned all the technical intricacies, we’re hit again with the reality that association doesn’t imply causation. So, after being competent with this amazing tool – we have to go back to ideas from causal inference, if we want to get a more than associative insight about our theory and understanding of phenomena.",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "references.html#bayesian-machine-learning-and-bart",
    "href": "references.html#bayesian-machine-learning-and-bart",
    "title": "Bayesian Statistics: Study Guide",
    "section": "",
    "text": "As a bouns, I’m adding two flexible, general-purpose models, which you can treat as in the Machine Learning practices, but are Bayesian to the core. These are extremely useful if we care more about predictive accuracy, either for decision-making, or as a part of causal inference process, where a certain complicated relationship details aren’t important to us, but just the conditional average treatment effects.\n\n\n\n\n\n\nBayesian Additive Regression Trees\n\n\n\n\nBART from osvaldo, on bike shares\nPodcast Episode and an R package\nNonparametric methods - what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. This is the simplest and the most accessible explanation I know of so far.\n\n\n\n\n\n\n\n\n\nGaussian Processes\n\n\n\n\nGelman: Birthdays, hilbert space approximation repo, also in stan",
    "crumbs": [
      "| 1. Business |",
      "3. Applied Bayesian Statistics"
    ]
  },
  {
    "objectID": "sim/1_intro.html",
    "href": "sim/1_intro.html",
    "title": "Simulation of economic processes",
    "section": "",
    "text": "Welcome to the simulation course! First, let’s talk a little bit about movies. You’re probably well aware how popular films about multiverses and time travel have become. 1 Their main point is that our actions have consequences and that there is a “garden of forking paths” leading the protagonist to very different outcomes – some very likely, others almost impossible. This idea is faithful to real life: we have to make good decisions under uncertainty and with limited resources. It’s not an easy task.\nIf we had a crystal ball which flawlessly predicts the future state of the world and the outcomes of our decisions, we wouldn’t be here in this class. The next best thing is if the oracle tells us the calibrated probabilities of relevant events happening (e.g. if our startup or investment will succeed), but we’ll have to ask the right questions and solve a pretty large optimization problem.2 It’s also useful to know that one of the most important ideas in statistics is the counterfactual, a probabilistic answer to a “what would happen if” question.",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#decisions-under-uncertainty",
    "href": "sim/1_intro.html#decisions-under-uncertainty",
    "title": "Simulation of economic processes",
    "section": "Decisions under uncertainty",
    "text": "Decisions under uncertainty\nIn the context of businesses, we want to improve financial and non-financial outcomes like revenue, profitability (EBITDA), market share, unit economics, production efficiency, customer acquisition, user experience, customer lifetime value, etc. A firm will increase its chances to improve performance if it has an accurate diagnosis of the current state and formulated a strategy, but it still has to test their ideas and make many good decisions over a long period of time, i.e. execute well. 3\n3 Even then, there is no guarantee. Most startups fail in the first 5 years\n\n\n\n\n\nWhat makes a decision good?\n\n\n\nThink for a few minutes and write down what are your criteria which would make a decision good. How did you deal with cases in which we have a great outcome by sheer luck and chance?\nCome up with an example of an important decision a fashion e-commerce has to make. Define the objective, constraints, tradeoffs, and a few alternative choices. Test your criteria. Do they make sense? Are they necessary? Sufficient?\n\n\nI hope you figured where I am leading with the multiverse story. We study probability theory, statistical modeling, optimization (operations research), and economics in order to make better decisions under uncertainty, possibly at a large scale. We also need to know enough programming to analyze data, build models, and solve optimization problems. Think about how many decisions Uber has to make every day in each city to match drivers with passengers’ routes and set the right prices.\nI call this applied, quantitative, and interdisciplinary field “decision science”, which is also known as data science, specialized AI, and economic cybernetics. In practice, a decision scientist should collaborate with domain experts, decision-makers, clients, and stakeholders to understand their domain and challenges, ask the right questions, and formulate a problem.\n\\[Question \\longrightarrow Model \\longrightarrow Insight \\longrightarrow Action \\longrightarrow Outcome \\]\nThen, they will perform experiments, collect data, and build statistical models which bring insights into consequences of actions, interventions, and policies. 4 These insights, inferences, and predictions will inform the firm which decisions are more promising and whether their hypotheses hold up to evidence.\n4 Inspired by A. Fleischhacker’s Business Analyst Workflow\n\n\n\n\n\nWhat is a model?\n\n\n\nIn the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. Reality is overwhelming and combinatorially explosive in its possibilities. 5\nWe build models, because we want to understand a system and capture / explain the essential aspects of the phenomena of interest, in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen.\nA model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions.\n\n\n5 Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etcTo make things more concrete, let’s look at an example of a bakery which produces fresh, but perisable good(ie)s. We assume that if they don’t manage to sell the pastry in the same day, it will have to be thrown away. They choose to sell at the price \\((p)\\) after lots of market research and a bit of experimentation. The cost of producing an unit is \\((c)\\). The bakery has data on daily historical sales \\(\\{d_1, d_2, ..., d_t\\}\\) and for the sake of simplicity, we’ll assume that they never ran out of stock. The question is how much they should produce tomorrow \\((q)\\).\nThis is called the “newsvendor problem” and is a prototypical example of inventory management.6 If we order or produce too little, we’ll lose sales, and in the long term, customers. If we order too much, we’ll end up with waste. In a 2003 paper, M. Koschat and N. Kunz7 have shown how they brought an additional $3.5m of incremental profits to the Time Inc. magazine by solving this optimization problem.\n6 Even with this simple model, things can get complicated very quickly: both in terms of demand forecasting and optimization7 M. Koschat, N. Kunz - “Newsvendors tackle the newsvendor problem”, 2003\\[\n(\\max_q) \\; p \\cdot  \\mathbb{E}_D[\\min(D, q)] - c \\cdot q\n\\]\nAt first, the problem might look similar with what you did in other classes: we want to maximize the gross margin (profit) and q is our decision variable, which is a count (\\(q \\in \\mathbb{Z}_+\\)). However, the demand is a random variable which can be modeled as simply as the normal distribution or as complicated as a multilevel regression. Moreover, it’s not clear that reducing the problem to an expectation maximization is the best thing to do in practice.\nIf your hunch is that we will need to do some modeling and simulations in order to solve this problem, you’re absolutely correct.8 We will not be satisfied with \\(d_i \\sim N(\\mu, \\sigma)\\), as this will probably be an over-simplification of the processes generating the demand. In the real world, we don’t want to ignore seasonality, day of week effects, out of stock and promotions in historical data, trends, and serial correlation.\n8 It will quickly become unfeasible to find out an analytical solutionOf course, the modeling approach will be pretty different if the product is slow-moving, selling a lot, or has an intermittent demand. We will investigate these issues in great detail in the following weeks, but for now you have an example of the idea that models are a simplified representation of reality and have assumptions.\n\n\n\n\n\n\nSources of uncertainty\n\n\n\nIn their book Algorithms for Decision Making, MIT Press - 2022, M. Kochenderfer, T. Wheeler, and K. Wray make a helpful classification of the sources of uncertainty, based on agent-environment interactions:\n\nOutcome uncertainty suggests we can’t know the consequences and effects of our actions with certainty. We can’t take everything into account when making a decision\nModel uncertainty implies we can’t be sure that our understanding, assumptions, and chosen model are correct. In decision-making, we often misframe problems and in statistics, well, choose the wrong model.\nState uncertainty means that the true state of the environment is uncertain, as everything changes and is in flux. This is why statisticians argue that we always work with samples\nInteraction uncertainty due to the behavior of the other agents interacting in the environment. For example, competitive firms, and social network effects.\n\nWe will focus very little the last aspect of uncertainty, but you have some tools to reason about it: game-theoretic arguments, ideas from multi-agent systems, and graph theory.\n\n\nIn our toy example of the newsvendor problem, the uncertainty in the state (demand) translated into outcome uncertainty (profits). If we chose the wrong model for the demand, our conclusions might also change significantly. As an exercise, think of a few scenarios where the interaction uncertainty would need to be taken into account.",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#simulation-and-numerical-methods",
    "href": "sim/1_intro.html#simulation-and-numerical-methods",
    "title": "Simulation of economic processes",
    "section": "Simulation and numerical methods",
    "text": "Simulation and numerical methods\nNow that you have the multiverse metaphor in mind and the context of decision science, simulation can be seen as an extremely useful set of numerical methods and algorithms used in estimation, optimization, and modeling. Its practical value boils down to the fact that most problems and models of interest to us won’t have efficient and scalable closed-form or algorithmically exact solutions. 9 Therefore, we need to learn techniques which make good approximations.\n9 In plain english, it means that we can’t do it by hand and have to implement in code. Even if there is an exact algorithm, it might not scale with the amount of data and problem size – which most often renders it useless In the following sections, I explain different ways in which simulation and numerical methods can be applied, which is much more than the “Monte-Carlo” method you might be familiar with. Last, but not least, simulation is extremely useful when learning probability, statistics, and econometrics. It’s a way to take an abstract mathematical idea, for example, Central Limit Theorem or HAC robust standard errors, see how it behaves under different assumptions, and how it could be useful in practical applications.\n\nTheoretical and process models\nThere is a famous saying that explains some differences between natural and social sciences: “Atoms are not people”. Human behavior is an endlessly fascinating and complex area of inquiry, but we can’t hope to achieve the same level of confidence in theories and hypotheses we develop as physics, chemistry, or biology. Also, we can’t perform experiments in the way engineers do – it will be highly unethical and often unfeasible.\nTherefore, in applied economics, mathematical modeling takes a back seat to statistical, econometric, and machine learning methods.10 Whatever “laws” we might conjecture, it will be incredibly difficult to empirically validate them and “prove” that the decisions / interventions made bring an improvement. Often, when it comes to consumer behavior, we don’t even have a theory: think about the choices of what movies to watch on Netflix or the premium we’re willing to pay for a branded hoodie. But we do have an understanding and we might have tons of data.\n10 Recent Nobel prize awards in economics show how important econometrics and causal inference has been for the fieldYou have to know that “theory-free methods”, for all practical intents and purposes, do not exist. Even when we don’t make distributional assumptions (in nonparametric statistics) and learn a pattern from data (machine learning), we still do have to decide what are the constraints, objectives, relevant data, measurement, sampling, interventions, and most plausible causal mechanism underlying the behavior / observed phenomenon.\nThere are multiple techniques for model validation which you will study in econometrics / data analysis classes and they’re extremely important. They will signal to you possible ways in which the model fails and that you might’ve picked the wrong one, but they’re not sufficient. How so?\n\n\n\n\n\n\nThe causes are not to be found in data\n\n\n\nImagine two different models with wildly different assumptions which give similar predictions. We’ll conveniently call them “heliocentric” and “geocentric”. How can we know which one is more plausible?\nThe short answer is not from one dataset, and not from a single thing we measure, but from “networks of cumulative evidence” and competing against the next best hypothesis. 11\nThe idea is not to celebrate the triumph of Galileo/Copernicus, but to point out that such debates happen routinely. A few examples are the fields of molecular biology and ecology which spanned decades-long bitter rivalries.\nThe explanation is simple once you are aware of it – that different vague hypotheses can be operationalized in multiple ways as mathematical / process models, which might result in very similar statistical and distributional patterns.\n\n\n11 I highly recommend that you read a little bit about the philosophy of science and how the scientific process works.I hope you bear with me along this justification to finally get to the point of the value of simulation. It’s a way to implement probabilistic models of economic phenomena in code, which forces us to declare explicitly our assumptions about the causes and processes underlying the data. Recall the idea of “data generating process” from statistics.\nThink of these models as an airplane or driving simulator, which will resemble real piloting if enough effort has been put into its development and enough scenarios have been taken into consideration. Even though they’re just a simulation, we can use them to practice and gain understanding.\nIn this course we’ll be much less confident that the patterns which result from our simulations are realistic,12 but it will be immensely helpful when building statistical models and using real data to solve business problems. This preparation will increase our chances of success in practice and will give more confidence to stakeholders that the approach we’re taking is reasonable.\n12 We’ll have to make many simplifying assumptions and can’t take into account all relevant factors or ways things can go wrong\n\n\n\n\n\nThink generatively. Does the model work in principle?\n\n\n\nThink for a minute what is the difference between probability and statistics. Probability is generative (of data) and forward-looking, predictive, given a set of “rules” and assumptions we’ve chosen. Probability is mathematics – a logic of uncertainty.\nOn the other hand, statistics is about chaning our actions and opinions in the face of evidence. It looks backwards, given the data and a chosen model – trying to infer and estimate what the “rules” or parameters were. Once we have those estimates, we can make predictions and generalizations. It uses mathematical methods, but is closer to art and engineering.\nSo, one important way in which simulation is used in practice is checking that our modeling strategy works in principle and what are different ways in which it can fail and mislead us. This is important before commiting to an expensive real-world experiment or intervention, be it a medical study, marketing campaign, or a macroeconomic policy.\nIsn’t it important to know if our modeling idea or experiment is doomed or unfeasible from the very start?\n\n\nThis process in which we try to see how the model behaves under different assumptions is called sensitivity analysis. It is an important exercise to do, because we can’t be certain our assumptions are correct and it’s likely we won’t have the necessary data or experiment to check one key assumption. We might be lucky and our models robust, or we could get wildly different results and recommendations on a slight change. It is clear that if we don’t check, we’ll never know before the costly project.\n\n\nWhat do engineers do?\nBefore moving on to the applications of simulation to optimization and statistical inference, I think it’s important to differentiate this class from what engineers would typically study in “Numerical Methods and Scientific Computing” or “Computational Methods for Data Analysis” classes.\n\n\n\n\n\n\nWhy do engineers put so much effort into simulation?\n\n\n\nYou might have friends which study physics, chemistry, or all kinds of engineering. If you ask for their numerical methods textbook, you will be shocked at how hard it is. Why?\nIn short, because their fields require complicated mathematical modeling, like partial differential equations, fluid dynamics, signal processing, 3D rendering, and so on. So, it’s not only challenging to implement/solve them on a computer, but it’s hard to do it efficiently at scale. And when we talk about climate modeling, for example, it’s the damn planet-scale.\nI’ll also give credit to quantitative finance, where modeling becomes incredibly hard – with stochastic differential equations and other mathematical nightmares.\n\n\nThere is a series of important tools which we will NOT cover,13 but might be useful in data analysis. These include SVD, QR, LU, and Choletsky matrix factorizations which pop-up in machine learning; Fourier transforms and wavelets which are important in signal processing and high-frequency time series; interpolation, approximation methods like Runge-Kutta, Euler-Maruyama for differential equations and SDEs.\n13 It’s best that you study these methods when you encounter them in context, in order to understand what do those software packages do behind the scenes\n\nStatistical inference and optimization\nAt this point you probably encountered only the simplest optimization and estimation methods, like least squares, generalized method of moments, and simplex algorithm. These algorithms are fundamental and have a very nice set of properties, but they apply to a very specific (and narrow) set of problems and models.\nEfficient parameter estimation and highly-dimensional, nonlinear optimization is an active area of research which has lead to recent breakthroughs in machine learning and AI. You could build an applied or research career in this field, if you wish so – it will be in demand for a long time. However, from the point of view of decision science, these are “just” methods so that we can get stuff done, i.e. answer our research question and make the right business decision.\nIn frequentist statistics, we want point estimates of parameters and their confidence intervals, but for more complicated models we often don’t closed-form solutions. Even worse, we might not even have an idea of how the sampling distribution looks like, which will make us doubt if our hypothesis test was reliable.\nIt is the job of theoretical statisticians to figure these things out, but we’ll inevitably encounter a problem in which we don’t want to make certain statistical assumptions – therefore, no assumptions, no guarantees. Some methods which come to rescue are maximum likelihood, versions of gradient descent, and bootstrap.\nIn Bayesian statistics,14 we view parameters as distributions and data as fixed. The advantage is that we can report the whole posterior distribution as our estimate, capture the uncertainty, and propagate it when making predictions. Does it sound more intuitive than frequentist inference? Yes, but there are modeling gotchas and computational gotchas.\n14 Don’t worry if it doesn’t make sense right now, I’ll show some simple models and the benefits of Bayesian inferenceAgain, the most interesting models to us will not have a closed-form solution, but this time, because we work with samples from probability distributions, the estimation will be muuuuch slower. This is where modern sampling methods like Markov Chain Monte Carlo (MCMC) and Hamiltonian Monte Carlo (HMC) allow us to have our cake (reap the benefits of the Bayesian approach) and eat it too (make it computationally feasible for “mid-sized” datasets). If we work with millions of data points with a large number of features, we’ll definitely have to reconsider.\nIn the previous section we were concerned with causal and scientific / economic aspects of statistics. But it’s also clear that we need reliable and efficient estimation, data analysis, model validation and diagnostics. It’s important that you keep in mind both of these aspects, as most classes focus on one or another.\n\n\n\n\n\n\nThree challenges in statisical modeling\n\n\n\nA. Gelman highlights three different aspects of statistical inference. Always remember this when designing your study or learning about a new statistical tool! We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.\n\\[Sample \\longrightarrow Population\\]\n\\[Treatment \\longrightarrow Control\\]\n\\[Measurement \\longrightarrow Construct\\]\nThe holy grail is to build statistical models based on the causal processes informed by theories and hypotheses. If we take into account how we measured, and collected data, we’ll increase our chances to generalize our conclusions and will have stronger evidence.\n\n\nIn machine learning, for the vast majority of models, the training process boils down to a nasty, nonlinear optimization problem, in which we hope that we won’t end up in a local minimum. As a slight detour, the training process is an algorithm which takes as an input the model, its hyperparameters, and data, which returns another model (with optimal parameters) which hopefully has the best out-of-sample, predictive performance.\n\\[\\mathcal{A} : \\{\\mathcal{(X_i, y_i)}\\}_{i=1}^N, m, \\gamma \\longrightarrow m^* \\]\nThe good news is we have many variations of stochastic gradient descent which work well for most models and a whole array of more sophisticated techniques when it fails. Think of the transformer architecture which made ChatGPT possible – training would be unfeasible with more “naive” optimization algorithms.\nA hyperparameter is a quantity which governs the behavior of the model and is not directly learned from data, so we’re responsible for making an informed choice or determining it empirically. This process is called hyperparameter optimization.15 If we have a big model which trains for a long time, clearly, we will have to try as few hyperparameters as possible.\n15 There are many tree-based models like LightGBM and Random Forests which are pretty good out of the box, and in practice don’t need much hyperparameter tuningGenerally, this problem is known as optimization of (expensive) black box functions. The key here is to approximate the slow model via a flexible, but much quicker and simpler one, then iteratively pick hyperparameters which will get us closer to optimal values. We will not cover this techniques here, or in introductory data science classes, but you should know about this solution when you encounter such a problem in practice.",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#stories-and-case-studies",
    "href": "sim/1_intro.html#stories-and-case-studies",
    "title": "Simulation of economic processes",
    "section": "Stories and case-studies",
    "text": "Stories and case-studies\nSo far, you studied probability, statistics, operations research, and economics from an ultimately mathematical and procedural point of view (i.e. steps to solve a problem). This is a solid foundation to have, but it needs to be balanced out with practical aspects of modeling, data analysis, and programming.\nI will be using stories and real world case-studies to highlight the practical relevance of theoretical ideas like Central Limit Theorem (CLT), Law of Large Numbers (LLN), p-values, conditioning, common distributions, etc. Moreover, by simulation, we’ll reinforce our knowledge and understanding, which will help us avoid most common pitfalls in statistical modeling.\nA funny thing is that we can use simulation to better understand probability and statistical inference, and at the same time, probability justifies why simulation works. I can’t emphasize enough how much your learning will improve if you will apply what you learn here in your econometrics, data analysis, and quantitative economics classes. If you do not trust me, trust Richard Feynman, who said: “What I cannot build, I do not understand.”\nYou can think of this course as having two parts. First, we use simulation as a problem-solving approach to gain insight into an economic problem. The second part develops specialized methods for estimation, sampling, approximation, and optimization – which can be viewed as tools to overcome a range of technical problems in statistical modeling.\n\n\n\n\n\n\nThe bare minimum\n\n\n\nSimulation is perhaps the most beginner-friendly course you had, because we need to know just two things to get started.\n\nHow to generate iid, uniformly distributed pseudo-random numbers. This problem is solved, since all programming languages have good RNG (random number generators).16\nHow to generate samples from any probability distribution which is “well-behaved”. Fortunately, a theorem called “Universality of the Uniform” proves that we can and gives us the method for doing so. In R or Python we have access to efficient implementations for the vast majority of distributions we’ll ever need.\n\nThis is not sufficient to understand why simulation works, how to apply it effectively, or how to sample from complicated distributions which don’t have a closed-form solution. However, you can still go a long way in practice with these two simple facts.\n\n\n16 You should still know how are they computed behind the scenes and what happens when they are not-so-randomFirst, we’ll need to set-up the programming environment (R and RStudio), create a script or (quarto) notebook and we’re ready to go! Take your time to understand how to navigate the IDE, run commands, investigate the errors, and read the documentation. We want to solve problems and program, thus techical issues like how to run a line of code or where can I find the output shouldn’t stand in our way.\n\n\n\n\n\n\nThe full-luxury development setup\n\n\n\n\nR v4.4.2 (later than 4.3.x)\nRStudio as our main IDE\nQuarto or Rmarkdown for literate programming (only needed towards the end of the course)\nInstalling tidyverse will get us most needed packages:\n\ndplyr for data wrangling, purrr for functional programming, and stringr / glue for making our life easier with text\nggplot for data visualization\n\n\nR is a beginner-friendly language, but has many gotchas because of its weak data types. Tidyverse is an important ecosystem of packages which solves a lot of the issues in base R and makes our life easier and coding much more pleasant.\nIf you’re really serious about becoming a data scientist or a ML engineer, you will have to study and practice a lot on your own. This is a non-exhaustive list of practical skills you will need in the future.\n\ngit & github for versioning your code and collaborating on a code-base\nrenv for managing environments and packages\nduckdb to practice SQL and data engineering on a analytical, columnar, in-process database\ndata.table and arrow for processing huge amounts of data\nhow to build interactive applications in Shiny and model serving APIs in plumbr\nhow to use the command line interface and automate stuff in Linux\nhow to package and deploy your training and prediction pipelines, possibly to a cloud provider\n\n\n\nWithout further ado, here are the stories and case-studies we’re going to discuss and implement in code, along with theoretical ideas they highlight.\nWe’ll start with a warm-up. Birthday paradox is a straightforward, but counter-intuitive result which is a good opportunity to review key concepts from combinatorics and apply the naive definition of probability. We’ll derive the analytical solution, visualize it, and compare with our simulation. This simple exercise will teach us most things we will need for the near future: how to sample, generate sequences, work with arrays, functions, data frames, and repeat a calculation many times.\n\nNewsvendor problem and inventory optimization is a great case-study of decision-making under uncertainty\nShowing up to a safari. This is a cute story which will teach us about the intuitions behind Binomial distribution, probability trees, independence, and sensitivity analysis\nSimulations which exemplify convergence in probability and the law of large numbers. We’ll discuss why Monte Carlo methods work and what happens if we forget to take into consideration sample size\nUS schools, the most dangerous equation. This is a great example of what can go wrong when we draw conclusions from data via a sloppy analysis.17 CLT is just one of key theoretical ideas from statistics which could’ve prevented the policy makers to start a costly and wasteful project.\nQuality control and measurement error. We’ll discuss the story of Gosset in Guiness beer factories, the original purpose for which the t-test was invented, the philosophy and practice of hypothesis testing. A key idea is one of action in the long run and error control (not being too wrong too often). This perspective of statistics justifies our definition of “changing your actions in the face of evidence”.\nWhat p-values can we expect? Most people misunderstand the idea of p-values. We will use an example of a scientific study and its probability to find a true, significant finding. In simulation, we can see the distribution of p-values, which is impossible to know for sure in practice, even after a rigorous meta-analysis.\nWikipedia A/B test and Landon vs Roosevelt elections. These stories serve as drills so that you remember how to calculate confidence intervals for proportions and interpret them correctly. They also serve as a warning of what can go wrong if we fail to randomize.\nBayes rule, medical testing, and coding bugs. Bayesian thinking and the idea of updating your beliefs is key for rational decision-making under uncertainty. You will also get into the habit of articulating and elliciting your prior knowledge (before seeing the data) about a problem.\nBeta-Binomial model of left-handedness and quality control. This will be perhaps the only time in your bachelor’s degree where you will encounter a fully Bayesian approach to inference (and not just an application of Bayes rule). We will learn what probability distributions are appropriate in modeling proportions and a principled approach to deal with misclassification error.\n\nI will mention how can we model counts of asthma deaths and kidney cancer with the Gamma-Poisson model, and how it can be applied to customer purchasing behavior.\nBy a similar reasoning we’ll see how can we model waiting times in a Starbucks by a Gamma-Exponential model. This is precisely the reason why you studied all of those discrete and continuous distributions in your probability class – they have a purpose and are useful!\n\nLinear regression and confounding. You probably heard a hundred times that correlation doesn’t imply causation. I will show three simple causal mechanisms and graphs of influence which can mislead us into a wrong conclusion: confounders, mediators, and colliders. We’ll discuss real-world studies about divorce rates, gender discrimination, and ethical intuitions.\n\nIn the context of linear regression, we’ll use bootstrap as an alternative way to compute confidence intervals. It’s a must-have technique in your quantitative toolbox, which will be useful any time you don’t have a theoretically proven sampling distribution.\n\nJustifying the sample size for an A/B test. Power calculations is what trips up most product managers and analysts, who end up confused by the online calculators or complicated formulas (which I’ve seen mostly misused). This is where simulation shines and will be helpful in choosing the appropriate sample size of data we need to collect (or how long the experiment should run), while being very clear about our assumptions and expectations.\n\n17 I really like the metaphor of “fooled by randomness”At last, after we get a sense how Markov Chain Monte Carlo works, we will gain a new superpower – to sample from unknown distributions. We can apply it to the modeling of grouped / clustered / correlated data, like in a classical case-study of radon (toxic gas) concentrations in U.S. homes in various counties.\nThe idea of partial pooling will help us to automatically adjust our inferences (means and variances) with respect to the sample size of each group (county). This is an example of a multilevel or hierarchical model, for which Bayesian inference has many benefits. However, we don’t have analytical solutions and will have to use a much more sophisticated algorithm to generate samples from the posterior distribution of parameters.\nTreat this topic of multilevel modeling as the culmination of the course, which also serves as a powerful complementary approach to what you study in econometrics.",
    "crumbs": [
      "| 1. Business |",
      "~ Simulation of economic processes"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html",
    "href": "01_fundamentals/0_study_guide.html",
    "title": "Business school study guide",
    "section": "",
    "text": "In the study guide we go into more details and granularity – with references, readings, and practices for particular topics within each lecture. I suggest to read about the course philosophy, which motivates why it is structured in this way. If you’re unsure about the prerequisites and background knowledge, refer to “Why did you study all of that?”.\nflowchart TD\n\n    DM[Decision Science] -- big picture --&gt; App[Domains & Use-Cases]\n     --&gt; Strat[Business Strategy] --&gt; Adv[Analytics vs ML vs Stats]\n\n    DM -- bussiness econ --&gt; NW[Newsvendor] --&gt; PD[Pricing Decisions] \n        --&gt; Churn[Churn & LTV]\n\n    DM -- methodology --&gt; ML[Learning & Bias] --&gt; ML12[12 Steps of ML] \n        --&gt; Stat[12 Steps of Stats]\n\n\n    style NW fill:#f7f5bc\n    style PD fill:#f7f5bc\n    style Churn fill:#f7f5bc\n\n\n\n\nFigure 1: A mind map of lectures grouped by theme so you can navigate the study guide easier. See below all the topics, case-studies, references, and readings for each chapter or lecture.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#introduction-and-course-overview",
    "href": "01_fundamentals/0_study_guide.html#introduction-and-course-overview",
    "title": "Business school study guide",
    "section": "Introduction and course overview",
    "text": "Introduction and course overview\nFirst, make sure you read the introduction for a motivation and brief summary of what are we going to study, then review the schedule and syllabus in order to understand how the course is structured in particular lectures.\n\n\nIf you’re a student, please, don’t skip the readings in the first two lectures. These are perhaps the most important learnings from 10 years of practice and will serve you well.\n\nUnderstand the particularities of decision-making in businesses\nWhy I prefer “decision science” over “data science”\nNecessary admin stuff and project requirements. Think about your goals, aspirations, interests and hobbies, so that you can pick a fun use-case\nGo through the slides again after the readings and lectures\n\n\n\n\n\n\n\nActivity: What is the purpose of the business?\n\n\n\nOn the card you received in the class, answer the following questions:\n\nAnswer from 1 (strongly disagree) to 5 (strongly agree) to each to the possible purposes (raison d’etre) of a busines.\nAre you considering to start a business in the next 5 years?\n\nNow, imagine you’re in charge of public policy, or take part of an entreprenorial think-tank or NGO for youths. Formulate a research question that a further study would try to answer. It doesn’t have to relate to the particular questions you answered before.\n\n\n\n\nIf you’re serious about data science and statistical fundamentals, I highly recommend the following two books by A. Gelman for self study: Regression and Other Stories and Active Statistics\n\n\n\n\n\n\nWikipedia donations experiment\n\n\n\nThis story can be found in “Active Statistics” and “Regression and Other Stories” by A. Gelman.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#business-context-decisions-and-strategy",
    "href": "01_fundamentals/0_study_guide.html#business-context-decisions-and-strategy",
    "title": "Business school study guide",
    "section": "Business context, decisions, and strategy",
    "text": "Business context, decisions, and strategy\nI build upon the previous lecture to develop the story of decisions in a business environment. We need additional terminology, concepts, and models in order to structure the problem space. What is key here, is to get immersed and put yourself in the shoes of decision-makers. Read the second chapter.\n\nEvolution of firm’s performance in time. Why, Where, How? 1\n\nStatus quo, desired, and feared trajectories.\nTradeoffs and limited resources, SWOT Analysis\n\nA deep-dive into Business Analyst’s Workflow\nWhat is strategy? What makes good or bad strategy? 2\nWhat is a Value Chain? Data Science Strategy Safari 3\n\n1 Read Kim Warren’s article on “The Dynamics of Strategy” here2 Read the article published in McKinsey by Richard Rumelt “The perils of bad strategy”3 Read about the framework of aligning business objectives to data science use-cases. Data Science Strategy Safari at bayesianquest\n\n\n\n\n\nSurvey of use-cases in various domains\n\n\n\nAs a second activity, think about as many domains and use-cases as possible in which AI and data science plays an important role. Put yourself in the shoes of the firm: how would you reverse-engineer the product? Then, see what resonates with you here\n\n\n\n\n\n\n\n\nCase-Study: LRB Subscriptions\n\n\n\nTake a subscription-based publication like London Review of Books and analyze it using the tools and frameworks you learned.\nThink about acquisition, churn, printing, transportation, and market share. How can it grow? What would make it profitable? What are the key decisions to be made?\n\n\n\n\n\n\n\n\nCase-Study: Food Stamps and Fermi Estimation\n\n\n\nThere is a good reason why management consultants practice Fermi Estimation (sanity, order-of-magnitude checks) and it is asked in many interviews.\nTake a look at the case study on Food Stamp Fraud by Carl Bergstrom and Jevin West in Calling Bullshit. Practice on other classical Fermi estimtion examples.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#decision-making-under-uncertainty-at-scale",
    "href": "01_fundamentals/0_study_guide.html#decision-making-under-uncertainty-at-scale",
    "title": "Business school study guide",
    "section": "Decision-making under uncertainty at scale",
    "text": "Decision-making under uncertainty at scale\nIn the previous lecture we learned how data science fits into the larger context of a business. The ideas of value chain and business analyst’s workflow are immediately applicable in practice. Now, read the third chapter.\n\nClarify what I mean by AI and Cybernetics and the historical confusion 4\nWhen do we need Analytics vs Statistics vs Machine Learning?\nA note on interdisciplinarity and thinking in buckets\nAgents, environments, and sources of uncertainty\n\n4 K. Pretz, “Stop Calling Everything AI, Machine-Learning Pioneer Says”; M. Jordan, “Artificial Intelligence - The Revolution Hasn’t Happened Yet”\n\n\n\n\n\nCase-Study: DTC Fashion E-Commerces\n\n\n\nChoose a direct-to-consumer e-commerce, like Zara, H&M, or a marketplace like Zalando (the big players in the oligopoly). How does a potential business strategy look like? What are some use-cases for Data Science / AI which can improve their outcomes?\n\n\n\n\n\n\n\n\nCase-Study: Uber and Dynamic Pricing\n\n\n\nIn the case of Uber, Bolt (ride sharing platforms), we’re very much interested in same questions as before, however, we need to get a grasp on the idea of market-making. If you were to reverse-engineer the pricing algorithms, how would you go about doing it?\n\n\n\n\n\n\n\n\nOverview of open datasets\n\n\n\nIt is very hard to find good, realistic datasets which map well on representative use-cases from this course. This is why I curated a list of public datasets in a variety of domains. These should help if you don’t know where to start your project, that is don’t have particular problems, hypotheses, or research questions in mind.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#newsvendor-problem-and-demand-planning",
    "href": "01_fundamentals/0_study_guide.html#newsvendor-problem-and-demand-planning",
    "title": "Business school study guide",
    "section": "Newsvendor Problem and Demand Planning",
    "text": "Newsvendor Problem and Demand Planning\nDemand planning, especially demand forecasting and inventory optimization is an obvious and widespread application of the models and methods from the course. The newsvendor problem is a good way to get started in this area of businesses.\nIn the economics’ courses you might’ve solved this problem analytically, under strong assumptions, but we have a huge problem of trying to quantify and take into account the uncertainty. Therefore, we rely on simulation, motivate the need for statistical inference and optimization algorithms. 5\n5 There is an excellent presentation of the newsvendor problem by Adam Fleischhacker in Chapter 3 and Chapter 5 of “Persuasive Python”. Chapter 20 of his Business Analytics book contains another classic economic problem interpreted in modern way. If you prefer, there is a video lecture on the newsvendor.\n\n\n\n\n\nCase-Study: TIME INC. printing\n\n\n\nIn the paper “Newsvendors Tackle the Newsvendor Problem”, Koschat, Berk et. al. show how an analysis and optimization of printing decisions led to TIME INC. to revise its policies and generate additional \\(\\$3.5m\\) in profit.\n\n\n\n\n\n\n\n\nCase-Study: Restaurant Data and Project Ideas\n\n\n\nBesides Persuasive Python, you can find an alternative case-study on “Yaz” restaurant data, which makes playing around with the newsvendor problem easy, due to its Python package.\n\n\nFor an academic paper with a meta-analysis of data-driven approaches to the newswendor problem, check out S. Buttler, A. Philippi. If you have a demand planning use-case at work, the best resources are written by N. Vandeput and I. Svetunkov’s CMAF seminars.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#learning-intuition-and-bias.-what-is-ml",
    "href": "01_fundamentals/0_study_guide.html#learning-intuition-and-bias.-what-is-ml",
    "title": "Business school study guide",
    "section": "Learning, Intuition, and Bias. What is ML?",
    "text": "Learning, Intuition, and Bias. What is ML?\nAt this point you will have a better grasp of the business environment, the processes involved in decision-making and had exposure to multiple contexts via case-studies. The newsvendor problem exposed the fact that we need additional skills and understanding of statistical modeling, machine learning and analytics. Read chapter five.\nI start the introduction to machine learning in an unorthodox way, from a cognitive science perspective. The reason is: because we gain additional insights into decision-making. But don’t worry, top courses like Shai Ben-David’s “Understanding Machine Learning” 6 and Yaser Abu-Mostafa’s “Learning from data” 7 start with similar motivations.\n6 Shai-Ben David - Understanding Machine Learning, 2014. Read chapter 1 and 2 for the intuition. The following ones are difficult, mathematical, but highly rewarding if you want to understand the theoretical foundations of ML.7 Learning from data is still a great foundational course a decade later. The teaching in the recorded lectures is exceptional\nImplicit learning, intuition, and bias. Bait shyness and pigeon superstition\nThe double edged sword of our intelligence. Bounded rationality\nCalling bullshit in the age of Big Data. Small data problems in Big Data\nIntelligence, Rationality, Wisdom, and Foolishness\nThe learning problem and empirical risk minimization\nThe surprising consequences of Bias-Variance tradeoff\n\n\n\n\n\n\n\nCase-Study: ML workflow Demonstration\n\n\n\nWe can pick a (seemingly) simpler use-case like housing prices prediction, credit default risk assessment, churn prediction, demand forecasting, or recommender systems – and show how training and prediction look like in code.\nI will show that there is much more nuance behind the scenes than what kaggle datasets show, and that often the problem framing makes ML look deceptively simple.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#steps-of-machine-learning",
    "href": "01_fundamentals/0_study_guide.html#steps-of-machine-learning",
    "title": "Business school study guide",
    "section": "12 Steps of Machine Learning",
    "text": "12 Steps of Machine Learning\nBy now, I introduced the bare minimum of motivation, formalism, and practices of Machine Learning for you to get by. However, we need a structured approach to tackle real-world problems with ML models. The methodology I like most is articulated by Cassie Kozyrkov, (ex) Chief Decision Scientist at Google. 8\n8 Cassie Kozyrkov - Making friends with machine learning, full course. It is 6 hours which will serve you during the entire carreer\nPresent the 12 steps with representative examples of classification and regression\nIs the ML project feasible? If yes, is it a good idea?\nSplit your damn data. Cross-Validation and hyperparameters. Pitfalls\nSimilarities with CRISP-DM, tuckey’s EDA, and tidy modeling\n\n\n\n\n\n\n\nCase-Study: Mercari pricing challenge\n\n\n\nI like this Kaggle challenge and dataset, because of how realistic and open-ended it is. You can keep it simple, with out-of-the-box models, or build some highly customized ML pipelines.\nIt also has data which requires a combination of different approaches to feature engineering. It is messy enough that we have to justify how we deal with those missingness patterns and weird data points. Those modeling decisions will have an impact on model performance.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#price-optimization-under-limited-capacity",
    "href": "01_fundamentals/0_study_guide.html#price-optimization-under-limited-capacity",
    "title": "Business school study guide",
    "section": "Price optimization under limited capacity",
    "text": "Price optimization under limited capacity\nIn contrast with the newsvendor problem (where prices were fixed), it’s time we focus on pricing decisions. The limited capacity of airplane seats, concert tickets, hotel rooms, on the one hand simplifies the problem formulation, but introduces many complexities in practice.\nThis seemingly simple problem motivates why the young field of revenue management was created. When forecasting the demand, we need to think about how to model it well 9 and take into account most relevant factors influencing it. This is where we have to be careful, or at least aware of what economic theory has to say about demand elasticity, competition, and price discrimination.\n9 This is where what we learn in Module 3: Applied Bayesian Statistics, especially the hierarchical models help a lot.On the optimization side, we will keep things simple, as in the newsvendor problem. We will not go into the complexities of dynamic programming.\n\n\n\n\n\n\nCase-Study: Concerts, Events, and Airlines\n\n\n\nWe can use the Kaggle simulator for dynamic pricing in the airline industry, get creative with the Mercari Challenge or other open datasets. There isn’t a firm which will make such kind of data public, so we need to get creative in how we simulate, estimate, or collect the data from what is available.\nAdam Fleischhacker has an excellent tutorial (Chapter 24: Compelling Decisions and Actions Under Uncertainty) which is a good starting point.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#steps-of-statistics.-ab-testing-scheme",
    "href": "01_fundamentals/0_study_guide.html#steps-of-statistics.-ab-testing-scheme",
    "title": "Business school study guide",
    "section": "12 Steps of Statistics. A/B Testing Scheme",
    "text": "12 Steps of Statistics. A/B Testing Scheme\nI assume you have been working on Module 2 (Fundamentals of Statistics and Probability) in parallel, so that we can get to a point where we discuss the practical and methodological aspects of A/B testing and randomized experiments.\nIn this lecture I bridge the gap between the theory/fundamentals and practice. We will see how an end-to-end process of experiment design looks like. In the end, you will learn how to make justified choices in how you set-up and run an experiment.\n\n\n\n\n\n\nCase-Study: Conversion Rate Optimization\n\n\n\nMarketing is one area of online businesses which heavily relies on randomized experiments and A/B tests. Firms try to make the most out of their advertisement spending (customer acquisition), by testing changes in user experience, promotion, merchandising, to convince a larger proportion of them to buy the product or subscribe (conversion rate).\nRead this end-to-end example, which takes a lot of attention and care into verifying potential pitfalls. If you need other perspectives about what can go wrong, you can refer to: HBR article, 8 pitfalls and solutions, A/A tests, and user interference\n\n\n\nBefore jumping into the hypothesis testing, we should carefully ask whether we need an experiment at all. 10 Maybe we want to explore or predict?\nDefault action is one of the most important ideas in statistics.\nWhat is a statistical hypothesis anyways? How does it relate to the original question?\n\nMake sure you define the minimal effect size of interest/relevance\nType 3 errors: be careful what is the question you really ask of your test\n\nWatch the 12 steps to statistics by Cassie Kozyrkov for a methodology of how to perform experiments in business settings. For a more traditional exposition, check out Poldrack’s Chapter 9.\nOne of the most difficult aspects in practice is metric design, since we deal with tradeoffs so often. An useful way to think about properties of good metrics is described in this paper, STEDII. Also see this example discussing sensitivity, stability, and directedness.\n\n10 Read Cassie Kozyrkov’s article to recognize when “not to waste your time with statistics”. You can also watch this video. You can also see her full, short lecture for Google employees.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/0_study_guide.html#churn-and-lifetime-value.-open-datasets",
    "href": "01_fundamentals/0_study_guide.html#churn-and-lifetime-value.-open-datasets",
    "title": "Business school study guide",
    "section": "Churn and Lifetime Value. Open datasets",
    "text": "Churn and Lifetime Value. Open datasets\nCustomer retention and repurchase is what makes or breaks businesses, especially in the past few years, with so many subscription and SAAS business models. Therefore, this topic deserves a full lecture – to cover the contractual and PAYG/non-contractual settings.\nI show that predicting churn and remaining customer lifetime value is not just a simple classification / regression problem. First, we need some tools from survival analysis and second – what we really want to know is who can be convinced to remain by an intervention, like promotions or loyalty programs. 11\n11 This is a good opportunity to introduce the idea of Uplift and Calibration of classification scores returned by a ML model.\n\n\n\n\n\nCase-Study: BTYD models (Buy till you die)\n\n\n\nThere is a body of work spanning two decades by Bruce Hardie and Peter Fader, who developed statistical models that estimate the remaining LTV (Lifetime Value) of customers, which depends on their survival/activity and repurchase patterns.\nWe will implement a model for contractual and non-contractual settings on the limited data available on the web. Luckily, that would be sufficient to apply them for your own use-cases.\n\n\nLast, but not least, I present an overview of curated open datasets from various sources, organized by industry – which should serve as an inspiration for future practice and your project. There is a shortlist of the ones I found most intriguing and representative, then a long tail of alternatives and fun stuff.\nIn previous courses you probably did a lot of exploratory data analysis. It is a skill which demands lots of practice and experience in order to do it well (finding inspiration and interesting patterns in data). Having a methodology for EDA helps, however, in practice we have to work with databases.\n\n\n\n\n\n\nCase-Study: DuckDB and e-commerce database\n\n\n\nI use a relational dataset from a real e-commerce which was made public in Kaggle, in order to showcase how to interact with databases and highlight the importance of knowing SQL.\nWe use a recent innovation in databases, duckDB, which allows us to have an analytic, in-process database, with almost zero setup or dependencies. Once the e-commerce data is loaded and modeled inside the DB, we can start cleaning it, putting it all together, and extracting insights from it. This particular dataset is very rich in information and lends itself well to open-ended investigations.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology"
    ]
  },
  {
    "objectID": "01_fundamentals/4_ml_stats.html",
    "href": "01_fundamentals/4_ml_stats.html",
    "title": "Analytics, ML or Statistics?",
    "section": "",
    "text": "Analytics, ML or Statistics?\nAt this point, you should have a pretty good idea why data science is important, what are some possible applications and domains, what does it do and is concerned about. It is the motivation, real-world use-cases, and conceptual understanding that I promised at the beginning of the course.\nDisentangling the ambiguity around AI was one of the most difficult aspects of the course to articulate. Now, it’s time to transition to a lower level of analysis (inside data science, not outside it), break down the landscape into manageable chunks and develop our toolbox, in which we learn how to formulate problems well and match them with existing models, methods, and technologies.\nOne of the first tools I want to introduce, is distinguishing three ways of thinking, which have to work harmoniously together, in order for a data science project to be successful:\n\nAnalytics and Data Mining, where the main goal is formulating better research questions and hypotheses, that is, get inspiration, find interesting and relevant patterns and relationships in massive datasets\nMachine Learning, as a way to use training algorithms to go automatically from experience (data) to expertise (a program or recipe for prediction). Put in other words, learning from data, finding invariants, patterns which generalize beyond our sample and training data.\nCausal Inference 1 for making decisions with high stakes, where we have to understand the causal processes of the system in order to intervene optimally. It enables greater transparency, reliability, and rigor in the inferences and conclusions drawn.\n\n1 I find that statistics is not the right term here, as it is too broad, especially recently when the lines between ML and Stats are getting more blurry, as one adopts methods from other.\n\n\nSource: xkcd; Which one to choose and when? We will have some unlearning to do here, as much of the previous courses were in one way or another focused on analytics. Even in statistics or econometrics, there is little from the field of causality, which is necessary to apply it to real-world challenges. On the other hand, you applied ML methods, but in my opinion, without understanding what ML is, without a rigorous process which would ensure we don’t overfit or snoop the data, without a clear plan of how to deploy it to production and make decisions based on those predictions.\n\n\n\n\n\n\n\n\nThe art of formulating a hypothesis\n\n\n\nIn many intro to statistics or science courses, we take for granted the hypothesis, it is often our starting point. How does one come up with a business or scientific hypothesis with makes sense, is reasonable, plausible, deserves serious consideration? Since there are an infinity of possible ones, how do we pick the most relevant?\nI argue, it is an art which requires a kind of intuition, sensibility, and attunement to the problem. In my opinion, it is the most underrated aspect of scientific enquiry and process: a good problem formulation often gets us halfway towards a solution.\nIn this course we focus on data mining and analytics as a way to get inspiration for good questions to ask and hypotheses to formulate. However, in anthropology or evolutionary biology, it could be done by careful observation of the behavior, coupled with a deep understanding of the field, existing theories and their shortcomings and inconsistencies. Often, these hypotheses follow as a consequence from the theory itself.\n\n\nIf we don’t have to make decisions and want to find interesting patterns in data, to inform our future questions, we have lots of methods for exploratory data analysis – from visualization (manual) to clustering (automated) and model-driven exploration. Sometimes, we just want to monitor and display the facts and current state of a business on a dashboard – this is why your previous class was on BI (Business Intelligence).\nThen, in the decision-making processs, these questions and hypotheses can be communicated to statisticians and decision-makers, so that they have a clearer direction and more promising candidates to experiment with. This doesn’t mean that what we found the causal process which makes some clients more profitable than others, when we notice a difference between groups or clusters of clients.\nIf we do have to make a few, high-stakes, strategic decisions of major importance to business outcomes and user experience, that means we need some rigorous statistics. For example, how to price the products, whether to enter a new market, what products to develop, how to allocate advertisement spending across different platforms, whether to deploy a new recommender system. We will discuss at length what can go wrong in drawing conclusions from data alone (with analytics or ML), and how that can backfire spectacularly.\nIf we have to make lots of decisions at scale and high frequency, for example – doing demand forecasting and inventory optimization for 100k product SKUs, it cannot be done manually or with carefully designed experiments. In this case, an appropriate choice would be to learn from data and get predictions as reliable as possible. Keep in mind, that we will have to be very careful when defining what the model is optimizing for – it has to be aligned with business objectives.\nWhy ML, since we put so much emphasis onto scientific rigor and trying to infer the causal processes? Sometimes – you don’t have a theory. For example, in recommender systems it’s just too complicated, with so many heterogenous users and items, each with their specific preferences and idiosyncracies.\n\n\n\n\n\n\nWhy not use all at various stages of a project?\n\n\n\nIt is not a debate of which one is better: ML vs Stats vs Analytics. One has to cycle through these approaches, gain greater understanding, experience, and skill in order to use the appropriate tools in the right context.\nI recommend the following 4-part presentation 2 by Cassie Kozyrkov, so that you get a good idea of how AI fits into organization and decision-making process. I recommend following her and, basically reading everything she has written on medium.\n\n\n2 C.Kozyrkov - Making Friends with Machine Learning3 C.Kozyrkov - 12 Steps to Applied AI4 People and AI Research | Google - GuidebookPay close attention to the process of developing data-driven products 3 and what are the prerequisites for an AI project to be successful (or doomed from the very start). It is important not to skip the relevant steps, understand the roles of people involved: from decision-makers, to statisticians, and data engineers. A good blueprint 4 for thinking about how to define and plan an AI project is given by Google’s PAIR (People and AI Research group). We will discuss all of this in detail during our next lectures and case studies.\n\n\n\n\n\n\nAre we in the business of ML?\n\n\n\nThe next two questions are tremendously important and will prevent you from embarking on an AI project which is doomed from the start:\n\nIs there a value proposition for AI? In other words, is there an underlying pattern to be discovered?\nDo I have the (necessary and relevant) data?\n\nIf yes and yes, we MIGHT be in business! But we shall not forget about the pragmatic aspects: is it feasible to be done with a small team, without a huge investment? What is the simplest way we can solve it? Are we solving the right problem? Are we making the job of people in the firm easier and more efficient?\n\n\nMake no mistake, the data science field is fascinating and full of exciting applications, but as you well know from statistics, there are numerous pitfalls we can fall into. I think it is useful to demistify AI and get humble, down to earth about what it can and can’t do – its power, but also the limitations:\n\nJust take a look at how many AI tools have been built to catch covid, and none helped 5\nOne part of the problem is the mismatch between the real/business problem and objectives, versus what models optimize for. Vincent Warmerdam brilliantly explains it in “The profession of solving the wrong problem”6 and “How to constrain artificial stupidity” 7.\n\n5 W.Heaven - Hundreds of AI tools have been built to catch covid. None of them helped.6 V. Warmerdam - The profession of solving the wrong problem7 V. Warmerdam - How to Constrain Artificial Stupidity\n\n\n\n\n\nSplit your damn data! (on data snooping)\n\n\n\nSince we’re engaging in the business of data mining and analytics at one point or another of the project, we have to be extra careful. Intuitively, you understand that discovering hypotheses and testing them on the same set of data is a bad idea, because we’ll get an overly confident estimation of how good it is.\nHowever, sometimes, we don’t shy away from doing an exploratory data analysis, finding relevant and predictive features for our target variable by trying out a few models. Next day, we forget about this, having the conclusions crystalized in our mind, and apply a new, final model … on the same data. Often we get away with this, but it is as bad, meaning equivalent to the first case – we contaminated the data with our mining.\nSo, before we get into the nuances of model validation, selection, and experiment design, get into the habit of always splitting your data. Give that test dataset to a friend, locked under a key and don’t let her give you that data, until you have your final model to be deployed and used for decisions.\n\n\nWhy go through all of that pain to critique our own model with such a vigor? The answer is simple – if it passes this rigorous critique, it has greater chance of finding a real/causal pattern and generalize to examples outside our sample.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "4. Analytics, ML, Stats"
    ]
  },
  {
    "objectID": "01_fundamentals/roadmap.html",
    "href": "01_fundamentals/roadmap.html",
    "title": "Module I: Business School for Data Scientists",
    "section": "",
    "text": "The “business school” emphasizes again and again the idea of Decision-Making Under Uncertainty at Scale, across many industries and use-cases. We walk through three perspectives: Analytics, Machine Learning, and Statistics – then develop processes for each one. This module is heavy on methodology, since I want to bring back science into “data science”, but don’t worry, there will be enough case-studies.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "Schedule & Roadmap"
    ]
  },
  {
    "objectID": "01_fundamentals/roadmap.html#a-selection-of-realistic-datasets",
    "href": "01_fundamentals/roadmap.html#a-selection-of-realistic-datasets",
    "title": "Module I: Business School for Data Scientists",
    "section": "A selection of realistic datasets",
    "text": "A selection of realistic datasets\nDeciding what topic, domain, and problem to choose for your project is a daunting task in itself. During the course, I present new tools to formulate good questions and how to design your research study. Unfortunately, in most cases, we will be limited by what open data is available. We encounter the same problem of “too much content”: there is so much data to work with, but most of it isn’t any good for our purposes.\n\n\nYou can rely entirely on simulation, but the way you desing the underlying data-generating process has to be very well thought out, informed by the specificities of the problem and an expectation of what data we’ll encounter in practice.\nFinding good data about business challenges is incredibly hard, since few firms will be open to sharing it. Even when we get our hands on a good kaggle competition, the problem is that the data has been already framed, curated, and put together for us. This means that we don’t get the real experience of end-to-end problem solving that we would encounter in practice. Therefore, I collected a list of datasets which are realistic and diverse enough. You can consider them as an inspiration and starting point.\n\n\n\n\n\nDataset name\nDomain\nProblem / Area\nComments\n\n\n\n\n1\nYaz restaurant demand\nbusiness\nNewsvendor problem\nA real dataset used for benchmarking inventory optimization algorithms from a restaurant in Stuttgart. Source: ddop\n\n\n2\nMercari vintage clothes marketplace\nbusiness\nPrice suggestion\nAn excellent dataset for GLMs and machine learning, where the text data is important. Source: kaggle / Mercari\n\n\n3\nAvocado prices and volumes from Hass board\nagriculture\nMarket research\nThis is a good opportunity to understand the dynamics of a whole industry. Source: kaggle / hass\n\n\n4\nOlist e-commerce\nbusiness\nEDA, databases\nA very rare example of freely available data published as a relational database. Good for open-ended projects. Source: kaggle\n\n\n5\nCorporacion favorita\nbusiness\nDemand planning\nOne of the best datasets to practice demand forecasting for groceries. Source: kaggle\n\n\n6\nTevec retail sales\nbusiness\nInventory management\nGood for short-term demand forecasting for the purposes of inventory optimization. Source: kaggle\n\n\n7\nLending club loans\nfinance\nCredit risk\nA large dataset of peer-to peer, graded loans, with data about clients, loan, and interest. Source: kaggle\n\n\n8\nDataCo orders\nbusiness\nLogistics and fulfillment\nOne of very few datasets for you to get a better grasp over outbound and inbound logistics. Source: kaggle\n\n\n9\nCriteo Campaign\nbusiness\nMarketing\nThe biggest dataset of randomized control trial marketing campaign for Uplift modeling. Source: kaggle\n\n\n10\nEase my trip\nairlines\nPrice prediction\nThis dataset has a few gotchas related to how the flights were selected and unavailability of seats remaining. Source: kaggle\n\n\n11\nAmazon reviews for beauty products\nbusiness\nNLP, Customer analytics\nAnalyzing customer reviews and feedback is a widespread use-case in businesses. Lots of data is available. Source: nijianmo\n\n\n12\nExpresso churn prediction\ntelekom\nChurn\nWe will discuss the challenges around modeling customer churn, especially survival models and interventions. Source: kaggle\n\n\n13\nSantader customer satisfaction\nbanking\nChurn\nSince the data is anonymized and mostly numeric, we would have to take a ML approach. Source: kaggle\n\n\n14\nSupply chain allocation\nbusiness\nLogistics and shipments\nAnother rare dataset on logistics, in which you assign routes to purchase orders from manufacturers. Source: kaggle\n\n\n15\nHospital customer satisfaction\nhealthcare\nCustomer analytics\nA pretty large-scale, general-purpose survey on client satisfaction. A lot of EDA and data cleaning is needed. Source: kaggle\n\n\n16\nBike sharing demand\ntransportation\nDemand planning\nSeveral datasets from different cities and ride sharing firms: Washington, Boston, London\n\n\n17\nNYC subway rides\ntransportation\nDemand planning\nAnother aspect of demand planning is load prediction and minimizing delays: NYC traffic, Toronto subway delays, NYC entry and exits\n\n\n18\nTaxi trips\ntransportation\nDemand planning, pricing\nMultiple datasets from different firms and cities: Chicago, NYC taxis, NYC uber\n\n\n\n\n\nThe curated list of datasets doesn’t appear in most books and resources that I recommend. The challenges they present are not easy and require quite a lot of work. Some of them will also teach you how to work with larger amounts of data. That said, there is still a lot of value in the didactic examples, so here are a few more directions I recommend to look into:\n\nP. Fader, B. Hardie, and E. Ascarza research on BTYD (buy till you die) models of customer repurchase behavior, churn, and LTV\nThis website on Bayesian networks has a few amazing case-studies on self-worth and depression, which are perfect for practicing causal thinking\nFacebook has synthesised the recent research in Marketing Mix Modeling into their open-source project called Robyn.\nRohan Alexander has a good example of multilevel modeling with post-stratification on US 2020 elections. Andrew Gelman in “Regression and other stories” has lots of great examples from political and social science.",
    "crumbs": [
      "| 1. Business |",
      "1. Business School and Methodology",
      "Schedule & Roadmap"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html",
    "href": "01_fundamentals/prerequisites.html",
    "title": "Why did you study all of that",
    "section": "",
    "text": "Why did we have to go through all those excruciating months and years doing mathematical analysis, linear algebra, probability, statistics, econometrics, operations research, and lots of economics? What about computer science and low-level programming in C? What about more abstract subjects like cybernetics and game theory?\nIt was very frustrating for me, because it wasn’t clear how they fit together, what is the common thread, and more importantly – what part of the theory is relevant. It would’ve been great to know what works well in practice and why. I will try to be as clear as possible which ideas are helpful in solving the kind of problems in businesses we’re interested in, and which ones are designed to enhance our conceptual understanding.\nLet’s draw a map, stop at each field and in a sentence explain why we learned it and how it contributes to decision science, ML, and AI. I mentioned from the very beginning that decision science is an applied, quantitative, and interdisciplinary field, but it is not just an union of those subjects – the inspirations and tools are quite carefully picked.\nflowchart TD\n  LA[Linear Algebra] --&gt; OR[Operations Research]\n  MA[Mathematical Analysis] --&gt; OR\n  MA --&gt; SD[Systems Dynamics]\n  %% CS[CS Algorithms] --&gt; OR\n  \n  PT[Probability] --&gt; MS[Statistics] --&gt; EC[Econometrics]\n  EC --&gt; Caus[/Causal Inference\\]\n  EC --&gt; TS[Time Series]\n\n %% subgraph 1\n  Caus --- DM[/Data Mining\\] --- ML[/Machine Learning\\] \n  ML --- Caus\n %% end\n\n  OR --&gt; ML\n  MS --&gt; ML\n  MA --&gt; PT\n  SD --&gt; Caus\n\n  Caus --- Econ[[Economics]]  \n  Econ --- GT[Game Theory]\n  Econ --- DT[Decision Theory]\n\n  style Caus fill:#f7f5bc\n  style ML fill:#f7f5bc\n  style DM fill:#f7f5bc\n\n  DM --- FSDA[/Full-Stack Apps\\]\n  FSDA --- DB[Databases/SQL]\n  FSDA --- OOP[OOP]\n  Econ --- TS\n\n  style FSDA fill:#f7f5bc\n\n\n\n\nFigure 1: Think of this as a stuctural organization of the fields and courses you studied before. Some are more useful in analytics, some in ML and some in making causal inferences, that is, based on data + theory. This diagram is organized around those three central pillars and perspectives.",
    "crumbs": [
      "| 1. Business |",
      "iv. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#fundamentals",
    "href": "01_fundamentals/prerequisites.html#fundamentals",
    "title": "Why did you study all of that",
    "section": "Fundamentals",
    "text": "Fundamentals\nNote that all subjects below can be studied in various levels of depth and mathematical rigor. Where do you stop heavily depends on what you want to do in your carreer.\nLinear Algebra is a language of data. Most data types (structured, nested, images, video, text, voice, etc) can be represented as multidimensional arrays and tensors. The vast majority of machine learning and mathematical models can be reduced to operations on matrices, which is important if we want to implement them in code. Moreover matrix decomposition algorithms like SVD, Cholesky are useful tools in many practical applications. 1\n1 On a personal note, linear algebra was my gateway drug to applied mathematics and machine learning. Without that course it is possible I would’ve never discovered a passion for mathMy perspective over linear algebra is computational and geometric, in the sense of the “space” the data points live in and transformations of that space. It’s also a very concise and elegant way to define models, loss functions, and optimization problems.\nI highly recommend the following resources: Essence of Linear Algebra (conceptual), Coding the Matrix (computational), Linear Algebra Done Right (abstract, mathematical), Mathematics for ML (applied), and S. Boyd’s course (applied).\nMathematical Analysis is all about change, formalizing how a function behaves with respect to its arguments and parameters. It is an essential building block in probability theory,2 optimization, and deep learning (automated differentiation).\n2 Many mathematically rigorous degrees also require real analysis, measure theory, and even functional analysisI argue that in order to understand any complex system, be it a firm, an economy, the climate, or environnment, we have to model how it evolves in time. This suggests the importance of differential equations and systems dynamics, modeling the feedback loops. All of this would be very awkward to reason about without mathematical analysis.\nHowever, be warned, mathematical analysis is a rabbit-hole! The only courses which I can recommend without you going into years of proof-based study, is “Calculus, Applied!” from Harvard and John Hopkins’ specialization, which has some cool modeling applications with Python code!\nProbability theory is the language and logic of uncertainty. In statistics, we change our mind and actions in the face of evidence. Just to highlight how important these two complementary fields are, it is hard for me to find an applied domain or aspect of life where probability and statistics isn’t useful (at least, in principle). I dare you to find one!\nMoreover, our everyday intuitions make us terrible at probability, which is another reason why we need a more formal tool to reason and make decisions under uncertainty. Why then so many people hate it as a subject? Is it dry teaching, fear of math and abstraction? In the module two, I recommend a simulation and story-based approach to probability, with many resources which will make studying it fun and practically relevant. 3\n3 In my case, Taleb Nassim’s “Fooled by Randomness” was what motivated me to study probability seriously. I do highly recommend it",
    "crumbs": [
      "| 1. Business |",
      "iv. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#give-statistics-another-chance",
    "href": "01_fundamentals/prerequisites.html#give-statistics-another-chance",
    "title": "Why did you study all of that",
    "section": "Give statistics another chance",
    "text": "Give statistics another chance\nIt is possible that you didn’t enjoy your statistics classes, which happened in my case, despite a passion for the field. In this journey we have to give statistics another chance, but we dramatically change the strategy of how we learn it. If you’re impatient and want to run with it, here is a short and preliminary list of resources, which completes the references found in the course:\n\nRead this short article on the difference between probability and statistics. Watch this mini-lecture on statistical thinking by Cassie Kozyrkov.\nRealize that lots of common statistical tests are particular versions of linear models. It takes a few hours to go through the theory and code in the book, which will save you a whole semester of painful and tedious calculations.\nFlip through a textbook which has a modern approach to teaching statistics, by leveraging code, simulations, and data analysis. Speegle’s Probability, Statistics, and Data: A fresh approach using R and Cetinkaya-Runde’s Introduction to Modern Statistics are excellent examples of the “new school”.\nCheck out A. Gelman’s Regression and Other Stories and Active Statistics. In my opinion, this is the best book to learn regression from, but the style might not be everyone’s cup of tea.\n\nThere are a ton of books and courses on statistics which basically do/teach the same thing. I curated a few which stand out with the right balance of data, code, simulation, theory, and real-world applications:\n\nCrump - Answering questions with data is a well written, but traditional introductory statistics book for psychology students\nHolmes, Huber - Modern Statistics for Modern Biology focuses on multidimensional methods and discrete data\nPoldrack - Statistical Thinking for the 21st Century is perhaps the most complete from this list\nRohan - Telling Stories with data has excellent chapters on modeling workflow and writing research",
    "crumbs": [
      "| 1. Business |",
      "iv. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#applied-mathematics",
    "href": "01_fundamentals/prerequisites.html#applied-mathematics",
    "title": "Why did you study all of that",
    "section": "Applied Mathematics",
    "text": "Applied Mathematics\nHere is an explanation for why other subjects are needed – which students from backgrounds other than quantitative economics or computer science might’ve not encountered. This is a point in which you see reseachers specializing.\nEconometrics, in my personal opinion, tries to separate the signal from noise and make inferences about the causal processes in economic phenomena. It specializes statistics to the domain of economics, which presents new modeling challenges. Remember that in order to draw causal conclusions, we need a scientific theory, not just data alone. It’s not sufficient to check if statistical assumptions hold, but we have to make clear our causal and methodological assumptions.\nYou should be careful in introductory econometrics, as you can claim only association with the tools you will learn. Even in literature, there is this weird tension between an emphasis that most conclusions aren’t causal, but them treating them as such for policy-making. Leaving the causal inference aside for a moment, I can safely recommend “Mostly Harmless Econometrics” and “Econometrics with R”.\nTime Series Analysis (senso largo), bridges the gap between theoretical dynamical models (systems of differential equations) with statistics and probability (stochastic processes). It adapts those tools to make inferences and predictions about phenomena which evolve in time, that are dynamic in their nature. I like the metaphor of data assimilation, which is actually an entire field trying to introduce the empirical dimension to all kinds of dynamical systems.\nI don’t particularly like any books I read or courses I took on time series. However, I can recommend N. Vandeput’s “Demand Forecasting Best Practices” and his other writing. For practitioners, R. Hyndman’s “Forecasting: Principles and Practice (3ed)” and I. Svetunkov’s “Forecasting and Analytics with ADAM” are a good starting point.\nOperations Research is about optimization with constraints, allocation of limited resources, and efficiency. The easy part is to apply an integer or linear programming algorithm to an already formulated problem. The hard part is to reduce a messy real world problem at a large scale to that formulation, especially under uncertainty and nonlinearity.\nI’d argue that without implementing a problem in code with established OR libraries and inferring the parameters of the problem from data – we don’t really know how to do optimization. The only free resources on Operations Research with code which I know of are C. Kwon’s “Julia Programming for Operations Research (2ed)” and Huber’s “Algorithms for optimization”. Others are strictly focused on optimization for ML and deep learning.\n\n\n\n\n\n\nA word of encouragement\n\n\n\nNone of those courses were useless. Think of how can we take parts from each of those prerequisites which are relevant for decision science, so that we have more tools to solve the complicated problems we encounter.",
    "crumbs": [
      "| 1. Business |",
      "iv. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#a-note-on-economics",
    "href": "01_fundamentals/prerequisites.html#a-note-on-economics",
    "title": "Why did you study all of that",
    "section": "A note on Economics",
    "text": "A note on Economics\nIn mathematically-oriented courses, you can think of economics as optimization with constraints, which are given by limited resources and our positive or normative theories of decision-making. In the most general sense, economics studies behavior and interactions of economic agents.\nWhen we make decisions, we like to think of ourselfs as objective, but we have lots of biases and blind spots which prevent us to see the reality clearly. For example, we find patterns and regularities which are not real, but treat the conclusions as causal; we tend to seek data which supports our hypothesis and ignore contradictory evidence, etc.\n\nThis kind of knowledge about human behavior helps us to be wiser and practice active open-mindedness 4\nIn other words, we want to prevent self-deception and self-sabotage towards achieving the stated goals and objectives – and even evaluate if the goals chosen well\n\n4 Actively open-minded thinking (AOT) is measured by items that tap the willingness to consider alternative opinions, sensitivity to evidence contradictory to current beliefs, the willingness to postpone closure, and reflective thought.Moving on to more practical things, the question is not how much economics a decision scientist should know, but what kind of economics. This list will be wildly different for people going into quantitative finance, banking, insurance, or public policy.\nIf you studied in a program which provides a strong background in quantitative economics, econometrics, and operations research – I have some good news and bad news. The good news is that you know all the mathematics you will need in practice and you’re well versed in formulating good economic research questions. The bad news is that you probably need a lot of effort to improve your programming skills and that the kinds of economics you studied don’t translate well inside a firm.\nPlease, do not mistake corporate finance, marketing, management, operations, supply chain, logistics, CRM, and production as fluff. There is a good reason people do business degrees and get MBAs. If you look at a top marketing journal, you will be surprised at the complexity of models and innovations in econometrics and machine learning needed to tackle those challenges. Moreover, when you combine marketing with behavioral economics and psychology, it adds another layer of nuance and understanding of consumer behavior. We can’t learn this all from data alone.\nDepending on your role in the firm, you might need to read about demand planning, advertisement, pricing, manufacturing, etc, in order to speak the language of decision-makers and stakeholders. I’m leaving you with a list of youtube playlists and channels which I found helpful to keep up-to-date with the way modern businesses operate:\n\nFinance: “Money & Macro”, Patrick Boyle, “The Plain Bagel”, Ben Felix, A. Damodaran’s NYU MBA\nWhen I was in university, I was interested in the Global Financial Crisis and found Perry Mehrling’s course on Economics of Money and Banking to be invaluable in cutting through the finance/banking jargon.\nBusiness: “Modern MBA”, “How Money Works”, Slidebean, Logically Answered\nEconomics: “Rethinking Economics”, “Institute for New Economic Thinking”",
    "crumbs": [
      "| 1. Business |",
      "iv. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#what-is-the-purpose-of-a-business",
    "href": "01_fundamentals/slides/lecture1/index.html#what-is-the-purpose-of-a-business",
    "title": "Decision Science",
    "section": "What is the purpose of a business?",
    "text": "What is the purpose of a business?\n\n\n\n\n\n\nUnderstanding and operating modern businesses is complicated\n\n\nMost startups fail. 1 One has to make many good decisions over a long period of time to have a chance at success in a competitive environment.\n\n\n\n\nMaximize profits? Compete, grow, and gain market share?\nMaximize shareholder value? 2 3\nBring added value to customers by solving a problem?\nContribute to economy and society (tax, employment)?\nCreate new markets to allocate resources efficiently?\n\n\n\nDiscussion: score from 1 to 5 on how important each reason is for you 4"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#model-simplified-representation",
    "href": "01_fundamentals/slides/lecture1/index.html#model-simplified-representation",
    "title": "Decision Science",
    "section": "Model = simplified representation",
    "text": "Model = simplified representation\nWhy do we build models? Mental, mathematical, statistical? 5\n\nWe need to understand 6 a system to make decisions\nReality is overwhelming and combinatorially explosive 7\nWe want to capture the essential aspects of phenomena\nWhile minimizing our biases and foolishness 8\n\nAll models have assumptions, pressupositions, and limitations:\n\nModels are golemns of Prague – Richard McElreath\nAll models are wrong, but some are useful – George Box"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#what-will-we-study",
    "href": "01_fundamentals/slides/lecture1/index.html#what-will-we-study",
    "title": "Decision Science",
    "section": "What will we study?",
    "text": "What will we study?\n\n\n\n\n\n\nFundamentals of business economics and statistical modeling:\n\n\n\\[ Question \\longrightarrow Modeling \\longrightarrow Insight \\longrightarrow Action \\longrightarrow Outcome \\]\n\n\n\n\nUnderstand problem space to ask the right questions 9\nSo that we can build custom statistical models\nWhich bring insight into consequences of actions\nSo that we’re informed which actions that should be taken\nSuch that a firm can achieve its ST/LT objectives\n\n\n\nNotice the language of problem space and solution space, as you will encounter it"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#data-science-is-an-umbrella-term",
    "href": "01_fundamentals/slides/lecture1/index.html#data-science-is-an-umbrella-term",
    "title": "Decision Science",
    "section": "Data science is an umbrella term",
    "text": "Data science is an umbrella term\nWe have to understand clearly what we mean by AI, ML, and Analytics. Then, pick the right tool for each problem\n\nAI | Decision making under uncertainty at scale\nCybernetics | The science of general regularities of Information Processing and Control in CAS 10\nML | Expertise from Experience in an automated way 11\nAnalytics | Exploratory analyses to formulate hypotheses\nStatistics | Changing your mind given new evidence\n\n\n\nDiscussion: what is a problem (AI, CogSci) and why framing is key"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#what-are-some-typical-applications",
    "href": "01_fundamentals/slides/lecture1/index.html#what-are-some-typical-applications",
    "title": "Decision Science",
    "section": "What are some typical applications?",
    "text": "What are some typical applications?\n\nDemand planning, inventory optimization, production 12\nRevenue management, pricing, and promotion\nAdvertisement and conversion optimization\nCustomer behavior and preferences: 13\n\nChurn, repurchase, engagement, LTV, WTP\nFraud, credit default, insurance risk 14\nChoice modeling, recommender systems, uplift\n\nImproving products, assortment, merch, processes"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#weak-or-specialized-ai",
    "href": "01_fundamentals/slides/lecture1/index.html#weak-or-specialized-ai",
    "title": "Decision Science",
    "section": "Weak or specialized AI",
    "text": "Weak or specialized AI\n\nDecision-Making Under Uncertainty at Scale\n\n\ndomain-specific (medicine vs finance vs automotive …)\ndata-driven (key idea of learning from data)\nlimited degrees of autonomy and notions of agency\nsometimes concerned with networks of agents\n\n\n\nAGI (Artificial General Intelligence). Michael Jordan - Stop calling everything AI"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#unpacking-cybernetics",
    "href": "01_fundamentals/slides/lecture1/index.html#unpacking-cybernetics",
    "title": "Decision Science",
    "section": "Unpacking Cybernetics",
    "text": "Unpacking Cybernetics\n\nControl \\(\\implies\\) goal-directedness. Action to steer to a trajectory or autopoesis (perserve \\((S-f)_{org}\\))\nInformation Processing \\(\\implies\\) pattern recoginition, perception, modeling & inference\nGeneral regularities \\(\\implies\\) plausible of control and information processing across fields and CAS\nAnimal refers to applications in biology, machine – in engineering, and human – in our society and behavior.\n\n\n\nIn economic cybernetics, we’re concerned with economics, society and human behavior, rather than engineering, biology, or natural science applications."
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#analytics-vs-ml-vs-stats",
    "href": "01_fundamentals/slides/lecture1/index.html#analytics-vs-ml-vs-stats",
    "title": "Decision Science",
    "section": "Analytics vs ML vs Stats",
    "text": "Analytics vs ML vs Stats\n\n\n\nSource: xkcd; Instead of Stats, I would say we want Causal Inference\n\n\n\n\nAnalytics is for inspiration. Formulating a hypothesis is a science and art. Think of a good analyst as a detective. One can also model, but still do analytics"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#three-challenges-in-statistics",
    "href": "01_fundamentals/slides/lecture1/index.html#three-challenges-in-statistics",
    "title": "Decision Science",
    "section": "Three challenges in statistics",
    "text": "Three challenges in statistics\nFirst and foremost, you have to start with a research topic, question and design your study or experiment. It’s hard!\n\n\n\n\n\n\nYou might’ve heard of internal and external validity\n\n\nA. Gelman clarifies very well three different aspects of statistical inference. Always remember this when we discuss stats! We want to generalize in terms of:\n\\[Sample \\longrightarrow Population\\]\n\\[Treatment \\longrightarrow Control\\]\n\\[Measurement \\longrightarrow Construct\\]\nThe holy grail is to build statistical models based on the causal processes informed by theories / hypotheses. Then model how we measured,15 and collected data.16"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#isnt-causal-inference-just-stats",
    "href": "01_fundamentals/slides/lecture1/index.html#isnt-causal-inference-just-stats",
    "title": "Decision Science",
    "section": "Isn’t causal inference just stats?",
    "text": "Isn’t causal inference just stats?\n\n\n\n\n\n\nThe hardest problem, worthy of a Nobel prize in econometrics\n\n\nHow many times have you heard that correlation doesn’t imply causation? Yet, ALL the methods you studied so far are not sufficient to fully justify causality, even Granger!\n\n\n\n\nTheoretical estimand (quantity or effect of interest)\nScientific (causal) models - DAGs of influence\nUse (1) and (2) to build a Statistical Model\nSimulate from (2) to prove (3) \\(\\implies\\) (1)\nAnalyze and make inferences with real data\n\n\n\nCausal inference can also be seen as a problem of inferring/predicting missing data"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#why-do-we-need-these-tools",
    "href": "01_fundamentals/slides/lecture1/index.html#why-do-we-need-these-tools",
    "title": "Decision Science",
    "section": "Why do we need these tools?",
    "text": "Why do we need these tools?\n\nVUCA: Volatile | Uncertain | Complex | Ambiguous\n\n\nA problem well formulated is halfway solved\nLimited resources, conflicting objectives\n\ntherefore, have to make tradeoffs\n\nDecisions informed by robust predictions and evidence\nDo you have what it takes to navigate this environment?\n\nthe tools, understanding and skills\nattitude: learning and growth mindset"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#to-make-better-decisions",
    "href": "01_fundamentals/slides/lecture1/index.html#to-make-better-decisions",
    "title": "Decision Science",
    "section": "To make better decisions!",
    "text": "To make better decisions!\n\nThink of youself as a business person with superpowers\n\n\n\n\n\n\n\n\n\nEngineering in the trenches\n\n\n\nDevelp skills – a hard, but rewarding path from novice to expert 17\nPragmatic data-driven software\nModeling workflow and pipelines\nSimulations as a safe playground\nThis course is NOT a bootcamp\n\n\n\n\n\n\n\n\n\n\n\nContemplating in the library\n\n\n\nCultivating understanding and insight into fundamental theoretical ideas\nUnderstanding your domain & clients\nApply the right model for the job\nAwareness of pitfalls and mistakes\nRigorous, but NOT detail-oriented\n\n\n\n\n\n\n\nThis is the course I wish I had when getting started in the field."
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#why-did-you-learn-all-of-that",
    "href": "01_fundamentals/slides/lecture1/index.html#why-did-you-learn-all-of-that",
    "title": "Decision Science",
    "section": "Why did you learn all of that?",
    "text": "Why did you learn all of that?\n\nLinear Algebra - language and geometry of data\nMathematical Analysis - formalism of change\nProbability - logic of uncertainty\nStatistics - changing your mind and action under evidence\n\ninference = data + assumptions\n\nEconometrics - does \\(X \\longrightarrow Y\\)? Causes or associations?\nOperations Research - optimization with constraints\n\n\n\nDiscussion on fundamentals: why we end up not using these tools in practice?"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#whoami-as-engineer-and-leader",
    "href": "01_fundamentals/slides/lecture1/index.html#whoami-as-engineer-and-leader",
    "title": "Decision Science",
    "section": "~whoami: as engineer and leader",
    "text": "~whoami: as engineer and leader\n\nSkin in the game: VP of Decision Science @AdoreMe (VS)\nAI strategy, decision-making, ML systems design\nStatistics, ML/data engineering, Full stack data apps\nCritical applications along the value chain:\n\ndemand forecasting & inventory optimization systems\nrecommender systems, NLP/LLMs for try-at-home\nmarketing: acquisition, CRO experiment design, CRM\n\n\n\n\nDiscussion: Target’s disaster launch in Canada, resulting in billion-scale losses"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#whoami-as-teacher-and-researcher",
    "href": "01_fundamentals/slides/lecture1/index.html#whoami-as-teacher-and-researcher",
    "title": "Decision Science",
    "section": "~whoami: as teacher and researcher",
    "text": "~whoami: as teacher and researcher\n\nMaven (yid: meyvn) – experience to help you find your path\n\nhelp you develop skills and understanding\n\nGraduate of Cybernetics and Quantitative Economics\n\nThesis & Dissertation on Bayesian Microeconometrics\nResearch in Probabilistic Methods for Time Series\nStarted in systems’ dynamics and economic complexity\n\nSpeaking at conferences, meetups, and panels\n\n\n\nDiscussion: study with a book, paper, and code editor – responsibility for self-study"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#whoami-as-a-person",
    "href": "01_fundamentals/slides/lecture1/index.html#whoami-as-a-person",
    "title": "Decision Science",
    "section": "~whoami: as a person",
    "text": "~whoami: as a person\n\nCultivating wisdom – philosophy as a way of life\nReading: art history, cognitive science, evolution\nPainting in oils and watercolor\nHiking, hipster coffee, 14 years of pro-ish chess\nConcerts: opera, metal, jazz, indie, cinema\n\n\n\nDiscussion: leverage your hobbies in studies"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#course-roadmap",
    "href": "01_fundamentals/slides/lecture1/index.html#course-roadmap",
    "title": "Decision Science",
    "section": "Course roadmap",
    "text": "Course roadmap\n\nBusiness school from data science perspective\nProbability theory: review with lots of simulation\nMathematical statistics: the useful parts of theory\nExperiment Design and A/B testing\nBayesian statistics and hierarchical models\nR and Python for data science (project)\n\n\n\nThe depth we go into each of the topics will heavily depend on your interests and challenges you’ll encounter along the way"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#covered-in-year-two-course",
    "href": "01_fundamentals/slides/lecture1/index.html#covered-in-year-two-course",
    "title": "Decision Science",
    "section": "Covered in year two course",
    "text": "Covered in year two course\n\n\nApplied Machine Learning:\n\nClassification, regression, probabilities\nClustering and dimensionality reduction\n\nSpecial topics like:\n\nPropensity score matching, choice, and conjoint models\nNLP / recommender systems / ML for time series\n\nOperationalizing ML pipelines, data-driven applications\n\n\nBy building a solid foundation this year, we can focus on more advanced models"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#simplification-and-relations",
    "href": "01_fundamentals/slides/lecture1/index.html#simplification-and-relations",
    "title": "Decision Science",
    "section": "Simplification and Relations",
    "text": "Simplification and Relations\n\n\n\n\n\n\n\n\n\nWe see a messy reality, which is the data. We want to get to the essence, underlying structure\n\n\n\n\n \n\n\n\n\n\nThis is a big picture course: see how it all fits together and how to apply the tools in practice to be useful for firms"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#too-much-content.-where-to-start",
    "href": "01_fundamentals/slides/lecture1/index.html#too-much-content.-where-to-start",
    "title": "Decision Science",
    "section": "Too much content. Where to start?",
    "text": "Too much content. Where to start?\nMoreover, how do you keep up with latest developments?\n\nMaster the fundamentals: understanding and skill\nCurated selection of excellent resources\n\nStill, years’ worth of study\n\nRoadmap and conceptual frame to navigate by yourself\n\nChoose what is relevant for your problem\nCome back to a topic with greater understanding later\n\nNo shortcuts that work, patience needed"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#admin-and-grading",
    "href": "01_fundamentals/slides/lecture1/index.html#admin-and-grading",
    "title": "Decision Science",
    "section": "Admin and grading",
    "text": "Admin and grading\n\nLectures and labs are in-person only\nChanges in schedule coordinated with representative\nFinal exam on 10th Feb: 5p\nCoursework 5p, out of which:\n\nAttendance and participation 1p\nProject in phases* 4p. Final submission on 9 Feb\nSurvey completion: bonus 0.25p\n\nPassing grade: round(course+exam) at least 5p"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#project-report-or-paper",
    "href": "01_fundamentals/slides/lecture1/index.html#project-report-or-paper",
    "title": "Decision Science",
    "section": "Project report or paper",
    "text": "Project report or paper\nAt most 5 pages + appendix + code attachement\n\nWhat is your domain and problem / research question / hypothesis? What are your objectives?\nWhy is it interesting, unsolved, or important (for you)?\nWhat have other people tried or written before?\nWhat is your approach and idea to solve the problem?\nWhat results did your approach yield? Did it work?\nConclusions and future work\n\n\n\nIf the project you choose can help you with dissertation, even better"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#different-project-flavors",
    "href": "01_fundamentals/slides/lecture1/index.html#different-project-flavors",
    "title": "Decision Science",
    "section": "Different project flavors",
    "text": "Different project flavors\nFor engineers, if you like programming:\n\nInteractive, data-driven application to support decisions\nShould follow the same thinking outlined before\n\nFor analysts, statisticians, and data scientists:\n\nAnswer an interesting question with data and modeling\nIf you solve a problem - focus on decisions and outcomes\nIf you like exploring data - extract insights and hypotheses\nAutomate and render the report with Quarto (pdf, html)"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#not-allowed-in-the-project",
    "href": "01_fundamentals/slides/lecture1/index.html#not-allowed-in-the-project",
    "title": "Decision Science",
    "section": "Not allowed in the project",
    "text": "Not allowed in the project\nML approaches are allowed, however the below aren’t:\n\nPCA, CA, FA, clustering, single decision trees, kNN\nARIMA, GARCH, VAR and other time-series methods\nMacroeconomic models and methods\n\nAlso not allowed:\n\nReuse of projects from other courses\nDashboards built via a BI tool like Qlik\nPlagiarism and lack of citations / credits"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#project-phases",
    "href": "01_fundamentals/slides/lecture1/index.html#project-phases",
    "title": "Decision Science",
    "section": "Project phases",
    "text": "Project phases\n\nForm a team of 2-3 people, pick a preliminary subject\n\nregister the team and topic on the spreadsheet\n\nSubmit a 1-2 page document which answers the key questions from project structure\n\nPick the dataset(s) you will be working on\nNo need to provide the results / conclusions\nGet feedback and make the necessary corrections\n\nImplement your solution / analysis and render project\n\nSubmit the final project before Feb 9th, 2025"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#the-easiest-and-most-difficult-course",
    "href": "01_fundamentals/slides/lecture1/index.html#the-easiest-and-most-difficult-course",
    "title": "Decision Science",
    "section": "The easiest and most difficult course",
    "text": "The easiest and most difficult course\n\n“It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of light, it was the season of darkness, it was the spring of hope, it was the winter of despair.” ― A Tale of Two Cities, (Dickens 1859),\n\n\nNo memorization, proofs or solving on paper\nYou have to really understand, justify choices, write code\nNo single, unique solution, but many tradeoffs"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#a-long-and-rewarding-journey",
    "href": "01_fundamentals/slides/lecture1/index.html#a-long-and-rewarding-journey",
    "title": "Decision Science",
    "section": "A long and rewarding journey",
    "text": "A long and rewarding journey\n\n\n\n\n\n\n\n\nWhy should you stick with the course?\n\n\n\nEffective problem solving & decision-making\nBridge the gap between theory and practice\n\nbetween math world and real world\n\nReframe what you already studied to make it useful – e.g. hypothesis testing, regression\nLots of new methods for your toolbox\nClear understanding and precision in:\n\ndata science terms and concepts\ncommunication with clients, decision-makers, and stakeholders\n\n\n\n\n\n\n\n\n\nNavigate the complexities of the field and choose your path. (Source: Generated with DALL-E)"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#homework",
    "href": "01_fundamentals/slides/lecture1/index.html#homework",
    "title": "Decision Science",
    "section": "Homework",
    "text": "Homework\nReading from course materials and external resources:\n\nCourse introduction, philosophy, and study guide\nPrerequisites: why study all of that?\nApplication domains and use-cases\nVideo about Target’s disaster in Canada\nCritique of the idea of shareholder value\n\n\nFill in the survey about your interests and prerequisites"
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#references",
    "href": "01_fundamentals/slides/lecture1/index.html#references",
    "title": "Decision Science",
    "section": "References",
    "text": "References\n\n\nDickens, Charles. 1859. A Tale of Two Cities. New York: Chelsea House Publishers.\n\n\nHunt, A. 2008. Pragmatic Thinking and Learning: Refactor Your \"Wetware\". Pragmatic Bookshelf Series. Pragmatic Bookshelf. https://books.google.ro/books?id=Pq3_PAAACAAJ.\n\n\nJordan, Michael. 2016. “Stop Calling Everything AI, Machine-Learning Pioneer Says.” https://spectrum.ieee.org/stop-calling-everything-ai-machinelearning-pioneer-says.\n\n\nKozyrkov, Cassie. 2021. “Making Friends with Machine Learning.” Google. https://www.youtube.com/watch?v=1vkb7BCMQd0.\n\n\nMontier, James. 2014. “The World’s Dumbest Idea.” https://www.gmo.com/globalassets/articles/white-paper/2014/jm_the-worlds-dumbest-idea_12-14.pdf.\n\n\nStout, Lynn. 2016. “The Myth of Maximizing Shareholder Value.” https://evonomics.com/maximizing-shareholder-value-dumbest-idea/."
  },
  {
    "objectID": "01_fundamentals/slides/lecture1/index.html#footnotes",
    "href": "01_fundamentals/slides/lecture1/index.html#footnotes",
    "title": "Decision Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHow many do you think survive in the first 5 years in the US?\nMontier (2014)\nStout (2016)\nThen ask yourself, are you open to founding a startup in the next 5 years?\nFor example, the coin flip is a wonderful abstraction, especially when the flips or events are independent\nIn a causal sense, that is if we intervene, this is what is going to happen. Or in the predictive sense\nImagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etc\nThis is not to minimize what intuition resulting from deep knowledge and experience can bring to the table\nThis is what you basically study in Business Analytics classes: ways to think about diagnosis, strategy, business processes, stakeholders, etc\nJordan (2016)\nKozyrkov (2021)\nDeep dive in Logistics class. My strong recommendation is to apply it in practice and read N. Vandeput\nYou will deep dive into customer psychology and decisions under uncertainty in the behavioral economics class\nThese models are the way you operationalize a risk management strategy at scale (“minimize bleeding”)\nWe’ll dedicate one lecture on it, but you have whole specialized classes on business metrics\nAgain, we treat it in the most general sense, but you have BI classes which show you how data is collected and structured in most businesses\nHunt (2008)"
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html",
    "href": "01_fundamentals/prob_study_guide.html",
    "title": "Fundamentals of probability and statistics",
    "section": "",
    "text": "In the introduction and first module (business school for data scientists), I tried to convince you that data science and AI in businesses is about quantitative questions, decision-making, and improving outcomes. By walking through many use-cases in different domains, we saw how widespread are its applications and how valuable a data scientist could be to a firm.\nPrevious readings should give you enough motivation, context, and background to decide if you want to follow the proposed path. If the answer is yes, the question becomes how to actually do it1 and where to start. There are two key insights to remember:\nIn the introduction, I highlighted two types of prerequisites, which roughly translate to hard skills and soft skills. The hard skills are linear algebra, calculus, probability theory, inferential statistics, and programming for data science. The soft skills have a big overlap with research methods, scientific thinking, and business understanding.\nBefore explaining how to study and what are the most important concepts and skills to develop in probability and statistics, I want to tell you two important lessons from Patrick Winston’s talk at MIT, “How to speak”:\nMy point is that you need both: skills and understanding, soft skills and hard skills. Many mistakes in statistical modeling come from a misunderstanding of the nature of statistical inference. Other mistakes are not statistical, but a poorly formulated problem, which leads to using the wrong tool, or the right tool in an incorrect way. There is this always-present gap between the real world and mathematical abstraction.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#prerequisites",
    "href": "01_fundamentals/prob_study_guide.html#prerequisites",
    "title": "Fundamentals of probability and statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe way you study probability and statistics will depend on your background. In terms of prerequisites, you need an exposure to linear algebra, calculus or mathematical analysis, and inferential statistics (hypothesis testing and linear regression).\nIn this module, I make little use of linear algebra or calculus, as it is an applied, not proof-based or theoretical course. However, if you never had those courses, it will be very difficult to understand the meaning behind mathematical notation and abstractions. If you never had a statistics class on hypothesis testing and linear regression, you won’t know the pain. Jokes aside, it means you will need to use another resource provided in section iv, in parallel with this module.\nYou might be at a point in your studies, usually second year of bachelor’s degree, when you don’t know how to code in Python or R yet, and have a probability & mathematical statistics class. Maybe you have another one on applied regression and econometrics as well. This is a perfect moment to study the module fully,2 in order to:\n2 for example, in a simulation or scientific computing class\nLearn programming for data analysis in R or Python, including data wrangling, visualization, and reproducible research\nApply theoretical concepts and models to real-world stories and case-studies. This will highlight why is that theory relevant, useful, and why was it invented in the first place\nUnderstand how and why the theory works by doing lots of simulations. Find out under what conditions it fails\n\nIf you already had the classes mentioned above, you might be in third, 4th year, masters’ degree, or finished your formal studies. It’s likely you know how to solve problems and get stuff done in R or Python. Then, I recommend you go through this module in an accelerated way,3 both in terms of theory and coding. You might ask, why not skip it entirely and focus on statistical modeling?\n3 for example, using some parts of the module as a review in the context of a data science or econometrics’ class\nFirst, it’s a good review and warm-up with fun and realistic case-studies. We all get rusty when not practicing for a while\nA much more important reason is the re-contextualization of the fundamentals and getting rid of misconceptions, bad habits, and questionable statistical practices\nI think that statistics is so tricky, that it’s almost guaranteed we misunderstood something the first time we studied it",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#roadmap-and-the-big-picture",
    "href": "01_fundamentals/prob_study_guide.html#roadmap-and-the-big-picture",
    "title": "Fundamentals of probability and statistics",
    "section": "Roadmap and the big picture",
    "text": "Roadmap and the big picture\nThe module is build around stories and case-studies highlighting key concepts from probability theory, mathematical statistics, and applied regression modeling. We start from the very beginning, with counting and combinatorics, ending up with more sophisticated statistical models.\nThere are a few purely theoretical lectures, but they are strictly necessary in order to combat common misconceptions and establish a correct, consistent terminology. The mind map below organizes the lectures into three main tracks: probability theory and simple Bayesian models, mathematical statistics and experiment design, regression modeling and estimation, sampling, optimization algorithms.\nNote that some case studies belong to and interact with other modules.4 For example, in newsvendor problem and Wikipedia A/B test, we have to approach the problem from the perspective of businesses making decisions and carefully consider a few methodological aspects. There are a few simple Bayesian models, for inferring proportions and rates; elements of causal thinking and experiment design; and a gentle introduction to generalized linear models.\n4 This is a good thing, as our field is interdisciplinary. Moreover, if we strip the statistics out of context, we end up with a dry and abstract exposition\n\n\n\n\n\n\nflowchart TD\n\n    U[Introduction] -- Prob & Bayes --&gt; Comb[Birthday problem. Combinatorics] --&gt; PTr[Safari reservations. Binomial, Prob. trees. LLN] --&gt; RVAR[Probability triple. Random variables]  --&gt; NEWS[/Newsvendor problem. Distribution stories/] --&gt; B[Medical testing. Bayes and Conditioning] --&gt; BB[/Left-handed proportion. Beta-Binomial/] --&gt; GP[/Waiting times. Asthma. Gamma-Poisson /]\n\n    U -- Math. stats --&gt; E[Splitting U.S. Schools? Central limit theorem] --&gt; SM[Population, sample. Estimands, estimators. ] --&gt; PV[ P-values of many studies. Error control]  --&gt; CI[Wikipedia A/B test. Confidence intervals] --&gt; PC[How much data? Statistical Power. Experiment design]  --&gt; RC[/Dead salmon experiment. Replication Crisis/] \n\n    U -- Modeling. Algorithms --&gt; EC[/Divorce rates. Three confounders/] --&gt; BV[Bootstrap conf intervals. Out of sample error] --&gt; MCMC[/Visiting islands. Markov Chain Monte Carlo/] --&gt; UCLA[/Berkeley Admissions. Logistic regression/] --&gt; OPT[/Gradient Descent. Convex optimization in GLMs/] \n\n    style MCMC fill:#f7f5bc\n    style OPT fill:#f7f5bc\n    style BV fill:#f7f5bc\n\n    style RVAR fill:#f5cdc4 \n    style SM fill:#f5cdc4 \n\n\n\n\nFigure 1: Lectures in pink are purely theoretical. Lectures in yellow present the most important algorithms for estimating our workhorse models. The vast majority involve coding and data analysis. The rhomboid indicates that the case-study belongs to another module.\n\n\n\n\n\n\nThese topics and case-studies are designed for a whole semester of intensive study. My promise is that by the end of it, you will become a more confident programmer and will have a very solid foundation for advanced statistical modeling, causal inference, and machine learning.\nWhen teaching to masters’ students, in the context of a data science class, I still feel it’s necessary to walk through many of these aspects. What is sacrificed though, is depth and a limited amount of time available for students to practice on these case-studies. In my opinion, this is evidence that everyone can benefit from this module: undergraduate and graduate students, people already working in the field as data scientists and analysts. Even machine learning engineers.\nThere is one more catch. For each of these topics and case-studies, you might feel the need to dive deeper into the underlying mathematics, philosophy, and explore more applications. In other words, many rabbit holes. Your feeling is correct, there is so much to learn and explore, but this should not intimidate you. Instead, let it serve as motivation, dive into a topic that resonates with you and see where does it lead.\n\n\nTime, attention, and energy are the only constraints you should keep in mind while diving much deeper into your chosen topic.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#learning-how-to-learn",
    "href": "01_fundamentals/prob_study_guide.html#learning-how-to-learn",
    "title": "Fundamentals of probability and statistics",
    "section": "Learning how to learn",
    "text": "Learning how to learn\nConsider the first lecture as a warm-up, where you have a chance to set-up your development environment and get into the flow of studying. I start with combinatorics, because there are several important concepts you might’ve missed or forgot, which have practical implications and pop up throughout the course.\nAnother goal of this lecture is to gain an appreciation and overcome the fear of mathematical abstraction and notation. We will see how these mathematical constructs and models have an underlying story and a reason for existence. By understanding the story and with a bit of practice, you will never, ever have to resort to rote memorization.\nAt last, I want to highlight that even these simplest tools, known for centuries, have real-world applications and were key to the development of the world as we know it today. 5 They look simple only in retrospective and we take them for granted. By connecting the appropriate abstraction (i.e. mathematical tool) to the phenomenon we want to explain and understand, we can make our scientific hypotheses precise and powerful. Urn models applied to physics are a great example of that. See the references in the “rabbit hole” section.\n5 Think of geometry and cathedrals, apeducts, calculus, physics, engineering, industrial revolution, computers, and information technologyI think we have a problem in the teaching of mathematics to high-schoolers and undergraduates, where by applications is meant a numerical or toy example. Those are not applications! In the best case, they’re stories and drills. Don’t get me wrong, drills are important and we can’t always do only what we like, but at worst, they’re tedious, useless stuff assigned as homework which kills any passion.\nIt might sound obvious, but we practice in a very specific way – with your reference book(s), code editor, and pen / paper at all times. Learn actively, by doing and problem solving, by testing yourself, by trying to come up with your own examples. We cannot afford to rush when studying mathematics and computer science.\n\nRead, pause, think, pause, write, pause, (perhaps erase), pause, read, pause, (perhaps go back), pause, write, … - A. Turing, 1936",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#combinatorics-and-sampling",
    "href": "01_fundamentals/prob_study_guide.html#combinatorics-and-sampling",
    "title": "Fundamentals of probability and statistics",
    "section": "Combinatorics and sampling",
    "text": "Combinatorics and sampling\nBefore jumping into the birthday problem, we need to know the stories behind core concepts in combinatorics, like \\({n \\choose k}\\) “n choose k”, factorial, falling factorial, and multiplication rule. We will then have to figure out how to apply these tools to ordered / unordered sampling with / without replacement. By learning the stories, you will never forget or misuse the mathematical relations:\n\nMultiplication rule and the “garden of forking paths”. Ordered sampling with replacement: urns and balls, unique sequences\nWays to order n people at a long bar table. Factorials\nSurveying only once, dealing cards from a deck, flags and poles. Sampling without replacement and falling factorial\nSplitting n people into two teams / subgroups of size k and r. Unordered sampling without replacement\nStory proofs of useful relations in combinatorics.\nNaive probability limitations and motivation for Kolmogorov’s breakthrough of putting probability theory on a solid foundation\n\nIf you are studying probability theory and mathematical statistics right now, or feel the need to go through the most important topics again, I highly recommend these lectures by Santosh A. Venkatesh and Joe Blitzstein’s Probability 110. You might rightfully object that no matter how amazing those courses and presentations are, it takes a lot of effort to work through them start-to-finish.\nI encourage you to cherry-pick lectures where there are synergies with this course and could deepen your understanding. In my experience, the majority of people don’t really get probability theory and mathematical statistics from the first time. It’s partly because of an outdated way of teaching, partly because this stuff is really damn deep and complicated if you do it rigorously.\nThere is absolutely no shame in going back to it again with a mindset of mastering the fundamentals. For me, this exercise turned out to be one of the most valuable things I studied. Also, understand that the resources I refer you to are a result of decades of teaching and research experience of those authors in top universities. For the purposes of learning probability, it’s almost impossible to explain these topics better. Our only secret weapons are simulations and knowledge of what parts are most important and useful in practice.\n\nWatch S. Venkatesh’s introductory lecture on probability, called “Prelude to a theory of chance” (Tableau 1).\nWhat is the difference between probability and statistics? Read this short article. Does your answer align?\nWatch J. Blitzstein’s first three lectures. Feel free to fast-forward and skip the things you already know well.\nWatch S. Venkatesh’s lecture on combinatorics, Tableau 2.\nFollow it up with the urn models in Tableau 3.\n\n\n\n\n\n\n\nBirthday Paradox. Pigenhole Principle. Collision Problems\n\n\n\nOne of the reasons we’re not intuitively good at probability, is that in many cases it is difficult to count the way favorable/possible outcomes could occur.\nThe birthday problem is one of such examples. We’ll do simulations and derive the analytical solution – suggesting unexpected applications in computer science and engineering.\n\n\n\n\nThis is completely optional, but I highly recommend the following provocative talk by Richard McElreath, “Bayesian inference is just counting” (slides and talk). The understanding you’ll gain from it will make learning Bayesian statistics much easier.\n\n\n\n\n\n\nRabbit hole: unordered sampling with replacement\n\n\n\nIn Tableau 3, walk through the proof of unordered sampling with replacement, only if you find it interesting. The result will be useful only when we’ll introduce bootstrap (explanation here)\n\n\nI understand if your reaction at this point is: “Are you kidding me, this is just a warm-up?”. I would like to convince you that the time investment is not that big and the payoff is huge. In a formal classroom setting, we would spend 1h 20’ in a lab, with roughly 2-4 hours of self-study and optional readings necessary for you to get a really good grasp on this. If you study by yourself, there are three lectures by J. Blitzstein (45’ each), 3 tableaus by S. Venkatesh (1h each). There will be a huge overlap in the content and not all of it requires hands-on study.\nSo, what do you get in return of this time investment? Fun stories and applications in multiple domains, confidence and not being scared of mathematical abstraction, courage to explore more. You might realize that probability and statistics don’t have to be a chore, but a worthwhile and fascinating profession to pursue. Motivation makes all the difference, so we have good reasons to be disciplined and work hard.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#binomial-distribution.-probability-trees.-simulation-and-lln",
    "href": "01_fundamentals/prob_study_guide.html#binomial-distribution.-probability-trees.-simulation-and-lln",
    "title": "Fundamentals of probability and statistics",
    "section": "Binomial distribution. Probability trees. Simulation and LLN",
    "text": "Binomial distribution. Probability trees. Simulation and LLN\nThe coin flip, which we’ll call Bernoulli distribution, is one of the most useful mathematical abstractions, as binary outcomes are ubiquitous – the event either happens or not, with a probability \\(\\theta\\). Think about a few examples from your day-to-day life and from the perspective of a firm.\nIf we have multiple independent events, each one being a coin flip, we can model the number of successes \\(k\\) out of \\(n\\) trials with the Binomial distribution. In order to apply what we learned about combinatorics, a good exercise is to derive its distribution (probability mass function) via a story proof. I want to remind you that probability is generative: if we know \\(\\theta\\) and \\(n\\) we can simulate data / outcomes, and make predictions.\n\n\nIn statistics, we estimate the parameters of (population) models from data (our sample). You will see the Binomial distribution a lot in hypothesis testing for randomized experiments and surveys. It will help you understand logistic regression.\n\n\n\n\n\n\nCouples showing up to a safari\n\n\n\nIn our case-study, we will try to find out what is the distribution of the number of people who sign up to safari and actually show up. We have only three seats available, \\(n = 3\\). If people decide to show up independently, the problem is straightforward, as you can see in A. Fleischhacker’s “Persuasive Python”, Chapter 2. Instead, I introduce a subtle change which will make the problem quite tricky.\nIn our version of the problem, a part of our potential customers are friends and couples making reservations for two. Of course, the events are now no longer independent: if one changes their mind and doesn’t show up, neither will the other (or so we assume).\nThere is one more gotcha. Even if we know the “prevalence” of couples in the target population, we still have to calculate the proportion of seats reserved by each type of customers.\n\n\nIn order to overcome these challenges, we will formalize the problem with the appropriate notation, make our assumptions explicit, use probability trees, Bernoulli and Binomial distributions. We’ll end up with a nasty formula which can be interpreted as a mixture model. Then, given a range of prevalences and individual probabilities of showing up, we simulate the process and interpret the results, comparing them to the case in which all passengers were independent.\nIn terms of programming, this case study teaches you how to create Quarto or jupyter notebooks in your IDE, how to import packages, work with data frames, functions, how to use vectorized calculations instead of for loops, manipulate data, make persuasive visualizations, and generate reproducible reports of your work. 6\n6 This is a lot for a beginner, as one has to understand what each piece of code does and how they connect together. The only solution is to practice, try to solve it yourself, read the errors and documentation, look at similar examples. Be resourceful!We picked and assumed certain values for prevalence of couples parameter. What if we’re terribly wrong? In order to assess that, a sensitivity analysis is needed. It will tell us at what proportion of couples in the population we start seeing dramatically different outcomes in terms of expected number of people who show up. There are two ways to compute it, depending on how the output of the simulation looks like:\n\nIf the 10k simulation runs return only the PMF (distribution), we can leverage the formulas for calculating expected value and variance of an arbitrary discrete random variable.\n\nIn this case, in order to represent the uncertainty in the mean estimation due to simulation (sampling), we could compute the standard error and display confidence intervals as error bars. This sounds a bit shady to me and out of place for this lecture\n\nIf the simulation stores all 10k runs, we can just take an average and the standard deviation of the data. For a grid of 50 possible values of the parameter, we work with 500k samples, which is not a problem for your laptop.\n\nHere, we have a more natural solution for summarizing the distribution: with a mean and a highest density interval\n\n\nAt this point, a fair question would be “what if I got really really unlucky with the sample I got?”. In our case, unlucky with the sequence of 10k random numbers we generated, in the sense of getting a result far from true mean. It is very unlikely, but not impossible. 7 There are two obvious ideas: increase the number of simulations and repeat those simulations under different seeds. It will work very well, but why?\n7 When we move to the topics in mathematical statistics, we’ll formalize this idea. But generally, it’s a good practice to ask how surprising is the data under a null hypothesis.\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nLaw of large numbers is the reason why simulation works, and also the reason we need to be very careful when interpreting the means of small sample sizes. To illustrate the theorem, we will simulate iid (independent and identically distributed) samples from the Bernoulli and Poisson distribution of an increasing sample size.\nThe graphs will show the formal idea of convergence in probability, and a rate at which we expect the average root mean squared error to drop. In other words, as n increases, sample mean will get closer to the population mean in the long run. Therefore, in simulation, we might find out that we don’t have to worry about being too unlucky after a certain number of runs.\n\n\nI want to emphasize once again, that most of traditional statistics rely on either exact results or asymptotic theory. We should be vigilent and recognize when neither holds in our applications. But this is a topic for another day.\n\n\n\n\n\n\nUniversality of the Uniform\n\n\n\nThis little, but important theorem is the reason we can generate random samples from any distribution, if we have speudo-random numbers which are uniformly distributed.\n\n\n\n\n\n\n\n\nRabbit hole: the hot hand strikes again\n\n\n\nYou might’ve heard about the hot hand fallacy. We model streaks of successful shots in basketball and ask whether streaks of \\(k\\) are surprising. Read the original paper by Kahneman and Tversky and ignore the statistical test they designed.\nTheir insight that fans and experts in basketball massively overestimate the magnitude of this effect is fundamentally correct. However, what doesn’t follow from their research is that there is no effect, which is pointed out by a few Bayesian statisticians in the recent years. The debate goes on, but the limitations of the original statistical test are undeniable\nThis is a true rabbit hole. Right now, we have the tools to simulte what can we expect if there was no effect or a small effect, but we can’t design a statistical test yet. For that, refer to the last section in Tableau 10 of S. Venkatesh, Kahneman’s original paper, and recent papers which challenge the statistical methodology and modeling approach based on which the original conclusion was reached.\n\n\nDespite the ongoing debate about the magnitude of the effect in the hot hand phenomenon, it is undeniable that such simulations are extremely valuable – they prevent us from be fooled by randomness. Let’s look at a few more examples:\n\namateur traders might see “winning” patterns in stock markets which can be explained by a random walk\npolicy-makers might not realize that amazing SAT performances in some schools with few students could be just due to small sample sizes\nin chess, some might attribute an observed pattern of ELO by gender to nonsensical explanations, when a simple simulation will show that an initial disparity in the number of players, combined with estimating a proportion in top k overall players results in similar patters.\npollers might underestimate the support for a political party due to differential non-response\nmedical research might overestimate the effectiveness of a treatment due to drop-out and censoring\nwe might think there is an association between variables, when both can be explained by a common cause, a confounder\n\nThis is one of the key reasons to study probability, statistics, and do lots of simulations. We train our brain to spot biases in our reasoning and decision-making. Even though we will still make foolish mistakes, at least we have a method to slow down and check a statement systematically. In the words of Richard McElreath:\n\nAnd with no false modesty my intuition is no better. But I have learned to solve these problems by cold, hard, ruthless application of conditional probability. There’s no need to be clever when you can be ruthless.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#probability-triple.-random-variables.-statistical-models",
    "href": "01_fundamentals/prob_study_guide.html#probability-triple.-random-variables.-statistical-models",
    "title": "Fundamentals of probability and statistics",
    "section": "Probability Triple. Random Variables. Statistical Models",
    "text": "Probability Triple. Random Variables. Statistical Models\nIf you understand and can explain the above ideas in a simple, yet rigorous way – you’re ready for the journey. Otherwise, if it feels shaky8, here are some readings:\n8 Some of you might find reviewing this insulting because it’s “trivial”, or useless theory, or a frustrating reminder of probability classes. Please, bear with me – because we will eliminate a whole class of errors practitioners make by not keeping these things in mind.\nProbability Triple and Random Variables - a quasi formal introduction is written in this chapter of the course website. From my experience, not many students have this understanding after their probability theory classes.\nCollectivity (“physical” structure), Statistical Population, Sample. We need to be a bit more precise in what we mean by a statistical model and a DGP.\n\nDefining the population and sampling process is a critical step in statistics.\nThe population is the “contract” we’re bound to when talking about inferences\nIt is a much more nuanced topic than it looks, explained well here and here.\n\nParameter (Estimand), Estimator, Estimation/Statistic. Never confuse those!\n\n\n\n\n\n\n\nHypotheses, Process models, Statistical models\n\n\n\nSome of the biggest debates in science, spanning across decades and causing much confusion and controversy could’ve been resolved much quicker by having this explicit distinction about Scientific Hypotheses, Process Models, and Statistical Models or procedures.\nI highly recommend this first lecture by Richard McElreath, showing how tricky could it be to map the correspondences between these three.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#newsvendor-problem.-stories-behind-distributions",
    "href": "01_fundamentals/prob_study_guide.html#newsvendor-problem.-stories-behind-distributions",
    "title": "Fundamentals of probability and statistics",
    "section": "Newsvendor problem. Stories behind distributions",
    "text": "Newsvendor problem. Stories behind distributions\nYou studied the properties of a whole zoo of probability distributions, but then, in statistics, encountered just a few – especially in the context of hypothesis testing. In Module 3 (Applied Bayesian Statistics) we will need to know the stories9 behind most of them, since the goal will be to build custom models for each application.\n9 There are particular physical processes and phenomena (stories, in general) which underly the patterns we observe. Often, those patterns can be accurately described by a particular probability distribution, governed by its parameters\nBernoulli, Binomial, Hypergeometric, Negative Binomial 10\nPoisson. Limiting and Conditioning. Overdispersion\nBeta, Gamma, Exponential. Exponential Family and Information Theory\nStatistical superstars: \\(\\chi^2_k\\), \\(t_k\\), \\(N(\\mu, \\sigma)\\), \\(F(d_1, d_2)\\)\nWeirdos: Mixtures, Dirichlet, Multinomial, Weibull, heavy tails\nRemember the differences between PMF, PDF, CDF, MGF, \\(\\mathbb{E}\\), \\(\\mathbb{V}\\), \\(\\mathbb{E}g(x)\\)\n\n10 Look at some examples with simulations and stories / applications with more math from Joe Blitzstein.\n\n\n\n\n\nPoisson and the Prussian army\n\n\n\nOriginally, Poisson distribution was used to estimate deaths by horses in the Prussian Army. Here is the historical data and a blog post telling the story.\n\n\nI limit the number of hands-on applications for this chapter/lecture, not only because of time constraints, but also because most use-cases come in Module 3, in the context of more realistic problems. I hope, however, that I sparked an interest about how to approach Probability, especially when we draw DAGs to tell stories.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#the-most-dangerous-equation.-clt",
    "href": "01_fundamentals/prob_study_guide.html#the-most-dangerous-equation.-clt",
    "title": "Fundamentals of probability and statistics",
    "section": "The most dangerous equation. CLT",
    "text": "The most dangerous equation. CLT\nHalfway in the module, we switch from Probability Theory to Mathematical Statistics. The goal is to develop the fundamentals needed for applied statistics, designing randomized experiments, and even machine learning. 11\n11 Although the perspective I take in Module 3 is Bayesian, I will take time in Module 2 to cover and re-contextualize the Neyman-Pearson frequentism\nWe continue with the key idea of estimators and sampling distributions, review laws of large numbers and the central limit theorem. See simulations here.\nIf you’re interested in the underlying theory, I go on a technical detour about convergence types: in probability, in distribution, and almost-sure\nWhat does a statistician want? Review important properties of estimators.\n\nFor an accessible explanation of Bias, Consistency, Efficiency – showcased with the corresponding R code, see openforecast\n\n\n\n\n\n\n\n\nThe most dangerous equation\n\n\n\nI think that “The most dangerous equation” is a must read for anyone, not just practicing scientists and statisticians. The example I usually do a demonstration on is about the dubious U.S. policy of splitting the bigger schools.\n\n\n\n\n\n\n\n\nCalling Bullshit: Best Barbecue\n\n\n\nContinuing on the reddit examples, there are some amazing case-studies in the “Calling Bullshit” website and book. One of them is exactly such a ranking problem: best barbecue in the states. I recommend you watch the whole playlist and work through the case studies: it is fun and an essential skill – to call out the bullshit.\nOnline platforms which have to rank posts and comments, face the challenges of how to take the sample size into account. It depends, but for inspiration, see the hackernoon ranking algorithm.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#p-values-and-error-control",
    "href": "01_fundamentals/prob_study_guide.html#p-values-and-error-control",
    "title": "Fundamentals of probability and statistics",
    "section": "P-Values and error control",
    "text": "P-Values and error control",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#confidence-intervals.-when-randomization-fails",
    "href": "01_fundamentals/prob_study_guide.html#confidence-intervals.-when-randomization-fails",
    "title": "Fundamentals of probability and statistics",
    "section": "Confidence intervals. When randomization fails",
    "text": "Confidence intervals. When randomization fails",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#conditioning-and-bayes-rule",
    "href": "01_fundamentals/prob_study_guide.html#conditioning-and-bayes-rule",
    "title": "Fundamentals of probability and statistics",
    "section": "Conditioning and Bayes Rule",
    "text": "Conditioning and Bayes Rule\nThere is a quote I like a lot: “Conditioning is the soul of statistics”. The Bayes rule, which follows directly from the axioms of probability, is an essential in decision-making and the most important tool in this course – both conceptually and technically. Any introduction to the subject will work out:\n\nA few excellent resources are Chapter 1/2 of BDA3, or Chapter 1/2 of Bayes Rules, or Chapter 1/2 of Statistical Rethinking. They will teach you about:\n\nConditioning, Marginalization, Priors, and Updating\n\nIf you prefer videos, enjoy the 3Blue1Brown visual masterpiece on how to think like a Bayesian or the explanation here.\nI introduce the idea of Likelihood, which would serve us in future use-cases. It is another important perspective over statistical modeling to consider\n\n\n\n\n\n\n\nMedical testing for rare diseases\n\n\n\nMedical testing for rare diseases, hypothetical example with code in my course repository. We use the same idea to reason about how confident are we our code has no bugs.\nIf you remember the Covid-19 rapid tests and their confusion matrices printed on instructions, you could’ve applied the same idea!\n\n\n\n\nOr maybe you’re passionate about biology, where you could apply it for Mendelian genetics and think about the mystery of deadly genes persistence\nFor the simplest models, one approach of comparing different hypotheses is Bayes Factors. However, these do not translate well in practice for more sophisticated, multilevel models. You can look it up in the following courses here and here for the theory and examples.\n\n\n\n\n\n\nFootball spreads and betting experts\n\n\n\n(BDA3, Ch1): Football spreads, that can be estimated from data about matches. What is the probability that a team wins? Are experts right, on average?\n\nIf you’re into betting and sports, can you replicate the analysis on other datasets? What are your options for data collection?\nFor brevity, I won’t elaborate much from now on, how to take an use-case and example to its limit. If you’re passionate about a particular topic – go for it!\n\n\n\n\n\n\n\n\n\nSpelling Correction\n\n\n\n(BDA3, Ch1): Spelling correction, based on empirical frequencies provided by Peter Norvig. As in the previous case-study, you will have to code it up and figure it out for yourself – it is good for a warm-up, but challenging enough to keep you occupied.\n\n\n\n\n\n\n\n\nMonty Hall. Simpson’s Paradox and DAGs\n\n\n\nThe Simpson’s paradox is usually introduced to highlight the importance of conditioning. However, the only resource I found which gets to the core of the problem is Bradley Neal’s first lecture on causal inference.\nThe “paradox” part of it is resolved (or at least not puzzling), when we think about the causal structure of the problem (or the DAG of influences).",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#bias-variance.-fisher-information",
    "href": "01_fundamentals/prob_study_guide.html#bias-variance.-fisher-information",
    "title": "Fundamentals of probability and statistics",
    "section": "Bias-Variance. Fisher Information",
    "text": "Bias-Variance. Fisher Information\nI spend another lecture to deep-dive into estimators, because the concepts of bias-variance tradeoff and Fisher information have far-reaching consequences in a myriad of tools, applications and fields – especially machine learning. It is also an appropriate point in time to introduce a technique which was revolutionary at its time: bootstrap.\nThere are objections to the Bias-Variance decomposition when seen as a tradeoff, in the context of Deep Learning – however, in the most general sense, it is a universal problem not only in statistics, but also for human cognition. For an intuitive explanation, watch lecture 8, slides. See how this tradeoff needs an update for the modern deep learning.\n\n\nThis lecture is highly mathematical, but we will get some powerful intuitions about some fundamental tradeoffs we make in statistics, when selecting a model or estimator.\n\nBias-Variance decomposition and the curse of dimensionality\nFisher Information and Rao-Cramer lower bound\nThe Bootstrap scheme: motivation, applications, and limitations\n\n\n\n\n\n\n\nBootstraping for Confidence Intervals\n\n\n\nBias-variance can be made more relatable in code, simulations, and visualization. However, I will not leave you hanging without introducing a technique you can use for solving practical and concrete problems, namely – bootstrap.",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#hypothesis-testing.-neyman-pearson",
    "href": "01_fundamentals/prob_study_guide.html#hypothesis-testing.-neyman-pearson",
    "title": "Fundamentals of probability and statistics",
    "section": "Hypothesis testing. Neyman-Pearson",
    "text": "Hypothesis testing. Neyman-Pearson\nIn order to make sense of frequentist hypothesis testing, I strongly recommend you read about the original idea of Neyman and Pearson (error control – don’t make a fool of yourself too often in the long run). It is a “path of action” perspective of statistics.\nThis is certainly the most difficult lecture of the module, combining the math, programming, and even philosophy.\nI start from the first principles and will let go of mechanical application of procedures and conventions (p-values, \\(\\alpha, \\beta\\), test choice). You should to be able to justify all the choices you make during the phase of experiment design.\n\nPicking a default action. Type I, II errors. How costly is each type of mistakes?\nMinimal effect size of interest, Cohen’s \\(d\\)\nPower Analysis and Sample Size justification. How surprising are significant findings under each hypothesis? Positive Predictive Value\np-values simulation, p-curve under \\(H_0, H_A\\).\nConfidence Intervals - first check out this simulation. Also chapter 12, uses bootstrap to estimate those. The tricky idea of “capture percent”\n\n\n\nIn order to put everything together, there are four resources I can recommend:\n\nSpeegle’s book on data+probability+R\nHuber’s Chapter 6 of Modern Statistics\nStatistical thinking for 21st century\nImproving your statistical inferences\n\n\n\n\n\n\n\nAsking better questions\n\n\n\nThe most complicated part of hypothesis testing is asking better questions. I mean that in a highly technical sense, and whole-heartedly recommend you the following course from a TU Eindhoven professor, named “Improving your statistical questions”.\n\nMake riskier predictions: Non-Inferiority testing, Equivalence Testing, Range predictions\nPublication bias, open science, pre-registrations\nMinimal Effect Size of interest: telescope method and resource-based\nType 3 errors (solving the wrong problem)\nRead Werner Stahel’s “Relevance” paper and Gelman’s “Sign and Magnitude” paper\n\n\n\n\n\n\n\n\n\nA detour on the philosophy of science\n\n\n\n\nUnderstanding the philosophy of falsification and how it applies to hypothesis testing. Week2 of this course has a great 20 minute explanation.\nPhilosophy of science: Popper and Latakos, in this lecture. “The null is always false”\n\n\n\n\n\n\n\n\n\nCommon statistical tests are linear models\n\n\n\nThere is a zoo of different statistical tests and procedures, which might be very confusing – especially trying to remember their particularities. It’s important to realize that a lot of seemingly unrelated statistical tests in frequentist statistics are particular versions of linear models.\n\nCommon statistical tests are linear models and the python port\nChoosing a statistical test: difference in proportions and means, test of \\(\\sigma\\), correlations\nFor a bayesian alternative to t-tests, see Krutsche’s example\nIf you’re not clear if your distributional assumptions hold, use a nonparametric test",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#frequentism-vs-likelihood-vs-bayes",
    "href": "01_fundamentals/prob_study_guide.html#frequentism-vs-likelihood-vs-bayes",
    "title": "Fundamentals of probability and statistics",
    "section": "Frequentism vs Likelihood vs Bayes",
    "text": "Frequentism vs Likelihood vs Bayes\nThere are three main schools of thought in statistics, which have their respective metaphors: “path of action” (Neyman-Pearson frequentism), “path of devotion” (Fisherian Likelihood), and “path of belief / knowledge” (Bayesian). I like very much the presentation of each school of thought in the book of Hastie/Efron “Computer Age Statistical Inference”, chapter 2, 3, 4.\nEach one has their strenghts, weaknesses, and contribute tools & insights for our future use-cases. When we got into the topic of A/B testing and experiment design, we unavoidably stumbled upon a few fascinating philosophical questions in relation to the nature of evidence. The philosophical debate is fierce, but in statistical practice, less so. I suggest a level of pragmatism to pick the right tool/perspective for the particular job. In the courses I teach, I dedicate quite a lot of time on how not to fall into the most common pitfalls when applying frequentist methods. It’s an useful skill when critically reading the literature.\n\n\nBy now, you encountered the Neyman-Pearson (frequentist) approach. If you want another presentation, watch this lecture by Zoltan Dienes to get a sense of the orthodox approach: its power and limitations.\nThe likelihood approach is widely used in Machine Learning / Statistical Learning teaching and practice. This lecture by Zoltan Dienes contrasts Bayes Factors vs classical methods in t-test situations.\n\n\n\n\n\n\nThree approaches to single-parameter models\n\n\n\nWe can pick a simple example of inferring a proportion, which has many practical applications that you might remember from “Distribution Stories”. We care not just about the estimation, but also about confidence/credible intervals and the practical workflow.\n\nFrequentist: Normal Approximation, Agresti-Coull intervals\nLikelihood: Maximum likelihood, point estimates, bootstrapping. Check out this interactive visualization an lecture / lab.\nBayes: The full posterior distribution, the tricky business of prior choice",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#dead-salmon-experiment.-replication-crisis",
    "href": "01_fundamentals/prob_study_guide.html#dead-salmon-experiment.-replication-crisis",
    "title": "Fundamentals of probability and statistics",
    "section": "Dead Salmon Experiment. Replication Crisis",
    "text": "Dead Salmon Experiment. Replication Crisis\nLastly, we can’t avoid a conversation about the replication crisis happening in multiple disciplines, but especially in social sciences. What scientific literature can we trust? This is relevant not just for research and science, but will help you avoid many pitfalls in the business practice – therefore, you will be less likely to be fooled by randomness.\n\nMultiple testing, p-hacking, HARKING, snooping. Ethics and Integrity\nUnderpowered studies and vague questions\nPublication Bias, Open Science, Pre-registration and simulation\nFalse-discovery rate, Bonferoni correction\nConfounding, Mediation and all that causal jazz\nComputational Reproducibility vs Replication. Meta-Analysis\n\n\n\n\n\n\n\nDead salmon experiment\n\n\n\nAn examination of a famous experiment in neuroscience, putting into question standard/current statistical practices, leads to a conversation of controversies in medicine, psychology, and social science.\nJust think about how important this experiment was for the field of medicine – it won the nobel prize!",
    "crumbs": [
      "| 1. Business |",
      "2. Fundamentals of Probability & Statistics"
    ]
  }
]