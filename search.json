[
  {
    "objectID": "01_fundamentals/stat_foundations.html",
    "href": "01_fundamentals/stat_foundations.html",
    "title": "Fundamentals of Probability",
    "section": "",
    "text": "A well-rounded decision scientist should be fluent in the language of probability and statistics, easily go back and forth between a real-world problem and mathematical abstraction.1 Think how important is logic for maths, computer science, and philosophy. Similarly, probability theory is a formal language, the logic of uncertainty.\nEven though I emphasize the importance of studying through stories and simulations, there is one mathematical and quite abstract topic we can’t afford to avoid. Formally, at the core of probability theory is the triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) and we have to thank Andrey Kolmogorov for developing a solid axiomatic foundation. All the modern probability machinery we’re using in practice is build on top of that.\nSpeaking more bluntly, without being aware of the relevance and importance of the probability triple, no amount of stories will make us truly understand it. The reason for writing this chapter is that many students don’t know formally what a random variable and probability is. This leads to many mistakes in understanding and thus, in practice.\nOne of the tasks of probability theory is the construction of the probability triple with the tools of mathematical analysis (and measure theory). We will not go through this process of construction, but I will explain conceptually what each element is and why is it important.",
    "crumbs": [
      "Simulation of economic processes",
      "2. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "href": "01_fundamentals/stat_foundations.html#the-probability-triple",
    "title": "Fundamentals of Probability",
    "section": "The probability triple",
    "text": "The probability triple\nWe have to start with the notion of a random experiment (\\(\\mathscr{E}\\)). For example, a survey on voting intention, customer responses about their preferred product out of a selection, acidity of brewed beer in a factory. Think of it as a well-defined procedure that produces an outcome which we can observe, with the following properties:\n\nPossible results and outcomes are known apriori and exhaustably. For example: a coin/dice toss, quantity of laptops sold, time spent watching youtube, a borrower’s default on the loan, a choice between subscription options.\nWe don’t known for sure which result of \\(\\mathscr{E}\\) will manifest or appear before we run the experiment. Otherwise, it wouldn’t be random.\nDespite that, there is a perceptible regularity, which can be eventually measured and quantified, that is, encoding the idea of a probabilistic “law” in the results. That regularity could be a result of the large scale of the phenomena, for example, a small but systematic preference for flashy video thumbnails on youtube.\nRepeatability of the conditions in which the experiment runs, like the comparability and perservation of context. This is optional in the Bayesian perspective, where we’re not thinking in long-run frequency terms.3\n\n3 Note that Bayesian vs frequentist is a debate over the interpretation of probability. All of its constructions and facts are the same in both approaches4 In practice, it’s always a good idea to define what is the range of valid and allowed values that your variable or measuremet can take (scales)An elementary event is one of the possible results of \\(\\mathscr{E}\\), usually denoted by \\(\\omega_i \\in \\Omega\\), where \\(\\Omega = \\{ \\omega_1, \\omega_2, \\dots \\}\\) is called a sample space or universal set – which suggest the idea of all outcomes being listed apriori and exhaustively.4\n\nFrequentist vs bayesian interpretation\nFirst, let’s address the context in which the repeatable conditions requirement makes total sense (spoiler - it’s physics and coin flips). Think of measurements taken from the Large Hadron Collider experiments – the volume, noise, and complexity of data. Ultimately, we want to make inferences about the underlying, physical processes which govern the behavior and interaction of particles. The hypothesis tests based on the complicated models, despite their high confidence, are ultimately a probabilistic statement about how surprising is the data. Therefore, the long-run action of acting as if the null hypothesis is false, assumes that the experiment is reapeatable.5\n5 Practically speaking, we want to replicate the experiment multiple times in order to make sure it was not a flukeIn the frequentist perspective, the probability of an event is intrinsic to it’s nature, i.e. objective and is the limiting frequency with which that outcome appears in a long series of events. We can think of this convergence of frequency as a “statistical stability”, which can be supported only by a large number of trials, i.e. empirical evidence. This applies really well in gambling (by design) and physics, but is it plausible in human behavior? I don’t know, but it might be unreasonably effective at a large scale even if not true.\n\n\nFormally, we can represent this statement by Bernoulli’s theorem, where \\(m_n\\) is the number of times an event \\(A\\) occurs in \\(n\\) trials. \\[\n\\lim_{n \\to \\infty} \\frac{m_n}{n} = p\n\\]\nBayesians view probability as the degree of confidence in a belief. An apriori (subjective) knowledge, before seeing the data, will be updated iteratively with every new piece of evidence via inverse probability (Bayes rule). The fact that we need to define and ellicit our prior beliefs is not necessarily a weakness, especially in social science – if done correctly and rigorously, it can improve our inferences. If we have tons of data, our prior choice, if not too dogmatic or vague, will not matter.\n\n\nEvents and their spaces\nElementary events are rarely the events of interest to us. An event \\(A\\) is an union of elementary events: \\(A = \\bigcup\\limits_{i = 1}^n \\omega_i\\). We care not only about its realization, but also about other events, because they might contribute with additional information (about the probability) of our event of interest.6 This means that in one way or other, we need to conceptualize a space of all* possible events, which are subsets of \\(\\Omega\\). However, this is where the big (theoretical) trouble begins, because the event space has to have certain properties in order for the events to be measurable (so we can assign a probability which makes sense).\n6 This hints at the idea of conditioning, which is the “soul of statistics”\n\n\n\n\n\nAlgebra and Sigma-Algebra\n\n\n\nA set of subsets \\(\\mathcal{F} \\subset 2^\\Omega\\) is an algebra (field) if the following holds:\n\n\\(\\Omega \\in \\mathcal{F}\\) and \\(\\varnothing \\in \\mathcal{F}\\)\nIf \\(A \\in \\mathcal{F}\\) then \\(A^C \\in \\mathcal{F}\\) (closed under complements)\nIf \\(A, B \\in \\mathcal{F}\\) then \\(A \\cup B \\in \\mathcal{F}\\) (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection\n\nThe additional condition for sigma-algebra is that \\(\\mathcal{F}\\) should be closed under countable union. Sigma here refers to countability and additivity.\nIf \\(\\{  A_i \\}_{i \\ge 1} \\in \\mathcal{F}\\) then \\(\\bigcup\\limits_{i \\ge 1} A_i \\in \\mathcal{F}\\)\n\n\n\n\n\n\n\nSource: Wikimedia A beautiful visual representation\n\n\n7 If you wonder how in the world one can reason about the probability in a phylogenetic tree – this is where rigorous, measure-theoretic probability comes to rescueThe event space \\(\\mathcal{F}\\) is a sigma-algebra. For technical reasons which measure theory deals with, we usually can’t define a probability measure on all sets of subsets \\(2^\\Omega\\). Remembe that we don’t work only with discrete or countable outcomes. Besides \\(\\mathbb{R}\\), we can encounter wonky outcomes like simplexes, functions, trees, graphs, etc.7\nOn an intuitive note, we define the probability measure on sigma-algebras because if those conditions do not hold, the (probability) measure wouldn’t make sense, as unions of events would step out of the bounds of event space. The consequence of this is that important results in probability theory which are necessary for most statistical methods will not hold (fully).\nWe finally have the minimal tools to define the triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\). You can think of probability as an extension of measure, where we’re not concerned with lengths and volumes, but with chances of events realizing. Basically, we’re assigning a value between zero and one to elementary events \\(\\mathbb{P}(\\omega_i)\\). This is why additivity properties are key, as we care about random events \\(A\\), not only \\(\\omega_i\\).\n\n\n\n\n\n\nProbability Measure\n\n\n\nSuppose we have defined a measurable space \\((\\Omega, \\mathcal{F})\\), where \\(\\mathcal{F}\\) is a sigma-algebra. A probability measure is the function \\(\\mathbb{P}:\\mathcal{F} \\rightarrow [0, 1]\\) such that:\n\n\\(\\mathbb{P}(\\Omega) = 1\\) \nFor countable sequences of mutually disjoint effects, i.e. \\(\\forall \\{ A_i \\}_{i \\ge 1}\\) where \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing\\), the following holds \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\)\n\n\n\nProbability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is the fundamental object the whole probability theory is constructed upon. Kolmogorov took the informal, gambling-type probability and put it onto axiomatic foundations – which enabled future breakthroughs.\nNotice that this definition of probability is not the naive one, of number of successes over the total possible numbers an event could arise. You are allowed to use naive probability only in very special cases of discrete and finite sample space, where all elementary events are independent and equally likely. Otherwise and by default, carefully think about each element in the probability triple.\n\n\n\n\n\n\nMeasure theory rabbit hole: Why not all subsets?\n\n\n\nThe reasons for this are very technical, and the concept of a sigma-algebra is essential in resolving the resulting paradoxes. If you’re interested in these technical details, you can check out my relatively accessible introduction to measure theory and the Caratheodori extension theorem.\n\nEven though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its measure-theoretic foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.\nEvans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If \\(\\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\\) is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.\n\\[\\begin{equation}\n    p(x) = w_1 \\chi_{[0,1]}(x) +  w_2\\delta_1(x)\n\\end{equation}\\]\nEven though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.\n\n\n\nJeffrey Rosenthal begins his book by a similar motivation, constructing the following random variable as a coin toss between a discrete \\(X \\sim Pois(\\lambda)\\) and continuous \\(Y \\sim \\mathcal{N}(0,1)\\) r.v.\n\\[\\begin{equation}\n    Z = \\begin{cases}\n    X, p = 0.5 \\\\\n    Y, p = 0.5\n    \\end{cases}\n\\end{equation}\\]\nHe then challenges the readers to come up with the expected value \\(\\mathbb{E}[Z^2]\\) and asks on what is it defined? It is indeed a hard question.\nThank you for bearing with me through the theory you have probably seen before, but we’re not done. We’re still in the land of set theory, and it is very hard to operate that way in practice – so, we need a new concept which will allow us to use the tools of mathematical analysis in probability, in order to make it feasible for practical uses.",
    "crumbs": [
      "Simulation of economic processes",
      "2. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#random-variables",
    "href": "01_fundamentals/stat_foundations.html#random-variables",
    "title": "Fundamentals of Probability",
    "section": "Random Variables",
    "text": "Random Variables\nWe take the idea of random variable and probability density functions for granted, but at some point it was a huge breakthrough. It is a necessary abstraction in order to mathematically define the quantifiable characteristics of the outcomes of interest. Meaning, we start working with numbers instead of some qualitative properties.8\n8 Remember that we’re studying an interdisciplinary, quantitative field\n\n\n\n\n\nRandom Variable is not a variable, nor random\n\n\n\nA random variable is quantificator of elementary events, a function defined on the outcome space which maps the elementary events to the real number line. That mapping can’t be done in any way we wish, it has to perserve the informational structure of the sample space. That is one of the technical reasons for sigma-algebras we mentioned before and is related to the idea of measurability, meaning we can assign a meaningful “volume”.\n\\[\\begin{align}\nX(\\omega):\\Omega \\rightarrow \\mathbb{R} \\\\\ns.t. ~~ \\{\\omega \\in \\Omega | X(\\omega) \\leq r, \\forall r \\in \\mathbb{R} \\} \\in \\mathcal{F}\n\\end{align}\\]\n\n\nLet’s figure out what the hell do we mean by that fine print condition, using the diagram below. The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest.\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable. It is the construct that enables us to define the statistical population (some aspect of the phenomena which is relevant to us)!\n\n\nThe idea is that the preimage defined above \\(X^{-1}((-\\infty,r]) = E \\in \\mathcal{F}\\) on the following interval corresponds to an event E which should be in the event space \\(\\mathcal{F}\\). Because the only thing that varies is the limit of the interval r, the “randomness” comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is \\(F_X(X \\le r)\\).\nThe repartition or CDF – one of the most important constructs in probability is a direct consequence of the definition of the random variable.9\n9 Later in the course, we will see how ECDF (empirical cumulative distribution function) is an useful tool in data analysis and simulation\\[\nP(A ≤ r) = F_X(X ≤ r)\n\\]\nIn the practice of modeling, we often work with probability density functions, because it is more convenient in many cases. Then, in order to translate to probabilities, we would think in terms of areas under that curve. For sure, you remember the following duality between CDF and PDF (distribution, probability density function): \\(F'(x) = p(x)\\).",
    "crumbs": [
      "Simulation of economic processes",
      "2. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/stat_foundations.html#wonky-probability-triples",
    "href": "01_fundamentals/stat_foundations.html#wonky-probability-triples",
    "title": "Fundamentals of Probability",
    "section": "Wonky probability triples",
    "text": "Wonky probability triples\nAs a motivation of why do we have to understand all of this, when for most practical applications we can get by just fine with using the results and tools from probability, I will introduce two examples: one of compositional data analysis10 and time series analysis. What I want to say, is that for more “exotic” applications, we might need to tweak that probability triple because of the nature of the problem, which has downstream consequences for all the statistical machinery we use in those applications.\n10 \n\n\nSource: Dumuid: Data in a Simplex, which is later translated to \\(R^n\\) by a carefully constructed basis expansion\n\n\n\n\n\n\n\n\nThe curious case of Compositional Data Analysis\n\n\n\nSometimes, the data doesn’t “live” in our normal, intuitive, euclidian space \\(\\mathbb{R}^n\\). There are cases when the object of our analysis are proportions or compositions: think of what a material is made of, the proportion of the demand for different sizes of a shoe or garment.\nWe don’t necessarily care about their absolute value, but about their relative proportion. If we blindly apply traditional methods, or even statistical summaries, we will quickly hit weird results and paradoxes. So, we have to tweak existing methods such that they make sense for compositions.\nCompositional data analysis solves those issues by defining a probability triple over the simplex (instead of \\(\\mathbb{R}^n\\)): \\((\\mathcal{S}^n, \\mathcal{F}, \\mathbb{P})\\). This leads to a different definition of the event space \\(\\mathcal{F}\\), which is also a sigma-algebra and a different definition of the probability measure \\(\\mathbb{P}\\).\n\n\nFor the next example, you don’t have to understand what Gaussian Processes are or are used for.11 However, later in the course, we will discuss nonparametric methods for hypothesis testing. Their usefulness comes from the fact that we make less distributional assumptions about our population, therefore getting more robust results.\n11 Modeling complex patterns in (multidimensional) time series, for spatio-temporal modeling, any time our data points are not independent and we can quantify their “closeness”It’s not that these methods don’t have parameters, but the “parametrization” varies depending on how much data we have, which makes them very flexible in a wide variety of applications, where we just don’t know what is a reasonable distribution or parametric functional form for the relationship that we model.\n\n\n\n\n\n\nNonparametrics and Gaussian Processes\n\n\n\n\nIf we’re thinking about a regression from the nonparametric perspective: that is, over a set of abstract, smooth, twice differentiable functions: \\(f(x) \\in \\mathscr{C}^2, f : X \\rightarrow \\mathbb{R}\\), we might want to know how a draw of samples from an infinite set of functions might look like.\n\\[\nf(x) \\sim GP(\\mu(x); K(x,x'))\n\\]\nThe questions arises: how to define a PDF (probability density function) in this space? In my bachelor thesis, I got away with using Gaussian Processes by informally defining an apriori distribution of the mean vector \\(\\mu\\) and a Kernel (covariance function), then conditioning it on observed data with a Normal Likelihood.\n\\[\np(f(x) \\, |\\left \\{ x\\right \\})=\\frac{p(\\left \\{ x\\right \\}| \\, f) \\, \\mathbf{p(f)}}{p(\\left \\{ x\\right \\})}\n\\]\n\n\n\nSource: Bizovi: A posterior distribution of the Gaussian Processes, when conditioned on data\n\n\n\n\n\n\nIn the case of stochastic processes, we work with a sequence of random variables \\(\\{X_t, t \\in T \\}\\) and start asking questions:\n\nWhat kind of time dependency is there? (autocorrelation)\nWhat is the long-run behavior?\nCan we say something about extreme events?\n\nA lot of important applications in economics and finance are dynamic, so we have to work with time series very often. It gets worse when data is correlated not only in time, but also geographically – which is why the field of spatio-temporal data analysis is in such demand right now for policy-making.\nThus, a natural extension of this probability machinery we discussed so far is stochastic processes, underlying these dynamical systems. We can look at our time series as a manifestation, a particular instantiation of this latent process. Depending on which one we choose, we can model a wide range of phenomena.\n\n\n\n\n\n\nRabbit hole: machine learning theory\n\n\n\nLast, but not least, measure theory and advanced probability is important in machine learning theory, where we try to formally answer the question if our model can generalize successfully given its complexity, the sample size, and the “strength of signal”.12 Vapnik and Chernovenkis’ theory started a revolution in pattern recognition, gave a practical model of Support Vector Machines, and established the foundations of modern machine learning.\nOne such tool from probability which pops up in ML theory is uniform convergence, where “you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”. 13\nIn plain english, the object of investigation is an infinite of functions mapping from our input space (data, \\(X\\)) to the output / target variable \\(y\\). Therefore learning is an algorithmic process which uses our in-sample data to select a function from this infinite space, which will have the greatest chance to generalize and make good predictions.\n\n\n\n\n13 You can check out this answer on quora for a more detailed explanation of how measure theory is useful in ML theory12 Probably approximately correct (PAC learning)",
    "crumbs": [
      "Simulation of economic processes",
      "2. The Probability Triple"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html",
    "href": "01_fundamentals/ai_ml_cybernetics.html",
    "title": "What approach to take?",
    "section": "",
    "text": "In decision science, we have a lot of quantitative tools at our disposal, but we should have the wisdom to know which one is appropriate for each applied problem. Of course, it’s foolish to use machine learning when all we need is a controlled experiment or a statistical model which quantifies a causal effect. The converse is also true, you probably won’t use a linear regression to do demand forecasting at scale.\nThis might sound obvious, however, a surprisingly large number of people fall into the pitfall of misusing ML, or trying to do hypothesis testing when what’s needed is to find inspiration and come up with hypotheses. Even if you could use some ideas from LLMs (large language models) for time series forecasting, it’s often a bad idea.1\nThis is not a debate which tools are better, as the question makes no sense – each model has its purpose. Moreover, a statistical model can be used to make predictions, and machine learning methods are heavily used in analytics. For example, statistical models like GLMMs, used without a strong justification from causal inference, quantify only associations – which will result implicitly in doing “analytics”.2\nNow, you will be much more careful when people ask you “Can we use AI for that?” Remember, that our starting point is the problem and we choose the appropriate tool, not the other way around.",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html#analytics-and-data-mining",
    "href": "01_fundamentals/ai_ml_cybernetics.html#analytics-and-data-mining",
    "title": "What approach to take?",
    "section": "Analytics and data mining",
    "text": "Analytics and data mining\nIn analytics, the main goal is formulating better research questions and hypotheses, to get inspiration, find interesting, relevant patterns and relationships in massive datasets. These hypotheses might very well come from experience, observation, domain expertise, qualitative research, or even anecdotal evidence. However, in modern businesses they increasingly come from automated data mining methods and model-driven exploration.3\n3 First, it’s more efficient in very unclear situations and second, we can find patterns we couldn’t think of aprioriYou should be suspicious, as those hypotheses might not be true and the relationships spurious – they have to be tested in the context of a study or experiment! The mere fact of statistical significance shouldn’t tell you anything at this point, as there are too many potential ways in which our model might not be valid.\nEven when you monitor the current state of the business and its performance in a dashboard, which are supposed to be hard facts (given a correct data collection and processing pipeline) – you don’t know yet the reasons and causes of it.4 The same problem appears when we do a survey, if we don’t take into account how data came to be (the reasons for non-response and why our sample is different from population). Therefore, even a good description requires causal and statistical thinking.\n4 Reporting these “hard facts” is important, but almost immediately any business person will ask you “Why” and “What should we do about it?”\n\n\n\n\n\nThe art of formulating a hypothesis\n\n\n\nIn many introductory statistics courses, we take the hypothesis for granted, it’s often our starting point. Even worse, the standard textbooks often fail to differentiate between a business / scientific question, a process model, and a statistical hypothesis. They coincide only in the most trivial cases.\nThere is a combinatorially explosive number of hypotheses we could investigate, so how do we come up with one that is relevant to the outcomes, makes sense, is reasonable, plausible, deserves serious consideration?\nI argue it is an art which requires experience and knowledge in a domain, an intuition, sensibility, and attunement to the problem. In my opinion, it is the most underrated aspect of scientific enquiry and process.\nIn this course we focus on data mining and analytics as a way to get inspiration for good questions to ask and hypotheses to formulate. However, in anthropology or evolutionary biology, it could be done by careful observation of behavior, coupled with a deep understanding of the field, existing theories, their shortcomings and inconsistencies.5\n\n\n5 Often, these predictions and hypotheses follow from the theory itselfNow, being aware of the potential pitfalls, we can use these insights to select the most promising candidate hypotheses and questions to investigate further. This is one aspect in which an analyst is extremely important, as we can’t afford to test most ideas, nor do we have the time for it. Inspiration is not only useful with respect to questions, but for what features to include in our machine learning models (proxies for the strongest drivers of an outcome).",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html#statistical-modeling",
    "href": "01_fundamentals/ai_ml_cybernetics.html#statistical-modeling",
    "title": "What approach to take?",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nIf we have to make a few, high-stakes decisions under uncertainty which have major consequences on business outcomes and user experience, we probably need to do some rigorous statistics. For example, how to price products, to enter a new market or not, what products to develop, how to allocate advertisement spending across different platforms, whether to deploy a new recommender system.\nThis is an appropriate moment to remember and review the three challenges in statistics, 12 steps when we apply it to a research question, and the array of methodological issues we have to worry about.\nWe have to decide whether we need to do a randomized controlled trial, a quasi-experiment, or we’re limited to working with observational data, surveys, and self-reports. In contrast with analytics, where we engage in pattern mining, and with machine learning, where we want reliable out-of-sample predictions – in statistics, we need to think very carefully about the causal processes and the assumptions we’re making. Only then we have a chance to give trustworthy recommendations on how to intervene in a system.6\n6 Thus, when I say “statistics”, I mean statistical modeling informed by the causal processes and methodological aspects like measurement, samplig, validity, and reliabilitySince we deal with human behavior in the value chain of a firm, be it customers, employees, decision-makers, engineers – we need to be aware of the multiple subtle threats to the validity of our study. This means lots of planning and preparation. Moreover, we need to put in a tremendous amount of effort to criticize and validate our model so we can trust its conclusions.\nPerhaps, the most common mistake is when people use a predictive, machine learning approach for developing policies and interventions. More exactly, they train a machine learning model on past data and then try out different inputs for the decision variables and pick the ones which maximize an objective. This assumes that the system remains the same, therefore that the learned probabilistic relationship between inputs \\(X\\) and target \\(y\\) is unchanged under an intervention.7\n7 Perhaps, the clearest explanation of this phenomenon can be found in M. Faucre’s book “Causal inference for the brave and true”, in the chapter “When prediction fails”",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html#machine-learning",
    "href": "01_fundamentals/ai_ml_cybernetics.html#machine-learning",
    "title": "What approach to take?",
    "section": "Machine learning",
    "text": "Machine learning\nIf we have to make lots of decisions at scale to optimize a process, for example, demand planning and inventory management for 100k product SKUs, it cannot be done manually or with carefully designed experiments. In this case, an appropriate choice would be to learn from data.\nNotice how inventory optimization is a downstream task to the demand forecasting, therefore we’re ok with robust and reliable predictions from a black-box model. A big advantage is if we can also accuratey quantify the uncertainty in those predictions. We still have to be very careful in designing the objective the model is optimizing for,8 what features do we include in the model, and whether they’re drivers or reliable predictors of the demand.\n8 A hard problem in machine learning is to make sure the model performance metrics, and maybe loss function is aligned to business objectives. Your colleagues do not care about RMSE and cross-entropyWhy machine learning then, since we put so much emphasis on scientific rigor and trying to infer the causal processes? Sometimes, you just don’t have a theory or process model. For example, in recommender systems it’s just too complicated, with so many heterogenous users and items, each with their specific preferences and idiosyncracies. Then, we take down our scientist lab coat, put on the engineer’s hard hat, and build pragmatic solutions to decision-making at scale.\nMachine learning is “simply” a way to use optimization algorithms to go automatically from experience (data) to expertise (a program or recipe for prediction). In other words, models which learn from data find patterns and invariants which generalize beyond the training data.\n\n\n\n\n\n\nAre we in the business of ML?\n\n\n\nThe next two questions are tremendously important and will prevent you from embarking on an ML project which is doomed from the start:\n\nIs there an underlying pattern to be discovered?9\nDo I have the necessary and relevant data at training / prediction time?\n\nIf yes and yes, we might be in business, but we shall not forget about the pragmatic aspects: is it feasible to be done with a small team, without a huge investment? Are we solving the right problem?\n\n\n9 Think of a perfectly efficient financial market, there is no pattern to be learned. The same is true for absurd “research” which tries to predict crime propensity or sexuality based on photos.Don’t get me wrong, there are lots of exciting ML applications, but it’s not a silver bullet and there are numerous pitfalls one fall into. Thus, we should be aware of its power and usefulness, but also limitations. As a recent example, W.Heaven argues that Hundreds of AI tools have been built to catch covid and none of them helped.\nOne part of the problem is the mismatch between the real world, business problem and objectives, versus what models optimize for. Vincent Warmerdam brilliantly explains it in his talks “The profession of solving the wrong problem” and “How to constrain artificial stupidity”. 10 This should remind you of the conversation about statistical golemns which we had in the course introduction.\n10 V. Warmerdam - The profession of solving the wrong problem and How to Constrain Artificial Stupidity\n\n\n\n\n\nSplit your damn data! (on data snooping)\n\n\n\nSince we’re engaging in the business of data mining and analytics at one point or another of the project, we have to be extra careful. Intuitively, you understand that discovering hypotheses and testing them on the same set of data is a bad idea, because we’ll get an overly confident estimation of how much evidence we have.\nSometimes, we’ll do exploratory data analysis, find relevant and predictive features for our target variable by trying out a few models on the whole dataset. Next day, we “forget” about this, having the conclusions crystalized in our mind, and apply a new, final model … on the same data.11\nMaybe you’ll be lucky and get away with this, but the data is “contaminated” with our mining. Thus, before you get into the nuances of model building, validation, and selection – get into the habit of always splitting your data.\nGive that test dataset to a friend, locked under a key and don’t let them give you that data until you have your final model to be deployed and tested.\n\n\n11 Other times, people accidentally use some information from the test dataset when transforming their variables, e.g. by centering and using the mean on the whole dataset. Or they might use information from the future in a time-series applicationAt last, we have to go to similar lengths to validate and test our ML model as we did in statistics. You should build guardrails and sanity-checks for different scenarios in which the model might go really wrong. If it passes this rigorous critique, it has greater chance of finding a real pattern and generalize to examples outside our sample.",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html#implicit-learning-intuition-and-bias",
    "href": "01_fundamentals/ai_ml_cybernetics.html#implicit-learning-intuition-and-bias",
    "title": "What approach to take?",
    "section": "Implicit Learning, Intuition, and Bias",
    "text": "Implicit Learning, Intuition, and Bias\nIn the last two sections, I felt it was necessary to raise warning tales about what can go wrong in statistics and machine learning. It’s precisely because we will work with such a powerful machinery and attempt to tackle complex problems in our jobs and lives.\nA good metaphor for what we’re doing in this course is the way people learn. In a sense, we’re pattern recognition “machines”, with a powerful capacity for implicit learning. Implicit means we can’t articulate or explain how we did it. For example, riding a bike, catching a baseball, speaking, reading and writing, behaving in social situations, and so on.\n\n\n\n\n\n\nImplicit Learning of Artificial Grammars\n\n\n\nThere is a famous experiment, in which researchers invented words from two fake languages, i.e. two set of rules.12 Let’s say the words have between 3 and 8 characters, with appropriate vowel patterns. So, here we are, with two sets of words ready to be shown to our subjects.\nParticipants saw the lists and they had to say from what “language” does a word come from. It is a good example of experiment design coming from cognitive science research.\n\n\n\nSource: John Kihlstrom\n\n\nThey found that subjects differentiated them much better than chance and the results were statistically significant. During the interviews, when asked to explain how they did it, the responses were either “no idea”, or giving some rules. When implemented on a computer, those rules were not able to perform better than a coin flip.\n\n\n12 For the curious, they used a Markov Chain artificial grammarIn the artificial grammars experiment, those rules articulated by the people were a post-hoc justification and not how they were thinking. Even more surprising is the ability to distinguish between words which are total gibberish. This means we have a powerful capacity to find patterns and regularities in the real world, due to evolution building into us this powerful machinery of implicit learning.\nAnother hypothesis on how animals learn is the idea that some prior knowledge or mechanism is necessary to kickstart the process. Those mechanisms are there due to evolution optimizing for fittedness. We might refer to them as “instincts” or “natural intuitions”. The discussion about the fascinating interaction between nature and nurture is outside the scope of the point I’m trying to make right now. So, here we go with two more experiments.\n\n\n\n\n\n\nBait Shyness – Rats Learning to Avoid Poisonous Baits\n\n\n\nWhen rats stumble upon food with new smell or look, they first eat very small amounts. If they got sick, that novel food is likely to be associated with illness, and in the future the rat will avoid it. Quoting Dr. Shai-Ben David:\n“Clearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative effect when encountered in the future.” 13\n\n\n13 Shai Ben-David - Understanding Machine Learning, 2014. This book is difficult, mathematical, but highly rewarding if you want to understand the theoretical foundations of ML The bait shyness is an example of learning, not just rote memorization. More precisely, a basic form of inductive reasoning, or in a more statistical language – generalization from a sample. However, we do know intuitively that generalization is prone to error, as we might pick up on noise and spurious correlation instead of signal – we can get fooled by randomness and mistake it for a real pattern.\n\n\n\n\n\n\nB.F. Skinner: Pigeon Superstition\n\n\n\nIn an experiment, B.F. Skinner put a bunch of hungry pigeons in a cage and gave them food at random intervals with an automated mechanism.14 They were doing something when the food was first delivered, like turning around or pecking. The reward in terms of food reinforced that behavior.\nTherefore, they spent more time doing that exact same thing, without regard to the chances of those actions getting them more food. This is an example of superstition, a topic on which philosophers spilled so much ink.\n\n\n14 \n\n\nSource: skewsme; Here’s Skinner’s pigeon chambers\n\n\nShai Ben-David goes on to argue – what stands at the foundations of machine learning, are carefully formulated principles which will prevent our automated learners, who don’t have a common sense like us, to reach these foolish and superstitious conclusions.\nWhat is the common thread among these three stories about learining? In a nutshell, it’s about the ablility to differentiate between correlational and real, causal patterns.\n\nWhen all goes well we call it intelligence, intuition, a business knack. It’s our pattern recognition picking up on some real, causal regularities. It’s the common sense working as it is supposed\nWhen learning goes awry, we call it a (cognitive) bias. In worst cases, it can become a bigoted prejudice. We might attribute a causal explanation to a phenomena when it’s not. 15\nYou know very well that correlation doesn’t imply causation, due to multiple of threats to validity, including but not limited to: confounders, mediators, colliders, and reverse causalities\n\n15 For example, size of the wedding / long-lasting marriage, extraversion and attractiveness / competence, how religious people are / their ethical behaviorSo, what can we do as individuals and professionals? I think one way to get wiser is to cultivate a kind of active open-mindedness, which tries to scan for those biases and bring them into our awareness, such that we can correct our beliefs, behavior, and decisions. Another thing we can do is to update our beliefs in the face of new evidence, keeping a scout mindset, trying to see clearly, instead of getting too attached and invested in our beliefs and positions.\n\n\n\n\n\n\n99 biases, but Bayes ain’t one\n\n\n\nThere are a few biases so prevalent and harmful outside their intended evolutionary purpose, that we have to mention them: confirmation bias, recency bias, selection bias, various discounting biases, and survivorship bias.\nHowever, I’m not a big fan of how behavioral economics treats biases. First, if you take the axiomatic economic rationality of expectation maximization to be the normative standard – of course you’ll view any deviation as a bias. It makes no scientific sense to have lists of hundred of biases. At the very least we need a hierarchy, a structural-functional organization of them. Also we need to know their causes!\nThe best theory we got, in my opinion, comes from 4E Cognitive Science research. First, it challenges the normative position of economic rationality in real-world complexity, ambiguity, and our limited computational resources. Second, it gives us insight into why are we prone to foolishness.\n\n\nI think we’re extremely lucky to be in a field like decision science, where we can use formal tools from probability, causal inference, machine learning, optimization, combined with large amounts of data and domain expertise – in order to practice that kind of a mindset. However, let’s keep in mind that researchers are not immune to getting fooled.16\n16 I think we’re as vunerable as the next person, especially when it comes to decisions outside of our profession\n“And with no false modesty my intuition is no better. But I have learned to solve these problems by cold, hard, ruthless application of conditional probability. There’s no need to be clever when you can be ruthless.” – Richard McElreath, Statistical Rethinking\n\n\n\n\n\n\n\nThe double edged sword of our intelligence\n\n\n\nThe same machinery which makes us intelligent general problem solvers and extraordinarily adaptable, makes us prone, vulnerable to bullshit and self-deceptive, self-destructive behavior.\n\n\nBy analogy, but in a more technical sense, think of overfitting ML models and drawing wrong inferences. In business settings, I believe that firms will realize and appreciate that a decison scientist has this exact role – to help others rationally pursue their goals and strategy.",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/ai_ml_cybernetics.html#calling-bullshit-in-the-age-of-big-data",
    "href": "01_fundamentals/ai_ml_cybernetics.html#calling-bullshit-in-the-age-of-big-data",
    "title": "What approach to take?",
    "section": "Calling Bullshit in the age of Big Data",
    "text": "Calling Bullshit in the age of Big Data\nYou probably noticed that I used or implied the word bullshit a few times before. It is not a pejorative, but a formal term introduced by Harry Frankfurt in his essay “on Bullshit”. In romanian, the closest equivalent would be “vrăjeală”, a kind of sophistry.\n\n\n\n\n\n\nThe critical difference between Lie and Bullshit\n\n\n\nA liar functions with respect for the truth, as he inverts parts of a true story to convince you of a different conclusion. It is interesting that we can’t really lie to ouselfs, we kind of always know it’s a lie. Thus, we distract our attention away from it with salient, but irrelevant stuff and justifications.\nIn Bullshit, you try to convince someone of something without regard for the truth. You distract their attention, drown them in irrelevant, but appealing (supersalient) stuff. This is how advertisement works.\n\n\nIn our times, BS is much more sophisticated than the “good old” advertisement trying to manipulate you into buying something. I can’t recommend enough that you watch the lectures by Carl T. Bergstrom and Javin West,17 where they explain at length numerous ways we’re being convinced by bullshit arguments, but which are quantitative, have lots of jargon and pretty charts in them. These lectures are short, fun, informative, and useful for developing the critical thinking necessary when assesing the quality of evidence or reasoning.\n17 Carl T. Bergstrom and Javin West - Calling Bullshit: The art of Skepticism in a Data-Driven WorldThis kind of intimidating jargon comes from finance people, economists, when explaining why interest rates were risen, what happened in 2007-2008 great financial crisis. My “favorite” is cryptocurrency-related sophistry and some particular CEOs expertly making things more ambiguous, mysterious, and complicated with their corporate claptrap.\nThe last argument I want to address in this chapter is the following: “Due to ever increasing volumes and richness of data, together with computing power and innovations in AI – it will lead to the end of theory”.18 I couldn’t disagree more, both from a philosophical, statistical, and cognitive science perspective.\n18 Meaning, you will feed the machines all the data and it will figure out the theories and causal processes by itself, given the right feedback mechanisms\n\n\n\n\n\nSmall Data problems in Big Data\n\n\n\nIn huge datasets of customer behavior and web events, there are lots of observations and many features / attributes being collected, which theoretically should be excellent for a powerful ML model.\nHowever, when we go to a very granular aggregation level, the information can be extremely sparse, with high cardinality, ambiguous and inconsistent (all data quality issues). For example, in an e-commerce, you might have no information about a user’s purchases, just a few basic facts about their website navigation in the current session.19\nThus, you have deal with the cold start problem, data missing not at random, censoring and survival, selection biases. The data becomes discrete, noisy, heteroskedastic. You know the saying when it comes to machine learning – garbage in garbage out, and no one proved this wrong yet.\n\n\n19 From an engineering standpoint, I recommend you watch this talk “Big Data is Dead” by the creator of DuckDB.You can guess the second part of my rebuttal, which reduces to the fact that the causes are not in data. Even when there is a big, clean, and rich dataset, we can’t escape theory, which is our understanding of the world works and how data came to be. Implicitly or explicitly, we’re making an assumption somewhere: what data to include in the model and what is our desired objective.\nFor example, in demand forecasting, we need to provide the model relevant data about factors and drivers plausibly influencing demand: like weather, holidays, promotions, competition, and so on. First, we can’t collect “all the data” and by including irrelevant stuff we risk picking up on noise and spurious correlations.\nIn conclusion, AI is not a silver bullet, nor there is any magic to it. More high-quality data and better models are often necessary, but not sufficient to improve outcomes. We have to ask the right questions, frame and formulate a problem well, set objectives aligned with business strategy. If we let AI decide who enters quarantine during the pandemic, what would it optimize for and what unexpected side-effects can those decisions lead to?20 Therefore, a part of a decision scientist’s job is to constrain artificial stupidity. More precisely, foolishness, because it does perfectly fine what you instructed it to do.\n\n\n20 Remember, we can’t take everything into account, as this problem is combinatorially explosive and computationally unfeasible.",
    "crumbs": [
      "Introduction and Background",
      "v. Analytics, Stats, ML"
    ]
  },
  {
    "objectID": "01_fundamentals/roadmap.html",
    "href": "01_fundamentals/roadmap.html",
    "title": "Module I: Business School for Data Scientists",
    "section": "",
    "text": "The “business school” emphasizes again and again the idea of Decision-Making Under Uncertainty at Scale, across many industries and use-cases. We walk through three perspectives: Analytics, Machine Learning, and Statistics – then develop processes for each one. This module is heavy on methodology, since I want to bring back science into “data science”, but don’t worry, there will be enough case-studies."
  },
  {
    "objectID": "01_fundamentals/roadmap.html#a-selection-of-realistic-datasets",
    "href": "01_fundamentals/roadmap.html#a-selection-of-realistic-datasets",
    "title": "Module I: Business School for Data Scientists",
    "section": "A selection of realistic datasets",
    "text": "A selection of realistic datasets\nDeciding what topic, domain, and problem to choose for your project is a daunting task in itself. During the course, I present new tools to formulate good questions and how to design your research study. Unfortunately, in most cases, we will be limited by what open data is available. We encounter the same problem of “too much content”: there is so much data to work with, but most of it isn’t any good for our purposes.\n\n\nYou can rely entirely on simulation, but the way you desing the underlying data-generating process has to be very well thought out, informed by the specificities of the problem and an expectation of what data we’ll encounter in practice.\nFinding good data about business challenges is incredibly hard, since few firms will be open to sharing it. Even when we get our hands on a good kaggle competition, the problem is that the data has been already framed, curated, and put together for us. This means that we don’t get the real experience of end-to-end problem solving that we would encounter in practice. Therefore, I collected a list of datasets which are realistic and diverse enough. You can consider them as an inspiration and starting point.\n\n\n\n\n\nDataset name\nDomain\nProblem / Area\nComments\n\n\n\n\n1\nYaz restaurant demand\nbusiness\nNewsvendor problem\nA real dataset used for benchmarking inventory optimization algorithms from a restaurant in Stuttgart. Source: ddop\n\n\n2\nMercari vintage clothes marketplace\nbusiness\nPrice suggestion\nAn excellent dataset for GLMs and machine learning, where the text data is important. Source: kaggle / Mercari\n\n\n3\nAvocado prices and volumes from Hass board\nagriculture\nMarket research\nThis is a good opportunity to understand the dynamics of a whole industry. Source: kaggle / hass\n\n\n4\nOlist e-commerce\nbusiness\nEDA, databases\nA very rare example of freely available data published as a relational database. Good for open-ended projects. Source: kaggle\n\n\n5\nCorporacion favorita\nbusiness\nDemand planning\nOne of the best datasets to practice demand forecasting for groceries. Source: kaggle\n\n\n6\nTevec retail sales\nbusiness\nInventory management\nGood for short-term demand forecasting for the purposes of inventory optimization. Source: kaggle\n\n\n7\nLending club loans\nfinance\nCredit risk\nA large dataset of peer-to peer, graded loans, with data about clients, loan, and interest. Source: kaggle\n\n\n8\nDataCo orders\nbusiness\nLogistics and fulfillment\nOne of very few datasets for you to get a better grasp over outbound and inbound logistics. Source: kaggle\n\n\n9\nCriteo Campaign\nbusiness\nMarketing\nThe biggest dataset of randomized control trial marketing campaign for Uplift modeling. Source: kaggle\n\n\n10\nEase my trip\nairlines\nPrice prediction\nThis dataset has a few gotchas related to how the flights were selected and unavailability of seats remaining. Source: kaggle\n\n\n11\nAmazon reviews for beauty products\nbusiness\nNLP, Customer analytics\nAnalyzing customer reviews and feedback is a widespread use-case in businesses. Lots of data is available. Source: nijianmo\n\n\n12\nExpresso churn prediction\ntelekom\nChurn\nWe will discuss the challenges around modeling customer churn, especially survival models and interventions. Source: kaggle\n\n\n13\nSantader customer satisfaction\nbanking\nChurn\nSince the data is anonymized and mostly numeric, we would have to take a ML approach. Source: kaggle\n\n\n14\nSupply chain allocation\nbusiness\nLogistics and shipments\nAnother rare dataset on logistics, in which you assign routes to purchase orders from manufacturers. Source: kaggle\n\n\n15\nHospital customer satisfaction\nhealthcare\nCustomer analytics\nA pretty large-scale, general-purpose survey on client satisfaction. A lot of EDA and data cleaning is needed. Source: kaggle\n\n\n16\nBike sharing demand\ntransportation\nDemand planning\nSeveral datasets from different cities and ride sharing firms: Washington, Boston, London\n\n\n17\nNYC subway rides\ntransportation\nDemand planning\nAnother aspect of demand planning is load prediction and minimizing delays: NYC traffic, Toronto subway delays, NYC entry and exits\n\n\n18\nTaxi trips\ntransportation\nDemand planning, pricing\nMultiple datasets from different firms and cities: Chicago, NYC taxis, NYC uber\n\n\n\n\n\nThe curated list of datasets doesn’t appear in most books and resources that I recommend. The challenges they present are not easy and require quite a lot of work. Some of them will also teach you how to work with larger amounts of data. That said, there is still a lot of value in the didactic examples, so here are a few more directions I recommend to look into:\n\nP. Fader, B. Hardie, and E. Ascarza research on BTYD (buy till you die) models of customer repurchase behavior, churn, and LTV\nThis website on Bayesian networks has a few amazing case-studies on self-worth and depression, which are perfect for practicing causal thinking\nFacebook has synthesised the recent research in Marketing Mix Modeling into their open-source project called Robyn.\nRohan Alexander has a good example of multilevel modeling with post-stratification on US 2020 elections. Andrew Gelman in “Regression and other stories” has lots of great examples from political and social science."
  },
  {
    "objectID": "sim/3_lln.html",
    "href": "sim/3_lln.html",
    "title": "Core theorems for simulation",
    "section": "",
    "text": "In the first lecture, we learned how simulation can be useful in estimation, optimization, and for gaining insights into common economic processes. Last week, we reviewed the key constructs on which probability theory stands: the probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) and random variable \\(X:\\Omega \\rightarrow \\mathbb{R}\\). I hope I convinced you that it’s not “just theory”, but essential for understanding anything we’ll do next and that it has practical implications.1\nIn the first two hands-on labs, we used simulation techniques in R to solve the birthday problem and applied the Binomial distribution to the safari example. However, I didn’t justify or show why Monte-Carlo simulation works or how to draw random samples from a distibution, e.g. like rbinom() does.\nToday, I will introduce you once again to the law(s) of large numbers, show how the universality of uniform theorem gives us a method for generating samples from common distributions. We will then discuss the central limit theorem, which is an essential result in statistical inference.2",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/3_lln.html#practical-implications-of-lln",
    "href": "sim/3_lln.html#practical-implications-of-lln",
    "title": "Core theorems for simulation",
    "section": "Practical implications of LLN",
    "text": "Practical implications of LLN\nLaws of large numbers is a set of results in probability theory which justifies the Monte-Carlo approach to simulation and “makes it work”. You saw the simplest example of it when we discussed statistical stability and Bernoulli’s theorem in the context of probability triple.\nNote that one can go into a lot of mathematical depth when studying LLNs or have just a conceptual understanding of it. I’m opting for a middle ground, in which you will get just enough theoretical precision, so that you don’t confuse it with the central limit theorem. We will simulate it in R, highlighting the idea of convergence in probability (of the sample mean to the true/population mean).3\n3 Note that iid samples and finite variance are key assumptions and conditions\nlibrary(tidyverse)\nset.seed(137) \nnr_sim_pois &lt;- 100000\nlln_pois &lt;- replicate(n = 3, {\n    pois_sim &lt;- rpois(nr_sim_pois, 4.5)\n    seq(10, nr_sim_pois, 30) |&gt; \n    sapply(\\(x) pois_sim[1:x] |&gt; mean())\n}) |&gt; data.frame() |&gt; tibble()\n\n\n\nShow visualization code\nlln_pois |&gt;\n    mutate(nr_samples = 10 + row_number() * 30) |&gt; \n    pivot_longer(cols = c(\"X1\", \"X2\", \"X3\")) |&gt; \n    mutate(name = case_when(\n        name == \"X1\" ~ \"sim. 1\",\n        name == \"X2\" ~ \"sim. 2\",\n        name == \"X3\" ~ \"sim. 3\"\n    )) |&gt;\n    filter(nr_samples &gt; 60 & nr_samples &lt; 20000) |&gt;\n    ggplot(aes(x = nr_samples, y = value, col = name)) + \n    geom_line() + \n    geom_hline(yintercept = 4.5, lty = \"dashed\") + \n    scale_color_viridis_d() +\n    labs(x = \"number of samples\", y = \"sample mean\") + \n    theme_minimal()\n\n\n\n\n\nNotice the fluctuations at around n = 5000. This should not be unexpected, even when sampling from the true distribution\n\n\n\n\n\n\nAn immediate application of this result is that we get access to methods for numerical integration.\nBefore introducing laws of large numbers mathematically, let’s first discuss some of its practical implications. First and foremost, you should take great care when drawing conclusions about the mean from a small sample size, especially when comparing groups which have a different number of observations \\(n_j\\).\nIn the lab we will analyze a case-study of school performances on the final standardized test (SAT/ENEM) and you should not be surprised when you see most extreme performances for schools with a low number of students. Even if the schools are no different from population (in our simulation), we will get a large number of biased estimations for small schools.\nOn the bright side, we can use these theorems to our advantage any time we have to make small decisions in the very long run. Think of a casino as the classical example or an insurance company and why they always win on average given they stay in the business for a long time. You can also consider whether it’s foolish when people try to beat the stock market, i.e. do better in investment than a diversified set of index funds. You probably won’t own a casino, or work for insurance companies, or become a portfolio manager – but this insight could prevent you from shooting yourself in the foot.4\n4 So, parents know something when they say not to gamble and not to recover the losses with more gambling.\n\nShow code for squared error drop\nlln_pois |&gt; select(X1) |&gt;\n    mutate(nr_samples = 10 + (row_number() - 1) * 30) |&gt; \n    filter(nr_samples &gt;= 10 & nr_samples &lt; 10000) |&gt;\n    mutate(\n        theoretical = sqrt(4.5) / sqrt(nr_samples),\n        simulated = sqrt((4.5 - X1)^2)\n    ) |&gt;\n    ggplot() +\n    geom_line(\n        aes(x = nr_samples, y = simulated), \n        ol=\"grey70\", linewidth = 0.7) +\n    geom_line(\n        aes(x = nr_samples, y = theoretical), \n        lty=\"dashed\", col=\"darkred\", linewidth = 1) +\n    scale_color_viridis_d() +\n    lims(x = c(0, 3500), y = c(0, 0.52)) +\n    labs(x = \"Sample size\", y = \"Squared error\") +\n    theme_minimal()\n\n\n\n\n\nWe can compare the expected squared error curve vs the one we get in simulation. In practice, we care not only the asymptotic behavior, but how fast the average converges.\n\n\n\n\n\n\nIn the case of \\(X \\sim Pois(\\lambda)\\), we know that \\(\\lambda\\) is both the expectation and variance, hence the expected squared error will be \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{\\sqrt{\\lambda}}{\\sqrt{n}}\\)\nIn simulation and Monte-Carlo methods, we simulate a large number of independent and identically distributed random variables. By design, we can use the long-run convergence for numerical integration, estimation, sampling, and optimization. In our little artificial world of simulation, we’re the casino.\n\n\n\n\n\n\nLLN to the rescue from nasty integrals\n\n\n\nIn statistics, we encounter a lot of integrals of the following form, which do not have an analytical solution:\n\\[\n\\mathbb{E}_f[h(X)] =  \\int_\\chi h(x) \\cdot f(x) dx\n\\] where \\(f(x)\\) is a (unbounded) probability density function, and \\(h(x)\\) a transformation of the random variable. Robert and Casella5 rightfully argue that methods like Simpson or trapezoid rule suffer from the fact that we don’t know what is the relevant area to integrate over.\nThis means that we should be careful when using integrate() and MASS::area() in R, and in general, any method which might be numerically unstable or fragile.\nBut things become very easy if we generate \\(X_1, X_2, ..., X_n \\overset{iid}\\sim f(x).\\) as we approximate with the empirical average. This scales well to multidimensional integrals which are so common in statistical modeling.\n\\[\n\\bar h_n = \\frac{1}{n} \\sum_{j=1}^n h(x_j)\n\\]\n\n\n5 Robert, Casella - Introducing Monte Carlo simulation with R, 2009Maybe you didn’t realize, but we actually used this method for numerical integration in our safari case-study, where we computed the average number of people showing up as a function of the proportion of friends in people making reservations. In this week’s lab, we will see how can we use Monte Carlo integration for arbitrary functions, not just probability density functions.",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/3_lln.html#laws-of-large-numbers",
    "href": "sim/3_lln.html#laws-of-large-numbers",
    "title": "Core theorems for simulation",
    "section": "Laws of large numbers",
    "text": "Laws of large numbers\nNow, let’s define more precisely what we mean by the “convergence” mentioned above and present mathematically the weak and strong law of large numbers.\nLet \\(X_1, ..., X_n \\overset{iid}\\sim p(X)\\), with finite variance \\(\\mathbb{V}(X_j) &lt; \\infty\\). The weak form states that for all \\(\\epsilon &gt; 0\\):6\n6 In mathematics, when you see this, think that \\(\\epsilon\\) is our choice and it can be arbitrarily small7 I highly recommend Chapter 10 of Blitzstein and Hwang’s - Introduction to probability, 2nd ed (2019)\\[\n\\mathbb{P}(|X_n - \\mu| &gt; \\epsilon) \\overset{n \\rightarrow \\infty} \\longrightarrow 0\n\\] This is precisely the definition of convergence in probability, often denoted as \\(\\bar X \\overset{p} \\rightarrow \\mathbb{E}[X]\\). It simply means that the average of a large number of random variables “stabilizes” to the true mean. The proof is made by using Chebyshev’s inequality, which gives the upper bound on the expression above: \\(\\sigma^2 / (n\\epsilon^2)\\), which drops to zero as \\(n \\rightarrow \\infty\\).7\nMoreover, Slutsky’s theorem guarantees that if \\(\\bar X \\overset{p} \\rightarrow \\mathbb{E}[X]\\), then \\(g(\\bar X) \\overset{p} \\rightarrow g(\\mathbb{E}[X])\\) for real-valued, continuous functions \\(g\\).\nWe should remember, however, that the random variable \\(X_j\\) is not a single number, but a function \\(X_j: \\Omega \\rightarrow \\mathbb{R}\\). Therefore, for any \\(\\omega \\in \\Omega\\) we will have a sequence of numbers \\(X_n(\\omega)\\) which might behave very differently and the idea of convergence in probability doesn’t tell us anything about the long-run behavior of any particular elementary event.8 This is relevant in practice, because we don’t always care only about the average and not all \\(\\omega\\)’s are of equal interest to us.\n8 You can check out this answer which clears out many confusions in the difference between the weak and strong lawThe strong law of large numbers tells us that \\(\\forall \\omega \\in \\Omega\\), except a set \\(B_0\\) for which \\(\\mathbb{P}(B_0) = 0\\), the sample mean converges pointwise \\(\\bar X_n(\\omega) \\rightarrow \\mu\\). Think of this formulation when you encounter almost sure convergence in readings.\nSee, this stuff is not that scary. Unless you go into advanced econometrics and statistical research, you will not need to rigorously prove all of this or study it from a measure-theoretic perspective. But as an applied decision scientist, there is no excuse for not understanding the laws of large numbers conceptually.\n\n\n\n\n\n\nEmpirical CDF. Nonparametric statistics\n\n\n\nEmpirical cumulative density functions9 are often used in nonparametric statistics, where we don’t want to make strong distributional assumptions about the population model (e.g. \\(N(\\mu, \\sigma^2)\\)).\nWe instead use iid samples to approximate the unknown, arbitrary population CDF \\(F\\). We can count on LLN to assure us that it converges to the true \\(F\\).\n\\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{j = 1}^n \\mathbb{I}(X_j \\le x)\n\\] The sums of indicator functions means that we count how many X’s are less or equal than x. If you recall the last lab it’s precisely a Binomial distribution with \\(n\\) attempts and probability of success \\(F(x)\\).\n\n\n9 In romanian, “repartitii”. You encountered them when we discussed random variables and probability triples.Empirical CDF is also useful in quickly summarizing our data. In contrast with the histogram and kernel density estimations, we can quickly answer questions such as “what is the probability that a customer will spend less than 35$ in our store?”\n\n\n\n\n\n\nRabbit hole: Hoeffding inequality\n\n\n\nIn probability and statistics, we often don’t have an exact answer to some of the most important questions for modeling. But more often than not, statisticians can give us tighter or looser bounds on the answer. These statistical guarantees give us more confidence that our approach can work in principle.\nFor example, Hoeffding’s inequality is the simplest way we can think of a key idea in machine learning theory: probably approximately correct.10 Imagine you draw samples from a urn with green and red marbles, where we don’t know their true proportion \\(\\mu\\), but we observe the sample mean \\(\\nu\\). In a big sample, they will probably be close.\n\\[\n\\mathbb{P}[|\\nu -\\mu| &gt; \\epsilon] \\le 2\\exp(-2\\epsilon^2N)\n\\]\nNow, think of \\(\\mu\\) as the true, unknown function or pattern we want to learn: \\(f:X \\rightarrow Y\\) (in reality being a joint distribution of inputs and target \\(P(X, Y)\\)). Each marble is a data point \\(\\mathbf{x}_i\\) and the color is green if our model got it right. Therefore, Hoeffding’s inequality tells us the upper bound on verification for a single hypothesis (model with chosen parameter values). There is a long way to go from here to get some interesting results about learning, but it’s a good starting point.\n\n\n10 If you’re interested in ML, I recommend Yaser Abu-Mostafa’s “Learning from Data” book and course.Remember that the purpose of these deep dives is not for you to understand it right now, but to see whether something captures your attention and motivates to study in more depth. Also, when you will encounter these ideas in the future, in their appropriate context, you can refer back to these notes and tie it all together.\n\n\nI give lots of examples from machine learning and statistical modeling, as it is my specialty and these models can be applied in any domain\n\n\n\n\n\n\nRabbit hole: Cauchy and Jensen\n\n\n\nThere are other inequalities that are useful in statistics, machine learning, and optimization you have to be aware of:\n\nCauchy-Schwartz gives us a marginal bound on joint expectation, i.e. \\(|E(XY)| \\le \\sqrt{E(X^2)E(Y^2)}\\). It can be used to prove that correlation is between \\([-1, 1]\\), to prove the triangle inequality, and pops up in ML via cosine distance (a measure of similarity between two vectors).\nJensen Inequality states that if \\(g\\) is a convex function, then \\(E[g(x)] \\ge g(E[X])\\). It appears time and again in statistics, for example, in maximum entropy, Kullback-Leiber divergence (dissimilarity of two distributions), and log-probabilities (logistic regression, Bayesian estimation).",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/3_lln.html#universality-of-uniform",
    "href": "sim/3_lln.html#universality-of-uniform",
    "title": "Core theorems for simulation",
    "section": "Universality of Uniform",
    "text": "Universality of Uniform\nFor most practical intents and purposes, the section above outlines everything you need to know about the law of large numbers. Now, I will introduce a theorem which allows us to draw samples from most common distributions, assuming we have a way to generate pseudo-random uniformly distributed and independent numbers.11 The latter is a solved problem in R, just run runif(10000) and voila.\n11 You can refer to this chapter by Tom Kennedy for details on how uniform iid samples can be generated from scratch and how to test if they’re any good.\n\n\n\n\n\nTheorem: Universality of the Uniform\n\n\n\nLet \\(F\\) be a continuous CDF (strictly increasing function on the support of distribution), its inverse \\(F^{-1}:[0, 1] \\rightarrow \\mathbb{R}\\) (quantile function), and \\(U \\sim Unif(0, 1)\\).\n\nIf \\(X = F^{-1}(U)\\), then \\(X\\) is a random variable with a CDF \\(F\\).\nIf \\(X\\) is a r.v. with CDF \\(F\\). Then \\(F(X) \\sim Unif(0, 1)\\)\n\nWe can show that (1) holds with the following straightforward relations. In the last step we leverage the fact that \\(P(U \\le u) = u\\).\n\\[\\begin{align}\nP(X \\le x) & = P[F^{-1}(U) \\le x] \\\\\n& = P[U \\le F(x)] \\\\\n& = F(x)\n\\end{align}\\]\nWe can generalize the notion of an inverse so this theorem holds for an arbitrary CDF, not just continuous:\n\\[\nF^-(u) = \\inf \\{ x | F(x) \\ge u \\}\n\\]\n\n\nThe first part of the theorem sais we can construct X with the desired CDF, by plugging U (uniformly distributed random samples) into the quantile function. The second part is less interesting in practice, but has to be also true – if we plug a random variable X into its own CDF, we’ll get U.12\n12 Be careful, in this second part there is some potential confusion if you’re not careful with notation: it’s \\(F(X)\\), not \\(F(x)\\).To make sense of the second statement, remember that a function of a random variable is also a random variable. We know that \\(U = F(X)\\) should have values \\(u\\) between 0 and 1.\n\\[\\begin{align}\nP(U \\le u) & = P[F(X) \\le u] \\\\\n& = P[X\\le F^{-1}(u)] \\\\\n& = F(F^{-1}(u)) = u\n\\end{align}\\]\n\n\n\n\n\n\nExponential (Casella and Robert, pg 44)\n\n\n\nIf \\(X \\sim Exp(\\lambda = 1)\\), then its CDF is \\(F(x) = 1 - e^{-x}\\). If we assign it to \\(u\\) and solve for \\(x\\), we’ll get a way to generate samples from the exponential distribution \\(x = -log(1 - u) = -log(u)\\). The latter part is true for the range \\([0, 1]\\), since both \\(U_{[0, 1]}\\) and \\(1-U_{[0, 1]}\\) are uniform.\n\n\nCode\n-log(runif(1e4)) |&gt; \n    hist(\n        col = \"skyblue2\", border = \"white\", breaks = 50,\n        freq = FALSE, main = \"Exponentials from Uniform\", \n        xlab = \"x (time until engine failure)\",\n        xlim = c(0, 6)\n    )\ncurve(dexp(x), col = \"coral\", lwd = 2, add = TRUE) \n\n\n\n\n\nThe red line is the theoretical PDF from R’s dexp function\n\n\n\n\n\n\nI kind of lied to you by implying that the universality of uniform is everything we need to draw samples from distributions. The technical details to make it computationally efficient are not easy at all. Moreover, there are accept-reject methods when everything else fails. You can refer to Alexandru Marioarei’s extensive guide on simulation and Casella/Robert’s Chapter 2 and 3 for a much more in-depth explanation and demonstration.\nI think the best time to cover these behind-the-scenes details is only after you are comfortable and confident with actually doing the simulations to solve practical problems. Only then, it makes sense to ask how exactly do R or Python implementations work.",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/3_lln.html#stories-of-beta-and-gamma",
    "href": "sim/3_lln.html#stories-of-beta-and-gamma",
    "title": "Core theorems for simulation",
    "section": "Stories of Beta and Gamma",
    "text": "Stories of Beta and Gamma\nLet’s take this opportunity to highlight the story and applications of the Exponential distribution. It is the continuous counterpart to the geometric distribution, which counts the number of failures before the first success in a sequence of coin flips. In the case of exponential, \\(\\lambda\\) is the rate at which the events happen per unit of time. At a first glance, this makes it a good candidate for modeling waiting times, lifetimes or survival, time until a single (hardware) failure.13\n13 You can check out this presentation for a more in-depth explanation of \\(Expo(\\lambda)\\) applicationsHowever, the exponential has a key assumption and property of being memoryless, which doesn’t hold in the real world neither for waiting times, nor for machine lifetimes. If you remember your physics classes from highschool, radioactive decay does have this property and the exponential model is helpful there.\nOtherwise, we will use it as a building block in more flexible distributions which can accommodate more realistic assumptions – similarly as we did with Bernoulli and Binomial. We can use the probabilistic relations between the following distributions in order to simulate them not from the Uniform, but from \\(X_i \\overset{iid}\\sim Expo(1)\\).14 Note that all parameters of the distributions below take values in \\(\\mathbb{N}^* = \\{1, 2, ...\\}\\)\n14 Even though we can, it doesn’t mean it’s a good idea or that it’s efficient\\[\\begin{align}\nY & = \\beta \\sum_{j=1}^\\alpha X_j \\sim Gamma(\\alpha, \\beta) \\\\\nY & = \\sum_{j=1}^a X_j / \\sum_{j=1}^{a+b} X_j \\sim Beta(a, b) \\\\\nY & = 2\\sum_{j=1}^\\nu X_j \\sim \\chi^2_{2\\nu}\n\\end{align}\\]\nI don’t mention these relations as an academic exercise, but because we will heavily use these distributions in statistical modeling. Beta(a, b) takes values between 0 and 1 and can have different shapes based on the choices of its parameters – which makes it a good choice in modeling proportions.\nLater in the course I will introduce the Beta-Binomial model, which is a Bayesian alternative to the frequentist confidence intervals you’re familiar with.15 You should also know that there is a Beta regression, which can be used to model a proportion directly (not binary outcomes \\({0, 1}\\)). For example, how would you deal with modeling the share of income which is spent on housing?\n15 Where the Beta distribution will represent our prior beliefs about the distribution of the unknown proportion \\(\\theta\\)You can think of Gamma as the distribution of time until a machine breaks down, but where you need multiple pieces to fail. It is more flexible than the exponential, therefore can fit a larger number of patterns we observe in the real world.\nWe will use it to represent the uncertainty in the rate \\(\\lambda\\) of Poisson distributed outcomes (number of events in an unit of space/time). The resulting Gamma-Poisson model is a key building block for modeling counts of rare events like asthma deaths, kidney cancers, and deaths from horses in the Prussian army.16\n16 Do you know another model for this task? Linear regression won’t do the job, but you might encounter Poisson regression and generalized linear models in the futureLastly, this relationship of the exponential to Chi-squared isn’t very helpful. We would be better served by thinking of it as a sum of squared iid Gaussian random variables \\(X_1, ..., X_k \\overset{iid} \\sim N(0, 1)\\).\n\\[\nY = \\sum_{j=1}^k X_j^2 \\sim \\chi^2_k\n\\] You might remember \\(\\chi^2_k\\) from your hypothesis testing classes, as it appears when talking about the distribution of the sample standard deviation estimators: \\(S^2 = \\frac{1}{n - 1}\\sum_{i = 1}^n (x_i - \\bar x)^2\\). This should immediately make you worry if assuming normally distributed samples is appropriate.\n\\[\n\\frac{n - 1}{\\sigma^2}S^2 \\sim \\chi^2_{n-1}\n\\]\n\n\nCode\nsample_size &lt;- 4\nsigma &lt;- 9\nlots_of_sigmas &lt;- replicate(10000, \n    (sample_size - 1) / sigma^2 * sd(rnorm(sample_size, mean = 5, sd = 9))^2\n)\nhist(lots_of_sigmas,\n    col = \"skyblue2\", border = \"white\", breaks = 50, freq = FALSE, \n    main = \"Sample variances vs Chi-squared distribution\",\n    xlab = \"sample variance\",\n    ylim = c(0, .25), xlim = c(0, 13)\n)\ncurve(dchisq(x, df = 3), col = \"coral\", add = TRUE, lwd = 2)\n\n\n\n\n\nExample from Speegle & Clair, Chapter 5. Sample variances for \\(n=4\\), \\(\\sigma = 9\\), and \\(\\mu = 5\\).\n\n\n\n\nAs a segway to the next section, one can’t overstate the importance of Gosset’s student/t distribution and the idea of sampling distribution of the sample mean in statistics.17 If \\(X_0 \\sim N(0, 1)\\) and \\(Y \\sim \\chi^2_k\\), then\n17 Take a bit of time to read about Gosset’s story of inventing a method for hypothesis testing in quality assurance of Guiness’ beer factories.\\[\nZ = \\frac{X_0}{\\sqrt{Y / k}} \\sim t_k\n\\]\nThis should remind of the quantity you have seen a lot in your statistics’ classes, when the population variance is unknown and let’s be honest, population variance is always unknown. This result is exact, not asymptotic, but it requires our sample to be iid Gaussians. Can you tell which other assumptions are we implicitly making when using a t-test?\n\\[\n\\frac{\\bar x - \\mu}{ \\sqrt{\\frac{\\hat \\sigma^2} { n } }} \\sim t(n-1)\n\\]\nThe sample mean, technically, is an estimator \\(t : X \\rightarrow \\hat\\theta\\). Let \\(\\theta\\) be the true but unknown population parameter, which uniquely characterizes the data generating process. Then, the estimator is a function which takes as an input the sample X (viewed as random variables) and returns an estimate (statistic) of the population parameter. Remember that a function of a random variable is also a random variable.\nIn practice, since we work with a single, finite sample and would like to know how “surprising” is it to observe that “extreme” of a sample mean due to sheer chance (if null hypothesis is true).18\n18 We will pay much more attention and apply more rigor to this question in the following weeks",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/3_lln.html#central-limit-theorem",
    "href": "sim/3_lln.html#central-limit-theorem",
    "title": "Core theorems for simulation",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIn the law of large numbers, we saw how the mean of a sequence of random numbers \\(X_1, X_2, ... \\overset{iid} \\sim p\\), with \\(\\mathbb{V}(X_j) &lt; \\infty\\), converges in probability to the expectation \\(\\mu = \\mathbb{E}[X_j]\\). This result motivates the viability of the Monte-Carlo approach, but tells us nothing about how does \\(\\bar X_n\\)’s distribution looks like along the way. Therefore, the central limit theorem is an essential result for statistical inference.\n\\[\n\\sqrt{n} \\bigg( \\frac{\\bar X_n - \\mu}{\\sigma} \\bigg) \\overset{d}\\longrightarrow N(0, 1)\n\\]\nCLT states that for (very) large \\(n\\), the distribution of standardized sample means approaches the normal distribution, no matter how weird the distribution of individual \\(X_j\\) (as long as it has finite variance). This is remarkable, but we can’t celebrate for long, since we might not have sufficient data to leverage CLT for an accurate estimation.\nAlso, you have to remember that the theorem is concerned only with the central tendency and sais nothing about the tails, which is tremendously important in risk management, finance, and extreme value theory. Taleb Nassim wrote whole three books hammering down this point and the importance of (fat) tails.19\n19 Fooled by randomness, Black Swan, and Antifragile\n\nCode\nskewdata &lt;- c(\n    replicate(2000, 0),        # most customers buy nothing \n    rexp(200, rate = 1 / 10),  # some will buy a moderate amount\n    seq(100000, 500000, 50000) # some very large purchases or errors\n)\n\nmu &lt;- mean(skewdata)\nsig &lt;- sd(skewdata)\n\nsim_purchases &lt;- function(nr_samples) {\n    Z &lt;- replicate(10000, {\n        Xbar &lt;- sample(skewdata, nr_samples, replace=TRUE) |&gt; mean()\n        (Xbar - mu) / (sig / sqrt(nr_samples))\n    })\n    Z\n}\n\npar(mfrow = c(2, 2), mar = c(3, 3, 3, 3))\n\nfor (n in c(100, 500, 1000, 5000)) {\n    hist(sim_purchases(n),\n        col = \"skyblue2\", border = \"white\", breaks = 40, \n        probability = TRUE, \n        main = paste0(n, \" samples\"), xlab = \"\"\n    )\n    curve(dnorm(x), add = TRUE, col = \"coral\", lwd = 2)\n}\n\n\n\n\n\nExample from Speegle & Clair, Chapter 5. An example where we need a lot of samples to overcome the skewed distribution with outliers.\n\n\n\n\nImagine the following scenario, which might not be completely unrealistic in an e-commerce. We want to design an A/B test for the new cross-sell feature in order to decide if we launch it for all customers or not. The main KPI we optimize for is average order value. Before we start the experiment, we want to compute how many samples do we need in order for the experiment to answer our question – that the feature brings relevant (in $) and significant results.20\n20 We will come back to this question, as it is tremendously important and often done incorrectly in practice.So we choose \\(\\alpha\\) (type I error), \\(1 - \\beta\\) (statistical power), \\(\\Delta\\) (minimum effect size of interest) and we plug those into the R function power.t.test() to get the necessary sample size. But you remember in horror that the t-test assumes normal data and homogenous variance.\nSo you do a simulation which captures the typical features of customer behavior, in order to assess if this approach would work at all in principle:\n\nMost visitors buy nothing in a browsing session (or ever). This leads to a zero-inflated distribution\nThe average order value is skewed, with the majority of customers having smaller orders\nThere might be a few customers who spend a lot, i.e. clear outliers\n\nWhen it comes to the applications of CLT in simulation, we can use it to assess the convergence of our numerical calculation and draw confidence intervals or “confidence bands” for each fixed number of iterations. We did a rudimentary implementation of this in the sensitivity analysis of the safari case-study.\n\n\n\n\n\n\nRabbit hole: stock market fluctuations\n\n\n\nIf you want to dive a bit deeper into CLT and LLN, I recommend the following example from Blitzstein and Hwang, pg. 475, ex. 10.3.7.\nEach day, a very volatile stock rises 70% or drops 50% in price, with equal probabilities and with diﬀerent days independent. Let \\(Y_n\\) be the stock price after n days, starting from an initial value of \\(Y_0 = 100\\).\n\nExplain why \\(\\log Y_n\\) is approximately Normal for a large n and state its parameters.\nWhat happens to \\(E(Y_n)\\) as \\(n \\rightarrow \\infty\\)?\nUse the law of large numbers to find out what happens to \\(Y_n\\) as \\(n \\rightarrow \\infty\\)",
    "crumbs": [
      "Simulation of economic processes",
      "3. Three core theorems"
    ]
  },
  {
    "objectID": "sim/2L_safari.html",
    "href": "sim/2L_safari.html",
    "title": "Who will show up to a safari?",
    "section": "",
    "text": "In the previous lab we used elementary notions of combinatorics to solve and simulate the birthday problem. I also emphasized the limitations of the naive definition of probability and the need to understand more rigorously the probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\). We then saw that working with sets is awkward, hence the need for the idea of random variable.\nToday, we will learn how to represent uncertainty for a single variable in a story of people showing up to their safari trip reservation. If you need a refresher on descriptive statistics, I highly recommend you read Chapter 3 of the “Effect Book” and pay close attention to the hypothesis testing section.",
    "crumbs": [
      "Simulation of economic processes",
      "2L. Safari trips"
    ]
  },
  {
    "objectID": "sim/2L_safari.html#bernoulli-distribution",
    "href": "sim/2L_safari.html#bernoulli-distribution",
    "title": "Who will show up to a safari?",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nWe’ll start with the simplest version of the safari story, which is inspired by A. Fleischhacker’s Chapter 2 of “Persuasive Python”. Imagine you have a small business of organizing safari trips, where customers make reservations on the website, but don’t have to pay a deposit.1\n1 Of course, it’s a bad idea in practice since we all know people don’t always show up to what they signed up forYou have a pick-up truck with 3 available spots. Each customer has a probability of \\(p=0.85\\) of showing up and they make this decision independently. What is the probability that we’ll go for a trip with 0,1,2, or 3 people? For that, you’ll have to find the appropriate probability mass function (PMF) and the building block is the good old coin flip.\n\\[\n\\mathbb{P}(X=k) = {n \\choose k} \\cdot p^k \\cdot (1-p)^{n-k}\n\\]\nIn order to never forget this distribution, first, think about the story behind combinations – in how many ways can we split 10 people into teams of 4 and 6? This will be important when we want to compute in how many ways we could achieve k successes out of n trials, where each attempt is independent. 2\n2 Think of basketball shots, number of patients experiencing side-effects from a medication, number of fraudulent transactions, number of defects in a batch of manufactured products, etc3 This way, you will never have to rote-memorize the formulas, but can derive them logically when needed\\[\n{n \\choose k} = \\frac{n!}{k! \\cdot (n-k)!} = \\frac{n(n-1)(n-2) ...(n-k+1)}{k!}\n\\] Remember the falling factorial from last time and ordered sampling without replacement? In the case of combinations, we don’t care about the order in which we sample the people in the team. Therefore, by dividing by \\(k!\\), which is the number of ways we can arrange the k people in one of the teams, we correct for that over-counding. Last, but not least, it doesn’t matter which way we select the teams: \\(k\\) or \\((n-k)\\) first.3\n\n\n\n\nordered\nunordered\n\n\n\n\nwith replacement\n\\(n^k\\)\n\\({n + k - 1 \\choose k}\\)\n\n\nwithout replacement\n\\(n(n-1)...(n-k+1)\\)\n\\({n \\choose k}\\)\n\n\n\nWe can summarize what we learned about sampling in this neat table. The unordered sampling with replacement will be relevant only in the case of bootstrapping and its proof is a bit more involved.4\n4 You can check out the proof and the urn model that was used by Bose-Einstein to describe their condensate, in this lecture by Santosh S. VenkateshGoing back to our Bernoulli distribution, we use n choose k (combinations) to account for the number of ways in which, for example, 2 out of 3 people can show up. As a foreshadowing for the rest of the course, remember this key idea:\n\nFor each possible explanation of the sample (“hypothesis”, value of parameter \\(\\theta\\)), “count” all the ways the (obeserved) sample could happen. Explanations with more ways to produce the sample are more plausible. – Richard McElreath\n\nNotice the difference in R between dbinom() and rbinom(). The prior is the theoretical probability mass function (distribution) and the latter can be used to generate arrays of random numbers from the binomial. Now, you have all the necessary background in order to answer the initial question in a few lines of code.\n\nsetNames(dbinom(0:3, size = 3, prob = 0.85), 0:3) |&gt; \n    barplot(\n        col = \"skyblue\", border = NA,\n        xlab = \"number of passengers\", ylab = \"probability\"\n    )\n\n\n\n\n\n\n\n\n\n\nNotice how incredibly unlikely is it that no one shows up. And it’s clear why: \\(1 - 0.85^3\\)\nAfter this warm-up exercise, I will subtly modify the safari story, which will complicate the matters a lot. We will no longer be able to extract insights via mathematical analysis alone, hence, we will need to simulate!",
    "crumbs": [
      "Simulation of economic processes",
      "2L. Safari trips"
    ]
  },
  {
    "objectID": "sim/2L_safari.html#showing-up-to-a-safari-trip",
    "href": "sim/2L_safari.html#showing-up-to-a-safari-trip",
    "title": "Who will show up to a safari?",
    "section": "Showing up to a safari trip",
    "text": "Showing up to a safari trip\nIn reality, we might have friends and couples making reservations to the safari. For the sake of this problem, let’s assume that if one of them doesn’t show up, neither will the other. The proportion of people who make reservations for two is \\(c = 0.6\\), but we’ll have to compute the probability that we’ll have a couple in a given trip (\\(p_c\\)).5\n5 We can’t use the proportion \\((c)\\) directly, because a reservation for two leaves only one spot available\\[\np_c = c + (1-c)c\n\\] In order to see how we arrived at this formula, draw a probability tree and if still not sure, validate the results via a brute-force simulation which you can see below.\n\nlibrary(tidyverse)\nset.seed(13131)\n\nprevalence_couples &lt;- 0.6\nprob_couple &lt;- 0.6 + 0.4 * 0.6  # (1 - (1 - 0.6)**2)\nprob_showup &lt;- 0.85\n\nWe can check that our analytical answer for \\(p_c\\) = 0.84 is close to the simulation. This little exercise has no other purpose than for you to practice control flow and conditionals in R.\n\n\nShow code for simulating reservations by couples\n# validate final proportion of couples and individuals\nreservations &lt;- replicate(n = 10000, {\n    first_reservation &lt;- rbinom(1, 1, prevalence_couples)\n    first &lt;- ifelse(first_reservation == 1, \"CC\", \"I\")  # if-else expression\n    if (first == \"CC\") { second &lt;- \"I\" } \n    else {\n        second_reservation &lt;- rbinom(1, 1, prevalence_couples)\n        second &lt;- ifelse(second_reservation == 1, \"CC\", \"II\")\n    }\n    paste0(first, second)\n}) \n\nreservations |&gt; grepl(pattern = \"C\") |&gt; table() / 10000\n\n\n\n FALSE   TRUE \n0.1531 0.8469 \n\n\nOnce we know what is the probability of having two friends or a couple in a trip, we have all the necessary information to derive the PMF of the total number of people showing up. We’ll denote the random variable for the couple making a rezervation \\(Z \\sim Bern(p_c)\\), which is a coin flip or Bernoulli random variable. The probability that both friends will show up is \\(p^2\\), a fact which you can verify again by drawing a probability tree or 2x2 table.\n\\[\\begin{align}\nZ & \\sim Bern(p_c) \\\\\nY & = Bin(n=3 - 2Z, p) + 2Z \\cdot Bern(p^2)\n\\end{align}\\]\nThis is one way in which you can think about the total number of people showing up. The first term represents the individuals, which can be 1 when \\(Z=1\\) or 3 when \\(Z=0\\).6 The second term will be 0 if there is no couple and 2 otherwise, since \\(Bern(p^2)\\) can take values of \\(\\{0, 1\\}\\). This is an example of a mixture model, an extremely useful tool in statistical modeling, when we have heterogeneous sub-populations.\n6 More precisely, the distribution of number of successes k out of 1 or 3 attemptsAlternatively, you can look at this problem as a mixture of two cases: when we have only individuals or a couple and an individual customer.\n\\[\\begin{align}\nY_{ind} & \\sim Bin(3, p) \\\\\nY_{mix} & \\sim Bern(p^2) + Bern(p) \\\\\nY & = Z \\cdot Y_{mix} + (1-Z) \\cdot Y_{ind}\n\\end{align}\\]\nNow, good luck deriving analytically the distribution and expected value of Y. Even in this seemingly trivial problem we will have to simulate in order to understand what consequences does having reservations for two entail for our small business.\n\nsimulate_customers &lt;- function(\n    prevalence_couples = 0.6,\n    prob_showup = 0.85,\n    nr_sim = 10000\n) {\n    prob_couple &lt;- prevalence_couples + \n        (1 - prevalence_couples) * prevalence_couples\n\n    replicate(n = nr_sim, {\n        has_couple &lt;- rbinom(1, 1, prob_couple)\n        y_ind &lt;- rbinom(1, 3 - 2*has_couple, prob_showup)\n        y_mix &lt;- has_couple * 2 * rbinom(1, 1, prob_showup^2)\n        y_ind + y_mix\n    }) |&gt; table() / nr_sim\n}\n\nsimulate_customers()\n\n\n     0      1      2      3 \n0.0361 0.2101 0.1411 0.6127 \n\n\n\n\nNotice that this time we summarize the simulation by counting the number of times we observed 0..3 with the table() function.\n\n\nRelation between prevalence and probability of a couple making reservation\ntibble(\n    prevalence_couples = seq(0, 1, 0.01),\n    prob_couple = prevalence_couples + (1 - prevalence_couples) * prevalence_couples\n) |&gt; \n    ggplot(aes(x = prevalence_couples, y = prob_couple)) + \n    geom_line() + \n    labs(x = \"Prevalence of couples\", y = \"Probability couple makes reservation\") + \n    theme_minimal()\n\n\n\n\n\n\nThink why does this curve for probability of couples making a reservation as a function of couple’s prevalence have this particular shape.\n\n\n\nAs in the first lab, let’s do the same simulation in the tidyverse by using vectorized calculations of the formula we derived. We will then have to compare the results against a scenario in which customers are independent, as in the previous section.\n\nnr_sim &lt;- 10000\nsim &lt;- tibble(\n    has_couple = rbinom(nr_sim, 1, prob_couple),\n    nr_individ = 3 - 2*has_couple,\n    y_ind = purrr::map_int(nr_individ, \\(x) rbinom(1, x, prob_showup)),\n    y = y_ind + has_couple * 2 * purrr::map_int(has_couple, \\(x) rbinom(1, 1, prob_showup^2)),\n    model = \"mixed\"\n) |&gt; \n    select(y, model) |&gt;\n    rbind(tibble(\n        y = rbinom(nr_sim, 3, prob_showup),\n        model = \"binom\"\n    )) |&gt;\n    group_by(model) |&gt;\n    count(y) |&gt;\n    mutate(prob = n / sum(n))\n\n\nsim |&gt;\n    ggplot(aes(x = y, y = prob, fill = model)) + \n    geom_col(position = \"dodge\", width = 0.8) + \n    labs(x = \"Number of people showing up\", y = \"Proportion\") +\n    # scale_fill_viridis_d(option = \"E\") + \n    scale_fill_manual(\n        values = c(\"skyblue\", \"lightsalmon\"), \n        breaks = c(\"binom\", \"mixed\")\n    ) + \n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_minimal()\n\n\n\n\n\n\n\n\nThese results make sense, as we’re much more likely to end up with zero or one passenger than in the case of independence, because of that \\(p^2\\) term. Now, a fair question to ask is how sensitive is the result to different proportions of couples in the population. It is clear that if it is very small, the results will be more similar to \\(Bern(n, p)\\) – but we would like to know exactly, in expectation, how fewer people will show up as \\(c\\) grows.7\n7 This is perhaps the simplest example of sensitivity analysis, where we want to see how much our conclusion changes if we start from a different assumption.\\[\n\\mathcal{L}(p_c) =  \\mathbb{E}[Y_{ind} - Y(p_c)],  \\quad p_c \\in [0, 1]\n\\] As you will see in the simulation, the curve is not entirely smooth or monotonic, which shouldn’t be the case in theory. The fluctuations are small enough to be irrelevant in practice, but it’s useful in such cases to represent somehow our uncertainty in the estimation of the mean. Since our function simulate_customers returns only the distribution and not the samples themselves, we will have to use the formulas for the theoretical variance of an arbitrary, discrete random variable. We will use that to draw error bars around the mean difference (loss).\n\\[\n\\mathcal{L}(p_c) \\pm 2\\sqrt{\\frac{\\sigma^2}{n}}\n\\] This doesn’t make as much sense as the confidence intervals you encounter in statistical testing, but it gives us an idea how munch the mean could vary in our 10k simulations for each value of \\(p_c\\). We will investigate these question in much more detail and rigor in the following lectures and labs.\n\n\nI use the conventional name of loss function, as in machine learning, but for now think of it just as a difference / delta.\n\n\nCode\nset.seed(11131)\n\nprob_grid &lt;- seq(0, 1, 0.02)\nsapply(\n    prob_grid, \n    FUN = \\(x) simulate_customers(prevalence_couples = x, prob_showup = 0.85, nr_sim = nr_sim)\n) |&gt; t() |&gt; data.frame() |&gt; tibble() |&gt;\n    mutate(\n        prevalence = prob_grid,\n        expect = 0 * X0 + 1 * X1 + 2 * X2 + 3 * X3,\n        variance = (0^2*X0 + 1^2*X1 + 2^2*X2 + 3^2*X3) - expect ^2,\n        loss_due_couples = expect - max(expect),\n        loss_upper = loss_due_couples + 2 * sqrt(variance / nr_sim),\n        loss_lower = loss_due_couples - 2 * sqrt(variance / nr_sim)\n    ) |&gt;\n    ggplot(aes(x = prevalence, y = loss_due_couples)) + \n    geom_point(color = \"darkred\") +\n    geom_segment(aes(\n        y = loss_lower, yend = loss_upper, \n        x = prevalence, xend = prevalence\n    )) +\n    labs(x = \"Prevalence of couples\", y = \"Expected loss w.r.t. independece\") + \n    theme_minimal()\n\n\n\n\n\nNotice that we have a huge implicit assumption of unlimited demand. This means that after a certain threshold, it doesn’t matter if the proportion of couples grows, as we’ll always find another individual who makes a reservation. Thus, the distribution will be given by \\(Y_{mix}\\).\n\n\n\n\nThis story of people showing up to a safari should remind you to check for ways your observations are not independent. This happens quite often in statistical modeling when we deal with repeated observations, groups (like classes, schools, countries, firms), spatial correlation, clustered data (like members of a household), and time series.",
    "crumbs": [
      "Simulation of economic processes",
      "2L. Safari trips"
    ]
  },
  {
    "objectID": "sim/2L_safari.html#homework-and-study-resources",
    "href": "sim/2L_safari.html#homework-and-study-resources",
    "title": "Who will show up to a safari?",
    "section": "Homework and Study Resources",
    "text": "Homework and Study Resources\nAt the end of this week you should have a working understanding of the probability triple and random variables. This will help you better understand the stories behind probability distributions and move forward with simulations.8\n8 Look at your notes from probability and mathematical statistics class. Do they make more sense now once you have this new context and background?\nRead lecture two and understand what the probability triple means and how it’s relevant in practice\nReplicate and practice on the safari example, by changing the probability of showing up, number of passengers, and the proportion of couples in the population\nRead about the properties of the Bernoulli random variables in Speegle’s chapter three of “Probability, Stats, and Data: A fresh approach using R”. Run the R commands provided there\n\nLast, but not least, you might find some data processing with dplyr and visualizations with ggplot puzzling. In order to practice these skills, I recommend Hadley Wickham’s “R for Data Science (2ed)”. Try to work through the chapters outlined below in the next 2-3 weeks.\n\nChapter 1 discusses the “grammar of graphics”, which will help you understand the unusual syntax for data visualization.\nChapter 3 and 5 explains in more depth the essential elements of tidy data processing we’re using in this course.\nChapter 9 will show you how to make different kinds of visualizations step-by-step",
    "crumbs": [
      "Simulation of economic processes",
      "2L. Safari trips"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Applied Bayesian Statistics",
    "section": "",
    "text": "Once we have a confident grasp of the fundamentals, we continue on the path of applied Bayesian statistics. It is an extremely flexible and composable approach to building explainable models of increasing complexity and realism. We will learn new tools to deal in a principled way with missing data, measurement error, sampling biases, nonlinearities, discreteness, dynamics, and heterogeneity.\nInstead of applying an out-of-the-box model, we will build custom ones for each individual application in an iterative way. Choosing appropriate probability distributions and modeling assumptions will be critical in this process, along with model critique and validation. There are many ways to teach bayesian statistics and generalized linear models, but I think R. McElreath has the best approach, by giving a lot of importance to DAGs of influence, simulation, and ideas from causal inference.\nOne notable difference between the Bayesian approach and the traditional way advanced econometrics is taught, is that we will focus on computation instead of proofs and heavy-duty mathematical statistics. In a sense, it means that we work with a single estimator which updates our prior beliefs about parameter distributions. We declare the model and the sampler does the job for us! If it explodes, we probably mis-specified the model or priors.\nThe study guide for the third module was incredibly hard to put together, since there is so much content on the web and it’s so hard to strike a balance between clarity, simplicity, and the use-case being interesting, realistic. The most important criteria was for the case-study or resource to have code, data, and explained theory or methodology in a freely available e-book.\nI made sure to include interesting examples and archetypal applications for each statistical tool and theoretical topic, so you can immediately apply the concepts you read about or saw during the lectures. However, the responsibility to practice falls entirely on the learner – I can just do my best to make your journey less frustrating.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#introduction-to-bayesian-statistics",
    "href": "references.html#introduction-to-bayesian-statistics",
    "title": "Applied Bayesian Statistics",
    "section": "Introduction to Bayesian Statistics",
    "text": "Introduction to Bayesian Statistics\nAfter the first two modules, we should have a solid foundation for building more realistic models. As discussed in the introduction, randomized experiments and A/B tests have obvious limitations: might be unfeasible, unethical, or just not the right tool.\n\n\nWe are not wasting time by switching to Bayes (7 lectures to get to GLMs), because we treat all models under the same framework.\nInstead of taking regression as the starting point, then going into advanced econometrics – I prefer we switch to a Bayesian perspective, so we can build custom models taking into account the particularities of the phenomena of interest. The course culminates in Hierarchical GLMs, which should be sufficient for the majority of problems you encounter in practice, or at least a good first attempt.\nWe start simple, by modeling a single random variable \\(Y\\), choosing the appropriate distribution for each phenomenon, a conjugate prior for the parameters, doing simulations – then sampling from the posterior with pymc, numpyro, or bambi.1\n1 The R equivalents would be stan and brms, rstanarm. The latter two are a high-level API for most common models.Limiting? Yes, as in reality we care about the relationship between random variables. However, we can get a lot of insight from thoughtful modeling of the data generating process, which will serve as building blocks in more complicated and realistic models.\nIn the second module, we applied Bayes rule and got some insightful results in three totally different domains. However, we weren’t doing neither statistics, nor inference – but got into the right mindset. Now it’s time for full-luxury Bayes!\n\n\n\n\n\n\nBeta-Binomial Model. Estimating proportions\n\n\n\nI know, I know, the coin-tossing – simple, yet fundamental and found anywhere there is a binary outcome \\(Y_i \\in \\{0, 1\\}\\). There are many ways to estimate the success probability \\(\\theta\\), when we observe \\(k\\) successes from \\(n\\) trials.\n\nIn Bayes’ Rules is a detailed exposition of the theory, with examples about Michelle’s election support and Milgram’s experiment.\nShare of biking traffic in different neighborhoods (BDA3, Ch2, Pr. 8)\nA/B Testing for proportions in BMH. Just remember that experiment design is much more nuanced than performing such inference or a test.\nPolitical attitudes in 2008 Election data (BDA3, Ch2, Pr. 21)\nProportion of female births, given prior information. (BDA3, Ch2)\n\nThere are many more applications, but the ones below require more research and work. They are open ended and you can take these topics very far.\n\nThe debate on the hot hand phenomenon is not yet over. Here are the bayesians weighting in and some new research.\nBasketball shot percentages and true shooting, brilliantly explained by thinking basketball in a playlist about NBA statistics.\nImportant problem in ecology: estimating size of population based on samples (BDA3, Ch3, Pr. 6). The challenge is that in \\(Bin(\\theta, N)\\) both parameters are unknown. Here is an old paper.\nConfidence intervals and lower bound for reddit posts like/dislike ratio. Read more about simple ranking algorithms and the issues of sample size: reddit, hacker news. It is a good opportunity to work with the reddit API in order to collect data about posts and comments.\n\n\n\nThe next step is learning how to model count data, which will open up to us applications of a different flavor. It is not a coincidence that when learning linear regression, we will extend it to poisson and logistic regression.\nYou can notice how the issues of sample size creep in, as well as how to properly model variation within and between groups. I recommend you look up again the CLT\nNote that prior choice and justification is an art and science: you will have to learn and practice how to articulate assumptions and encode your domain knowledge into the priors. There is no universal recipe, but there are some guiding principles.\n\n\n\n\n\n\nPoisson Distribution. Gamma-Poisson Model\n\n\n\nCounts of independent events in a unit of (space/time/…), with a low probability. You can review the maths here. Below is a list of applications you can practice on:\n\nDeaths by horses in Prussian Army. Here is the historical data and a blog post if you need a refresher on Poisson distribution.\nAsthma mortality (BDA3, Ch2)\nAirplane fatal accidents and passenger deaths\nEstimating WWII production of German tanks based on samples captured\nComparing football teams and goals in football matches\nComparing birth rates in two hospitals\n\nCheck out the link functions for more sophisticated models. Also, in the examples above, we estimate the groups separately (corresponding to no pooling) – there are better ways. Also, you will see a poisson example of how to take into account the sample size\n\n\nThe next examples are a little detour, to appreciate the flexibility of the modeling approach we’re taking. We’re building upon previous models, by inferring which rate \\(\\lambda_1\\) or \\(\\lambda_2\\) is the most plausible at a given point in time. This way, we add complexity and realism to the model, by incorporating knowledge about the phenomenon we’re interested in.\nIdeally, we would leverage models which work well with time-series, like Hidden Markov Models. There is also a large literature in mining subsequences in a time series.\n\n\n\n\n\n\nPoisson changepoint detection\n\n\n\nEstimating rates, modeling a structural shift/change is a relevant, challenging, and unsolved problem in many fields. The models below are too simplistic to be useful in practice, but they capture the essence of real dynamics: things change not only continuously, but also structurally.\n\nCoal Mining disasters, pymc\nText Messages, pymc\nU.S. School Shootings, is there an increase in attempts and occurences?\n\n\n\nWe already worked with multiple parameters, even touching upon the relationship between two variables: counts and time \\(Y_t\\), but not really – it’s more helpful to think about that in terms of stochastic processes. Therefore, we need a new tool, which is a link function, a nonlinear transformation \\(g(x)\\) which maps \\(X\\) to the correct support of \\(Y\\). I recommend to introduce this before jumping into gaussian linear regression (LR), in order not to have the (flawed) impression that the LR is the only game in the town.\n\n\n\n\n\n\nLink functions. Golf case-study\n\n\n\nThe domain/geometry inspired, custom model is presented in pymc version, stan by Andrew Gelman, and stan by Aki Vehtari. It is modeling the relationship between distance and the probability of put, which is nonlinear and the sigmoid function won’t work well for this case.\n\n\n\n\nThis is an appropriate point to introduce linear regression and logistic regression. It is not too early, given the importance of those tools, however the presentation should be practical and pretty high level, as there is much nuance to deep-dive later.\nOf course, we cannot forget about the Normal/Gaussian distribution, which is so prevalent in nature and pops up everywhere in the statistical practice. It is the building block of many following models. Remember the key idea of the expectation of independent random variables and estimating the mean from samples. Also, keep in mind any time you’re doing regression that it’s all about the conditional expectation \\(\\mathbb{E}_\\theta[Y|X=x]\\).\n\n\n\n\n\n\nInverseGamma-Normal\n\n\n\n\nYou can find the theory and mathematical exposition in Bayes Rules, with a case-study of football concussions study\nSpeed of light experiment (BDA3, Ch3), data. You will find here and in any statistics textbook the cases of known and unknown variance, and how the \\(t_k\\) test is derived from the first principles.\nAs in the case of proportions, we can use the model above to model the difference between the means of two independent groups.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#multiple-groups-and-hierarchical-models",
    "href": "references.html#multiple-groups-and-hierarchical-models",
    "title": "Applied Bayesian Statistics",
    "section": "Multiple Groups and Hierarchical Models",
    "text": "Multiple Groups and Hierarchical Models\nWe already worked with groups in the case of difference in means (proportions and continuous variables) and making inferences for three and more groups, treating them as separate. We will see that such an approach is called “no pooling”.\nIn the traditional teaching of statistics, the above would be covered by t-test, tests for proportions, and when it comes to groups, by ANOVA. If you were lucky, these would’ve been treated as particular versions of linear regression, like in “Most common statistical tests are linear models”.\n\n\nOne more reason for this is that groups and categorical variables do not receive the deserved, nuanced exposition in linear regression. Also, comparing groups is so widespread, that having a tool to deal with the challenges which it poses is immediately useful in your practice inside any firm.\nIn this section, the goal is to show the idea of hierarchical models and partial pooling. I agree with BDA3 (Bayesian Data Analysis 3ed) approach to teach it before regression, as the latter needs a lot of nuance and a long time to learn to apply properly.\n\n\n\n\n\n\nBeta-Binomial for groups. Normal Approximation\n\n\n\nThere is no point in repeating all from the first section, as it is straightforward to apply for groups, as you’ll see when we compare it with a hierarchical approach.\nHowever, it is a good chance for a frequentist detour, to the Agresti-Coull confidence intervals, \\(\\chi^2\\) tests of independence, and nonparametric tests for proportions in multiple groups.\n\nErica Ma has a great talk for hypothesis testing for 2+ groups.\nI think the authoritative resource on Bayesian versions of the distribution-free methods for hypothesis testing is “Bayesian Statistics for Experimental Scientists”, found here. Unfortunately, it is expensive, so I suggest you look at the table of contents and search for the implementations elsewhere.\n\n\n\nI build upon the module 2 on A/B testing by introducing Multi-Armed Bandits. There are cases in which we are testing multiple groups, e.g. which images to show on the website, and we do it at scale. Moreover, we want to experiment countinuously and automatically, trading off between exploration and exploitation to maximize the payoff.\nMABs are a big topic in itself, and a very narrow, particular example of reinforcement learning. It can be very powerful when carefully designed and applied appropriately.\n\n\n\n\n\n\nA/B Testing and Multi-Armed Bandits\n\n\n\nI suggest you start from the didactic examples in Bayesian Methods for Hackers, Chapter 6. If you have an use-case in which this fits perfectly, you can check out the theory and more variants to implement it in more specialized resources.\n\n\nWe encountered the problem of sample size when ranking barbecue diners and reddit posts, but it deserves a few lectures in itself. Once you master a few methods of reasoning about \\(n\\), you can cut through so much bullshit in media, research, and literature.\nFor continuity with the previous “Building Blocks” section, here are a few examples for groups, where the dependent variable is following the Poisson distribution. In this case, the novelty is choosing a gamma prior based on the sample size information of each group. If you think we can do better than this trick – you’re totally right.\n\n\n\n\n\n\nGamma-Poisson for groups\n\n\n\nThe models become more complicated as we attempt to estimate parameters for each group of \\(n\\) observations:\n\nKidney Cancer rates, with priors chosen in accordance to the sample size (BDA3, Ch2). An R visualization\nGuns and suicides, with ideas from empirical and hierarchical Bayes.\n\n\n\n\n\nComplete pooling is when we ignore the fact that we have groups, and make one, global estimate. Of course, it would fall in the category of underfitting or model mis-specification, if the categories or groups are relevant.\nFinally, we’re ready to see how partial pooling and hierarchical (multilevel) models are such an important and powerful innovation, to the point where some practitioners argue (and I agree), that it should be the default way of (regression) modeling. Meaning, a strong justification is needed why your model doesn’t need that structure or modeling of heterogeneity. Keep in mind this advice, but always start with the most simple models when iterating towards the final one.\n\n\n\n\n\n\nHierarchical Models for Groups\n\n\n\nThe main example is a classic (gaussian): modeling the level of radon in houses on the first floor and basement, where the groups are counties (BDA3). We will see this example again in the Hierarchical GLMs section, where predictors at house-level and county-level are added.\n\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro, focuses on exactly the idea we need.\nPrimary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nBeta-Bionmial examples:\n\nEric Ma tutorial at pycon, about baseball batting, and the equivalent numpyro code. Another baseball example is a classic by Efron, implemented in pymc.\nPolice shooting training – detecting race bias. A full Bayesian workflow in bambi.\nAnother classic example is about Rat Tumors experiment, implemented in pymc, from BDA3, Ch5.\n\nA great Gamma-Poisson example is presented by Richard McElreath in Statistical Rethinking lectures: “Starbucks coffee-shops waiting time in queue”.\n\n\nOne of my favorite business applications of the ideas outlined in this section, is in the seemingly innocent problem of computing the lifetime value of a customer. Just to show how tricky this problem is, there is a niche, but extensive literature on the probem, starting from 2000-2005 by Rossi, Fader, and Hardie, which is entering into the mainstream only now, by 2020-2023.\n\n\nThe below is by far not the only model: there are versions for contractual and non-contractual settings with different assumptions. The culmination of this research, in my opinion, is Eva Ascarza’s 2013 paper.\n\n\n\n\n\n\nEstimating Customer Lifetime Value\n\n\n\nModel: Combining Gamma-Poisson and Beta-Binomial, with parameters at customer level. The math and code in pymc3 can be found here.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#regression-and-bayesian-workflow",
    "href": "references.html#regression-and-bayesian-workflow",
    "title": "Applied Bayesian Statistics",
    "section": "Regression and Bayesian Workflow",
    "text": "Regression and Bayesian Workflow\nI think most practitioners and educators will agree that Linear Regression is the most important tool to master in statistics. I mean it in the most general sense – not only knowing model details, assumptions, extensions and limitations, but how to use it effectively in practice in an end-to-end workflow.\nTo avoid naming confusions in the context of GLMs we’re going to study, and the fact that Regression can be done by many classes of models, I call the good old Linear Regression – “Gaussian Linear Regression”\n\n\n\n\n\n\nExamples. Introduction to the workflow\n\n\n\nI highly recommend the (freely available) book “Regression and Other Stories”, by Andrew Gelman, Jennifer Hill and Aki Vehtari. The latter has a course website for this book, with the examples coded in stan.\nAlso, the second resource, which I would say is even beter, takes on the perspective of causal inference from the very beginning of studying regression. In the course homepage you will find links to the pymc port, slides, and video lectures from 2023.\n\nEugene-Springfield community sample data: OCEAN personality traits as related to drug usage – no categorical variables, \\(Y\\) distribution normal-ish. Demo in bambi.\nEducational outcomes for hearing-impaired children, in pymc\nDangers of spurious correlation and accounting for them are displayed in the marriages and waffles example by McElreath – written in numpyro, but there is also a pymc version.\npymc moderation analysis: muscle mass and age\nThis case study about 100m runs from Calling Bullshit, shows the dangers of extrapolation beyond the range of \\(X\\) that the models were trained on.\n\n\n\nI have a third favorite book, which is freely available, called “Beyond Multiple Linear Regression”. It covers all that we discuss here, but from a frequentist perspective. Despite that, the examples are amazing and it has enough of theory to make it a self-contained resource – therefore, makes a perfect candidate for a port in pymc/numpyro. For a review of linear regression, take a look at the Kentucky derby horse race.\n\n\n\n\n\n\nSplines and Nonlinear Transformations\n\n\n\nThe linearity assumption in linear regression is often under-emphasized, as pointed out by the trio of RoS in a podcast episode. Linear Models are linear in parameters, but we should always think of the appropriate transformations of \\(y\\) and \\(X\\).\n\nSplines from Osvaldo Martin, on Bike Ridership (UCI data, bike sharing)\nSplines, from Statistical Rethinking, on Cherry Blossoms Data\nBe careful and recognize when you should be using a log-log model, expecially since power laws are so widespread in nature and human behavior.\n\n\n\nBy introducing splines and nonlinear transformations, we can easily increase the dimensionality of \\(X\\), with respect to \\(n\\) – which causes massive problems for inference. This is the appropriate moment to introduce a fundamental tradeoff between model complexity and out-of-sample predictive performance.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#regression-and-model-critique",
    "href": "references.html#regression-and-model-critique",
    "title": "Applied Bayesian Statistics",
    "section": "Regression and Model Critique",
    "text": "Regression and Model Critique\nUp until this point we took the sampling from the posterior provided by the probabilistic programming languages for granted, as magically converging to the true distribution. We also used very rudimentary methods of comparing models (via Bayes Factors).\nHowever, things go wrong in practice all the time: poorly specified and parametrized models lead to computational problems. It is important to be able to diagnose the chains, and even more, actively critique and interogate your models. At some point, you have to get an understanding of what MCMC, HMC does under the hood – and when you would trade off the accuracy for faster, approximate inference.\n\n\n\n\n\n\nModel Critique and Evaluation\n\n\n\nChecking domain and modeling assumptions, critiquing the models we build are one of the most difficult aspect of statistical practice. It is hard to give a tutorial for this, so I just suggest you read Richard McElreath’s Statistical Rethinking chapters on this topic, along with the more theoretical BDA3, Part II.\n\nFor an intuitive process which can structure your modeling and critique, I suggest McElreath’s “Drawing the Bayesian Owl”.\nChapter 2 of Bayesian Computation in Python, by Osvaldo Martin (BCP) is one of the best at exemplifying these concepts.\nPrior and posterior checks pymc docs\nAn end-to-end example in BCP, modeling the flight delays.\n\nThe frequentists and statistical learning approach have well-established tools for asessing the out-of-sample predictive performance. The fact that training is fast, they can easily use cross-validation and even bootstrap. A recent innovation in Bayes is LOO-PIT, an approximation to leave-one-out cross-validation, which leverages the fact that we have posterior sampling.\n\nA reminder of bootstrap, that is what the frequentists might do - chapter 10 or page 249. It is a powerful tool to have in your arsenal anyways.\nHere are the technical details of LOO-PIT, in the context of model comparison.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#regression-and-model-selection",
    "href": "references.html#regression-and-model-selection",
    "title": "Applied Bayesian Statistics",
    "section": "Regression and Model Selection",
    "text": "Regression and Model Selection\nOnce we are able to critique and evaluate a single model, it makes perfect sense to compare models of increasing complexity and with alternative assumptions. The following uses the same tools as before, but in a iterative workflow.\n\n\n\n\n\n\nModel Comparison and Selection\n\n\n\n\npymc. - Model selection with fake data and polynomials\nChapter 3 and 4 of BCP are beautiful, where linear and logistic regression are introduced, along with questions of interactions, nonlinear transformations, hierarchical models and model choice.\n\n\n\nThere are cases when the problem is in sampling and one common cause, besides mis-specification, are models with bad posterior geometry (from the perspective of samplers). As a rule of thumb – “round and smooth” is good. A prevalent solution in literature is to use better parametrizations, but in order to fix your problem, you first have to be aware that it might be the case.\n\n\n\n\n\n\nBad posterior geometry. Reparametrization\n\n\n\n\nTechnical: (mu, sigma) - Neal’s Funnel numpyro, there is equivalent pymc\nMore solutions in numpyro for bad posterior geometry\n\nThese examples in 2D are intuitive, however things become much more complicated and counterintuitive in multidimensional parameter spaces. The long story short, in practice, is to use heavier regularization (via stronger priors) in those cases.\n\n\n\n\n\n\n\n\nRegularization and Variable Selection\n\n\n\nA big topic in ML is regularization and encouraging sparse solutions (models). The point is that in highly-dimensional spaces, only a small subset of variables is relevant. Bayesians have sneaky methods of constructing priors which act as a mechanism for variable selection.\nThere is an additional topic of robuts methods (to outliers). Some machine learning models are robust(ish) out of the box: quantile regression, gradient boosted trees. Bayesians use heavier-tailed distribution to achieve a similar effect.\n\nSpike and Slab example on Kaggle\nHorseshoe prior implemented in pymc3\nRobust linear regression on simulated data, implemented here, in pymc/bambi.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#regression-and-nonlinearities",
    "href": "references.html#regression-and-nonlinearities",
    "title": "Applied Bayesian Statistics",
    "section": "Regression and Nonlinearities",
    "text": "Regression and Nonlinearities\nI can’t resist to suggest a case-study from finance: the good old portfolio returns optimization. The methods used in practice are much more sophisticated than the below, but it’s useful to be aware of the fundamentals. The Wishart distribution, used as a prior for positive semi-definite matrices (covariation) can be an useful building block in more complicated models to capture the covariation structure, e.g of different time series.\nAs a very advanced aside, I am fascinated by Michael I. Jordan’s approach to multidimensional time series modeling, where the covariation structure is evolving in time.\n\n\n\n\n\n\nPortfolio optimization a la Markowitz\n\n\n\nSee the example with Wishart prior and portfolios from BMH. We could do an optimization on point estimates, but this aprroach is more general, capturing the uncertainty in estimates.\n\n\nIf you got to this last topic on linear regression, months probably have passed. There is a good reason universities dedicate entire semesters for regression – there are so many nuances to get right: from both practical and theoretical perspective. One of these complexities is how to handle missing data, which deserves a course on its own.\n\n\n\n\n\n\nModeling how Data goes Missing\n\n\n\nIn Bayesian Statistics, we also have to be very explicit and declare our assumptions about how the data went missing – which is a hard, but beneficial thing to do. The good news – missingness is treated like an unobserved variable and is subject to the same inferences we’ve been doing before. We just need to pick the right model of missingness for each individual case.\n\nMissing data imputation, pymc, both for linear and hierarchical\nDiscrete missing data imputation numpyro, with simulated data and causal graphs\nPymcon - missing data tutorial in pymc3\nMissing Data Imputation",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#generalized-linear-models",
    "href": "references.html#generalized-linear-models",
    "title": "Applied Bayesian Statistics",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nNot every phenomena we model is continuous or appropriate for gaussian linear regression, but we can easily extend it with the building blocks we learned at the beginning. It’s all about those link functions and relaxing some of the assumptions of the linear regression. I want to point out how the following mirrors the simple, single-variable models we first practiced on.\nI do not want to make the transition from gaussian case to logistic, poisson and other variants sound trivial. Their behavior, evaluation, and interpretation is different and distinct. You can not equivocate them.\nAs a theoretical background of what connects these models into one single family, hence the name of GLMs, I suggest you read the section in the “Beyond MLR” about the exponential family. Richard McElreath has similar information-theoretic arguments in his lectures on Statistical Rethinking.\n\n\n\n\n\n\nLogistic Regression\n\n\n\nAs a fun aside about machine learning, logistic regression is the only model which is calibrated out-of-the-box, meaning the scores for the classification can be interpreted as probabilities, due to its specific loss function.\nFor the rest of the models (e.g. random forests), you would need to calibrate via an isotonic or piecewise-linear regression, on a separate, validation dataset. So, going back to the practice, here are some examples using the logistic regression:\n\nBeetles survival by concentration of chemical, via bambi – tries out different link functions and is a simple example to get started with.\nVote intention by age in the U.S., via bambi, 2016 ANES data, 1200 samples.\nSocio-economic influences on income bracket (\\(\\ge \\$50k\\)), also does model comparison, implementation in bambi\nMultinomial regression on the iris, the most boring dataset, but you’ll see that you have a solution for the multiclass classification without the multiple comparisons.\n\n\n\nWhen you have a numeric target variable, think twice if it is appropriate for the gaussian distribution. Sometimes, what we model is really, counts, or non-negative, or monotonically increasing with respect to a \\(X_i\\) (that alone deserves its own case-study). Sometimes, even poisson doesn’t work out due to overdispersion or zero-inflation, and it is not a rare exception, especially when modeling aspects of customer behavior.\n\n\n\n\n\n\nPoisson Regression\n\n\n\n\nNumber of laws regarding equality, which includes a discussion for the issue of overdispersion. Not sure at all that this would be a prototypical DGP story for a Poisson.\nStop and frisk data from BDA3 and RoS, the frequentist version\nCampus crime and estimating household size in Philippines from BeyondMLR.\nAlcohol and meds interaction, with simulated data.\n\n\n\n\n\n\n\n\n\nOverdispersion. Negative Binomial. Zero-Inflation\n\n\n\nThe negative binomial distribution is often found in customer behavior, in contractual or non-contractual settings, when it comes to purchases, subscription renewals.\n\nCockroaches and pest management, where Negative-Binomial, Poisson and Zero-Inflated NBD is investigated.\nFishing catches in a park, in numpyro\nStudents’ absence, UCLA data, application of negative binomial, written in bambi\n\n\n\nIn the probability fundamentals section, we discussed the use-case of estimating proportions and named the field of compositional data analysis. It is often found in practice, but disguises itself, which causes the wrong method to be applied.\nIf you encountered such a problem, when you care about proportions, mix of stuff, or compositions, not their absolute values or quantities, you can check out these lecture notes\n\n\n\n\n\n\nProportions. Compositional Data Analysis\n\n\n\nDirichlet regression, pymc3 - fake proportions dataset, but take some real ones from compositional data analysis books",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#introduction-to-hierarchical-models",
    "href": "references.html#introduction-to-hierarchical-models",
    "title": "Applied Bayesian Statistics",
    "section": "Introduction to Hierarchical Models",
    "text": "Introduction to Hierarchical Models\nWe finally reached the most exciting point in our journey so far, where we can properly model and explain sources of variation at multiple levels (of analysis). This is where we relax the assumption of \\(i.i.d.\\), replacing it with one of exchangeability.\nThe point is that correlated data causes problems in modeling when we don’t account properly for it: be it groups, clusters, categorical variables, time series, geography, etc. BeyondMLR has a very well thought, practical motivation for multilevel models.\n\n\n\n\n\n\nHierarchical Gaussian Regression\n\n\n\nWe come back to the radon example, by adding one covariate at each level: house and county. This explains a part of variation which was missed before and leverages pooling to take care of low sample size in some counties.\n\nRadon: Primary code reference: pymc - A Primer on Bayesian Methods for Multilevel Modeling\n\nbambinos a higher level API, models the log-radon. Alternatively, McStanPy implementation\nOmar Sosa - Practical introduction to Bayesian hierarchical modeling with numpyro\n\nBayesian Multilevel Regression numpyro on OSIC Pulmonary Fibrosis Progression data, which assesses the risks of smoking and not only.\nBeyondMLR presents a really interesting psychological study about stage anxiety for music performers at a conservatory.\nA repeated analysis of Stack’s facial feedback hypothesis, in the context of replication crisis, via bambi – full workflow.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#hierarchical-models-and-longitudinal-data",
    "href": "references.html#hierarchical-models-and-longitudinal-data",
    "title": "Applied Bayesian Statistics",
    "section": "Hierarchical Models and Longitudinal Data",
    "text": "Hierarchical Models and Longitudinal Data\nLongitudinal data is a special case of multilevel models, but has the distinct feature, that at the group level, the data isn’t iid, but comes as a realization of the stochastic process. Often called Panel Data in economics and social sciences. You will see a different terminology in that literature: of mixed or multilevel models.\n\n\n\n\n\n\nLongitudinal Data and Studies\n\n\n\nThese examples model the trend with a covariate, as a function of time – which is a happy case when we can do that. However, in practice, things become more complicated if we have to model and identify a stochastic process like \\(AR(k)\\), or \\(MA(n)\\).\n\nLongitudinal data with drinking teens, alcohol consumption per cohorts, pymc.\nSleep study and reaction times in time by subject. The same modeling idea is in pig growth study, both implemented in bambi.\nBeyond MLR charter schools longitudinal study.\n\n\n\nIf in the previous sections, I didn’t have a strong preference for the Bayesian Approach, in the case of multilevel models, I strongly believe Bayesian Inference to be superior and less prone to overfitting and numerical instability.\nAnother aspect of this, is that most mixed models packages will make certain decisions for us, without our consent, which could influence our results and conclusions. As you’ll see in these tutorials, we are forced to construct the model and declare, justify every choice of prior and model structure.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#hierarchical-models-case-studies",
    "href": "references.html#hierarchical-models-case-studies",
    "title": "Applied Bayesian Statistics",
    "section": "Hierarchical Models case-studies",
    "text": "Hierarchical Models case-studies\nThis is the end of the Module 3 (Applied Bayesian Statistics). These tools are so general and powerful, that they will last you a lifetime. However, there is so much more to learn and so many more challenges to tackle – that there is no end to the mastery of Multilevel GLMs.\n\n\n\n\n\n\nHierarchical Logistic Regression\n\n\n\n\nBeyond MLR, college basketball referee foul differential here\nGraduate Admissions, from McElreath,  UC Berkeley in Fall 1973 (numpyro)\nRasch item-response model for Nba fouls in crunch time, pymc\n\n\n\n\n\n\n\n\n\nHierarchical Poisson Regression\n\n\n\n\nAirbnb number of reviews\nEstimating the strength of a rugby team\nPaper investigating seat-belt use rates, with data probably taken from the department of transportation crashes website\n\n\n\n\n\n\n\n\n\nModels with 3 Levels\n\n\n\nAt last, it is useful to see an example where we have an even more complicated multilevel structure, as it will be the case in some practical applications.\n\nBeyond MLR 3-level seed germination\n\n\n\nEven after you learned all the technical intricacies, we’re hit again with the reality that association doesn’t imply causation. So, after being competent with this amazing tool – we have to go back to ideas from causal inference, if we want to get a more than associative insight about our theory and understanding of phenomena.",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "references.html#bayesian-machine-learning-and-bart",
    "href": "references.html#bayesian-machine-learning-and-bart",
    "title": "Applied Bayesian Statistics",
    "section": "Bayesian Machine Learning and BART",
    "text": "Bayesian Machine Learning and BART\nAs a bouns, I’m adding two flexible, general-purpose models, which you can treat as in the Machine Learning practices, but are Bayesian to the core. These are extremely useful if we care more about predictive accuracy, either for decision-making, or as a part of causal inference process, where a certain complicated relationship details aren’t important to us, but just the conditional average treatment effects.\n\n\n\n\n\n\nBayesian Additive Regression Trees\n\n\n\n\nBART from osvaldo, on bike shares\nPodcast Episode and an R package\nNonparametric methods - what is the benefit, name a few equivalents. Be able to explain a signed-rank transformation. This is the simplest and the most accessible explanation I know of so far.\n\n\n\n\n\n\n\n\n\nGaussian Processes\n\n\n\n\nGelman: Birthdays, hilbert space approximation repo, also in stan\n\n\n\nAfter the first two modules, we should have a solid foundation for building more realistic models. As discussed in the introduction, randomized experiments and A/B tests have obvious limitations: might be unfeasible, unethical, or just not the right tool. Even when we design experiments or surveys, we still need to model non-response, correct for biases, and account for confounds – meaning, engage in modeling.\n\n\nIn my opinion, among many excellent resources, Richard McElreath’s 2023 lectures on statistical rethinking is the best way to learn the kind of modeling which fits our course and problem space.\nI think that the best way to study regression and generalized linear models is to think very carefully about the causal processes underlying the phenomenon we’re studying. The big advantage of switching to a Bayesian perspective and GLMs, is that we treat all models under an unified framework. This means, we can focus on science and much less on the mathematical intricacies of advanced econometrics.\n\n\n\n\n\n\n\n\n\n\n\nLecture Agenda\nKeywords\nCase Studies / Activities\n\n\n\n\n1\nIntroduction to Bayesian Statistics\nEstimating proportions with Beta-Binomial, counts with Gamma-Poisson, defining priors\nElections, Football Goals, Speed of Light\n\n\n2\nRegression and the Bayesian Workflow\nDAGs of influence, categorical variables, assumptions, nonlinearities, splines, simulation\nE2E example on Howell’s study on height and weight. Summarizing estimands\n\n\n3\nConfounds. Good and Bad controls\nDeep-dive on translating DAGs into statistical models. Adjustment set, backdoor criterion\nE2E example of Waffle houses and divorce rates. Spurious correlations\n\n\n~\nOverfitting and Validation\nPrior and posterior checks, LOO-CV, LOO-PIT, regularization, robust regression, simulation-based calibration\nTBD\n\n\n4\nMultiple Groups and Hierarchical Models\nComplete pooling, No pooling, Partial pooling. Post-stratification and non-representative samples\nRadon concentration, Starbucks wait times, Customer LTV, Elections polls\n\n\n5\nBinomial GLMs\nLink functions, Simpson’s paradox\n1973 Berkeley admissions\n\n\n6\nPoisson GLMs\nOverdispersion, zero-inflated, Negative Binomial\nCampus Crime, Student absence\n\n\n7\nHierarchical GLMs\nDeep-dive into model formulation. Studies on stage anxiety and plumonary fibrosis.\nBasketball Fouls, AirBnb reviews, Seat Belts\n\n\n8\nLongitudinal Data\nChallenges of multilevel time series\nTeenage drinking, sleep study\n\n\n~\nSpecial topics\nBART and Gaussian Processes\nTBD\n\n\n~\nSpecial topics\nMissing data and measurement error\nTBD",
    "crumbs": [
      "Appendix",
      "3. Bayesian Statistics Study Guide"
    ]
  },
  {
    "objectID": "03_ml/roadmap.html",
    "href": "03_ml/roadmap.html",
    "title": "Modern Decision Science",
    "section": "",
    "text": "flowchart LR\n\n  DSc --&gt; Unsup(Dim. Reduction) --&gt; Clust(Clustering) --&gt; MM(Mixtures) --&gt; HDB(HDBScan)\n  Unsup --&gt; PCA --&gt; CA --&gt; UMAP\n\n  DSc --&gt; Cl(Classification) --&gt; T(Tree-based Models) --&gt; BG(Bagging) --&gt; XG(Boosting)\n\n  Cl --&gt; Im(Imbalance) --&gt; F(Fraud Detection)\n\n  DSc(Decisions, Scale) --&gt; Text[/NLP/] --&gt; EM[Embeddings] --&gt; Attn[Attention] --&gt; ABSA[ABSA]\n  DSc --&gt; RS[/RecSys/] --&gt; Mtr[Metrics] --&gt; FM[Factorization Mach.] --&gt; HM[Hybrid Models]\n  DSc --&gt; CV(DL: Vision) --&gt; Conv[CNNs] --&gt; AK[Approx. kNN]\n\n  Conv --&gt; HM\n\n  DSc --&gt; TS(Time Series) --&gt; MTS[Metrics] --&gt; XGB[ML Approaches] --&gt; DL[DL Approaches]\n  EM --&gt; HM\n\n\n\n\n\nFigure 1: In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods – while keeping in mind the particularities of the domain and applications."
  },
  {
    "objectID": "03_ml/roadmap.html#module-iii-ml-and-deep-learning",
    "href": "03_ml/roadmap.html#module-iii-ml-and-deep-learning",
    "title": "Modern Decision Science",
    "section": "",
    "text": "flowchart LR\n\n  DSc --&gt; Unsup(Dim. Reduction) --&gt; Clust(Clustering) --&gt; MM(Mixtures) --&gt; HDB(HDBScan)\n  Unsup --&gt; PCA --&gt; CA --&gt; UMAP\n\n  DSc --&gt; Cl(Classification) --&gt; T(Tree-based Models) --&gt; BG(Bagging) --&gt; XG(Boosting)\n\n  Cl --&gt; Im(Imbalance) --&gt; F(Fraud Detection)\n\n  DSc(Decisions, Scale) --&gt; Text[/NLP/] --&gt; EM[Embeddings] --&gt; Attn[Attention] --&gt; ABSA[ABSA]\n  DSc --&gt; RS[/RecSys/] --&gt; Mtr[Metrics] --&gt; FM[Factorization Mach.] --&gt; HM[Hybrid Models]\n  DSc --&gt; CV(DL: Vision) --&gt; Conv[CNNs] --&gt; AK[Approx. kNN]\n\n  Conv --&gt; HM\n\n  DSc --&gt; TS(Time Series) --&gt; MTS[Metrics] --&gt; XGB[ML Approaches] --&gt; DL[DL Approaches]\n  EM --&gt; HM\n\n\n\n\n\nFigure 1: In this ML/DL module, we focus on practical, challenging use-cases and reliable, workhorse methods – while keeping in mind the particularities of the domain and applications."
  },
  {
    "objectID": "business_context.html",
    "href": "business_context.html",
    "title": "Business context",
    "section": "",
    "text": "In the introduction, I loosely explained how decision science can contribute to improving firm’s performance and outlined some common applications. We saw how important is to understand the domain (like marketing, supply chain) and problem space by taking the point of view of our clients and decision-makers.\nThe environment many businesses operate in, is volatile, uncertain, complex, and ambiguous. The best way to learn to navigate it, is experience on the job and a solid background in business fundamentals.1 For example, if you’re a data scientist doing demand planning for a fashion e-commerce, it’s a good idea not only to review the forecasting literature, but also to understand the demand and supply particularities of the industry. Find a colleague who is a domain expert and learn as much as possible from them.\nI mean, there are no universal recipes in a business – it takes knowledge, experience, a well-exercised toolbox of models and frameworks to make good decisions. In this chapter, we will first discuss one way of thinking about diagnosis, what is strategy, and the value chain of the firm. Then, I will present in more detail the applications from the introduction and what quantitative tools might be helpful in tackling them.",
    "crumbs": [
      "Introduction and Background",
      "iii. Business context"
    ]
  },
  {
    "objectID": "business_context.html#warm-up-exercises",
    "href": "business_context.html#warm-up-exercises",
    "title": "Business context",
    "section": "Warm-up exercises",
    "text": "Warm-up exercises\nDuring a lecture, I usually ask students to give some examples of businesses, sevices, technologies, problems, and domains which have ML algorithms and statistical models behind the scenes.2 Did you take an Uber lately, watched a movie on Netflix, bought a t-shirt, got a credit card, had to update your insurance, went for groceries?\n2 Of course, there is that one person very passionate about sports, finance, or blockchain.\n\n\n\n\n\nDo some reverse engineering!\n\n\n\nPut yourself in the shoes of those businesses and think carefully about what strategic and operational decisions they have to make.\n\nWhat is the client need or problem and what value it adds?\nWhat was the firm’s objective?\nWhat constraints did they hit? Why was it a difficult problem?\nWhat made it an appropriate use-case for ML, Statistics, and AI?\nWhat would be a baseline, simple, or naive solution?\n\nIf you have a hard time picking an example, choose a direct-to-consumer e-commerce, like Zara, H&M, or a marketplace like Zalando.\nIn the case of Uber, Bolt (ride sharing platforms), we’re very much interested in same questions as before. However, we need to get a grasp on the idea of market-making. If you were to reverse-engineer their pricing algorithms, how would you go about doing it?\n\n\nIt’s ok it you can’t answer some of the questions yet, but one sign of growth as a decision scientist is when you have a pretty good idea of what systems and methods are powering Uber, for example. By the way, those firms often brag about it in their tech blogs, which gives us not a complete solution, but a decent working hypothesis.3\n3 In this course you no longer are the consumer, but you have to think like an employee of those firms\n\n\n\n\n\nWhat is the purpose of a business?\n\n\n\nThere is one philosophical question you have to answer for yourself: “What is the purpose (raison d’etre) of a business?” People will give different weights4 to the following options, depending on their background, context the question is asked in, beliefs, and aspirations.\n\nMaximize profits and gross margin?\nGain market share, grow, and maximize future discounted cash flow?\nMaximize shareholder value?\nBring added value to customers by solving a problem?\nContribute to economy and society (tax, employment)?\nCreate new markets to allocate resources efficiently?\n\nAre you considering to start a business in the next 5 years? Now, imagine you’re in charge of public policy or participating in a think-tank / NGO for youths. Formulate a research question that a further study would try to answer regarding students’ entrepreneurial intentions and perceptions.\n\n\n4 We can use a 1 to 5 scale from strongly agree to strongly disagree, also called Likert scaleThe first thought experiment shows you how prevalent are applications of ML and statistical modeling. The second one highlights how difficult it is to take a vague research question, hypothesis, or idea and design a study which has a good chance of answering it. It will take a lot of practice to formulate those questions well.\n\n\n\n\n\n\nFood Stamps and Fermi Estimation\n\n\n\nThere is a good reason why management consultants practice Fermi Estimation to do sanity and order-of-magnitude checks. Such problems are asked in many interviews: “how many people are online on Facebook right row?” or “what is the potential market size for socks in the US?” All of those are multiplicative processes and formulas.5\nTake a look at the case study on food stamp fraud by Carl Bergstrom and Jevin West in Calling Bullshit. Practice on other classical Fermi estimtion examples from this video of Phil Wilson.\nThen, try to come up with a guess-estimate of how much revenue does your university canteen is generating per year. Assess the estimation and refine your assumptions.\n\n\n5 Therefore, we can use the geometric mean for the quantities we only can put a range on, but don’t know their exact value.Fermi estimation teaches us that we can be systematic, even when making back-of-the napkin, rough guesses. We have to make our assumptions clear about the known and unknown quantities, their plausible ranges and refine those when the result doesn’t make much sense. A fantastic book on climate change by David McKay, “Without the hot air”, heavily leverages Fermi estimation to assess what each country can feasibly achieve with renewable energy.",
    "crumbs": [
      "Introduction and Background",
      "iii. Business context"
    ]
  },
  {
    "objectID": "business_context.html#diagnosis-and-strategy",
    "href": "business_context.html#diagnosis-and-strategy",
    "title": "Business context",
    "section": "Diagnosis and Strategy",
    "text": "Diagnosis and Strategy\nYou probably encountered before the SWOT analysis and SMART criteria for setting objectives.6 Those are good tools to start with, but can be easily misused and result in fluff. In order to avoid those vague answers and not to fool ourselfs, we need to start thinking like scientists, which we’ll begin to do here and elaborate in the next chapter.\n6 Strengths, weaknesses, opportunities, and threats. Specific, measurable, achievable, realistic, time-bound. Yes, management consultants absolutely love acronyms\n\n\n\n\n\nA must-read paper\n\n\n\nKim Warren’s paper “The Dynamics of Strategy”7 was the best investment of an hour in my professional journey. It will teach you to think about accumulation of strategic resources, rates of in-flow and out-flow, feedback loops, drivers of those rates, and decisions influencing the whole system.\nI will summarize below the most important conceptual and practical arguments from the paper, but there is no replacement to actually reading it.\n\n\n7 Which summarizes well his book “Strategic Management Dynamics” (Wiley, 2008)8 The paper thinks of causes in a systems’ dynamics sense, not in a statistical sense of associations and influencesThe first insight is that we need to look at history, dynamics, trajectories of how and why we ended up in current situation. We need to understand “how our business works”, the causes of performance, since current resources and capabilities with be determining to a large degree what will happen in the future.8 This understanding needs to be quantified, measured, and backed up by data.\nBy resources, I mean concrete or intangible “stuff” which can be accumulated in time, as a cash, equipment, products, production lines, a customer base, employees, brand reputation, employee expertise, etc. Formally, they’re stocks and measured at a point in time. On the other hand, we can look at performance in terms of sales, profitability or non-financial measures of customer retention, satisfaction, acquisition, brand recognition, production efficiency, etc.9\n9 The best practices for measuring performance will vary depending on the domain and is a topic of study in itself\n\n\nSource: Kim Warren: Status Quo, Desired and Feared Trajectories\n\n\nBefore we discuss the technicalities and practical relevance of the systems dynamics perspective10 of a firm’s performance, let’s think about the trajectories in the diagram above. This way we easily generalize a SWOT analysis to take into account the history and quantify the claims.\n10 Note, that this is the closest we have to “a science” in businesses and strategic management\nIf we keep doing everyting as before, what is the most likely trajectory of profits? Under what assumptions? If the trajectory is promising to us, that is our strenghts and capabilities contributing to it. If not, our weaknesses.\nA feared trajectory, e.g if our business is hit by a supply chain shock, crisis, competition starting a price war, or weak customer demand. It’s the threats a firm is vulnerable to\nA desired or aspirational trajectory. Is it reasonable and realistically achievable? If yes, what strategy and tactics should we implement, how sould we act? This is our opportunity.\n\n\nStocks, flows, and dynamics\nIf we want to launch a beer brand which sells in lots of grocery stores, our “resources” on the demand-side are customers, stores (as intermediaries), and marketing people responsible for advertisement and pitching it to stores. If we want to open a new restaurant, we talk about customers, staff, and the menu (our product).\nIt’s useful to classify supply-side resources into capacities (machine, aircraft seats, stores, production lines), staff (engineers, pilots, sales, factory workers), and product. Resources grow by getting an in-flow \\(\\psi_{in}(t)\\) at a given rate, which could be nonlinear and explained by drivers such as advertisement, availability, training. We should also remember that we can lose customers due to disinterest, churn, competition; we can lose employees due to dissatisfaction. This reduces our stocks by out-flows \\(\\psi_{out}(t)\\).11 The flows are measured in units per time interval, e.g. new acquired customers per month.\n11 I know all of this sounds tedious, but it’s the only way to rigorously reason about performance: stocks and flows!Our first equation tells us that the current level of resources (vector R) is given by the cumulative net flows over the whole history. It might take a long time and investment to accumulate resources needed to achieve the desired performance. This idea gives us a straightforward interpretation for the barriers to entry.\n\\[\\begin{align}\n\\mathbf{R}(t) & = \\int_0^t r_i(t) dt + \\mathbf{R}(0) \\\\\n              & = \\int_0^t [ \\psi_{in}(t) - \\psi_{out}(t) ] dt + \\mathbf{R}(0)\n\\end{align}\\]\nThe second equation “simply” tells us that the current level of performance depends on the resources, our decisions, and exogenous factors. We’ll have to do the hard work of translating our business understanding into the mathematical relation, i.e. define \\(f\\).\n\\[\n\\pi(t) = f(\\mathbf{R}(t), D(t), E(t))\n\\]\nAt last, the rate is a function of available resources, decisions, and exogenous factors. Notice how (1) and (3) allow for reinforcing and balancing feedback loops. Kim Warren’s paper goes at great lengths to define those and give practical, ubiquitous examples. We’ll come back to the topic when doing simulations.\n\\[\nr_i(t) = g(\\mathbf{R}(t), D(t), E(t))\n\\]\nAs an example of reinforcing feedback, think of the word-of-mouth growth of a shop: some of our happy customers recommend it to their friends, which in turn could become our customers. These loops influence the in-flows and are not always this happy – think of the debt deflation during the Great Depression of 1929. Another common example is of a shrinking business due to cost cutting, which leads to the inability to meet and sustain demand.12\n12 In my opinion, many businessed today are run in a very short-sighted way, with a short-term, ruthless focus on financial outcomes and maximizing shareholder value. I’m not saying this from a moral point of view, but for their own good and long-term, sustained performanceIn contrast, if we have a neighborhood restaurant, the number of remaining potential customers will drop with each new regular. Moreover, if it is so successful as to have large waiting times, we’ll hit a fundamental constraint of capacity, meaning the waiting times will limit the number of customers per week. These are balancing loops, fundamentally concerned with out-flows.\n\n\n\n\n\n\nHow do I do this stuff in practice?\n\n\n\nFirst, by drawing the stock-and-flow diagrams, the relevant feedback loops, defining your assumptions, functional relationships, and parameters to tweak. We’ll have to collect data to estimate the unknown quantities and to have a ground-truth of performance. At last, we simulate and explore different scenarios – which should give us insight into how “the system” works and what should we do.\n\n\nI know this is not easy to do well in practice and no wonder that systems’ dynamics is almost not used at all in businesses. Still, they are missing out on a powerful tool! But know that at the very least, this way of thinking will contribute to a much better diagnosis and strategy than if you weren’t aware of it. You could capture the nonlinearities and the dynamic complexity of the business, something which a rule-of-thumb, 3-scenario of profitability excel sheet can’t ever dream to do.\n\n\n\n\n\n\nCausality and process models\n\n\n\nPhysicists, biologists, and chemists develop mathematical models which result from their hypotheses and theory. They design experiments and statistical models in oder to see if those hypotheses are plausible.\nThe models which describe real-world causal mechanisms are also called process models, as they take a vague hypothesis and make it concrete and testable. In the most happy case, the statistical models and their parameters are identifiable (i.e. uniquely correspond to that theoretical model).\nIn social science, we can’t even hope for such results. But if you take this analogy far enough, the equations resulting from stock-and-flow diagrams are our causal, process models. They will fail to capture the full complexity, but they should be useful in decision-making and strategic planning!\n\n\n\n\nStrategic alignment\nSo, you found out this new (but old) way of thinking and modeling a firm’s performance in a competitive market and VUCA environment – but what is strategy, after all? Following Richard Rumelt’s argument in “The perils of bad strategy”, it is easier to define what it’s not.13\n13 I hope when you see fluff strategy, you recognize it and call it out as such!In short, it is not just aspiration towards a goal or having a vision or setting a target, and hell no, it’s not a wishlist. By analogy, remember the SMART criteria when you’re setting goals. As true consultants, we can summarize the steps involved in developing a strategy in a matrix.\n\n\n\n\n\n\n\n\nStep\nOutcome\nCharacteristics\n\n\n\n\nHonest diagnosis\nIdentify obstacles\nFew critical, relevant challenges\n\n\nGuiding policy\nGeneral approach to overcome challenges\nFocused on key aspects\n\n\nCoherent actions\nSupport policy with action plan\nCoordinated and focused\n\n\n\nSince strategy informs so much of decision-making, know your firm’s strategy – ask around, understand it, contribute to it. Call out bad strategy, especially when it tries to deal with everything all at once.\nFollowing the strategic resources argument, we can view a team which can operationalize machine learning systems, statistical models, and optimize processes at a large scale as an important capability and competence to develop inside a firm.\n\n\n\n\n\n\nThe value chain and data science strategy\n\n\n\nAs the firm (hopefully) has a strategy, a “data science” team should also have one, in order to know what work to prioritize and where can they bring the most value. So, our next challenge is how can we align business objectives with data science use-cases.14\nThis is where we have to think about the value chain of the firm, how it adds value from customer acquisition, engagement, to manufacturing and sourcing of materials. For each function of the value chain, we have to define the critical KPIs which assess its performance, derive use-cases which attempt to improve and are aligned to those metrics.\nIn turn, for each use-case, there are critical factors influencing the use-case and data points which serve as proxies or measures of the drivers. We’ll collect the data from the appropriate (software) systems which generate it and build pipelines to transform it in a clean, usable format on schedule or whenever we need it.\n\n\n14 I highly recommend the article published on the blog “bayesian quest”, named “Data Science Strategy Safari”. It saved me tons of headache.Notice how our approach to learning, strategy, and problem-solving is systematic, starting from first principles. At the same time, having a methodology doesn’t mean we’re following a mechanical process, a rigid procedure – as you see, there is plenty of space for creativity.\n\n\n\n\n\n\nCase-Study: LRB Subscriptions\n\n\n\nTake a subscription-based publication like London Review of Books and analyze it using the tools and frameworks you learned.\nThink about acquisition, churn, printing, transportation, and market share. How can it grow? What would make it profitable? What are the key decisions to be made?",
    "crumbs": [
      "Introduction and Background",
      "iii. Business context"
    ]
  },
  {
    "objectID": "business_context.html#the-value-of-methodology",
    "href": "business_context.html#the-value-of-methodology",
    "title": "Business context",
    "section": "The value of methodology",
    "text": "The value of methodology\nAn essential aid in this pursuit of improving decision-making at scale, are different processes, workflows, and methodologies for machine learning, experiment design, causal inference, and exploratory data analysis.15 It is important to mention that methodology is not a recipe, but way of structuring our work, a set of guiding principles and constraints over the space of all the things that we could do in analysis. Don’t think of these constraints as limiting your freedom, but as helpful allies in effective problem problem solving.\n15 We’ll discuss it in detail in the next chapterThese methodolgical fundamentals are not “just theory”, it is what will make or break projects in practice. There are so many pitfalls in ML and statistics that we cannot afford to do it ad-hoc. In my teaching, I try very hard to bring back the scientific process and methodology into decision science, as understanding and applying a statistical tool or model by itself is not sufficient.\n\n\n\n\n\n\nWhy most AI/ML projects fail?\n\n\n\nA lot of attention has been paid in the industry to the software engineering aspects of machine learning as one of the causes of failed projects. In the last 5 years, a lot of tools and best practices have emerged which can help us mitigate those risks and operationalize successfuly.\nIn my opinion, a bigger problem is methodological and organizational: a mismatch between business objectives, actions, constraints, tradeoffs, domain specificities vs modeling. This is why the ability to ask good questions, to formulate a problem, and think scientifically is critical. Therefore, an important goal of my writing is to recognize and avoid adhockery. 16\nThere is one more problem you have to be aware of, which I call the “Kaggle phenomenon”. Despite Kaggle competitions being an excellent platform for honing ML skills and developing better models, I think it gave too many people the impression that this is what data science is about. Kaggle misses the most important aspects of problem-solving and it is devoid of most of the original business context and decision-making.\n\n\n16 Adhockery in modeling, in product-management, in data-mining, and software engineeringSo, we’re moving from a strategic level of analysis to individual use-cases, where we have to think like a product manager, if we want our solution to be useful. For this, I found the Event Storming workshops from Domain-Driven Design and Google’s People+AI Research (PAIR) workbook really helpful.\n\n\nI have also written a guide about how to think about GenAI use-cases. “Fundamentals of GenAI governance”: A conceptual framework for AI product managers\nPerhaps, you won’t need to worry about the firm’s strategy and will have a really good product manager driving the project, so you can focus on modeling and data analysis. However, this is more of an exception than the rule in most businesses – which means that you have to be aware of some product management and consultancy techniques to get by. These skills become tremendously important when you transition into a management and leadership role, be it in business or tech.",
    "crumbs": [
      "Introduction and Background",
      "iii. Business context"
    ]
  },
  {
    "objectID": "business_context.html#applications",
    "href": "business_context.html#applications",
    "title": "Business context",
    "section": "Applications",
    "text": "Applications\nIn the course introduction and the first part of this chapter, you saw that ML and statistical modeling has applications in most industries and domains. I’ll give more examples here, highlighting what tools we’re going to study are helpful in tackling these problems. I’ll remind you that we’re talking about systems and models for decision-making under uncertainty, and sometimes, these decisions are at a large scale (for which you would normally need an army of employees).\nIn pharma, we need to rigorously design randomized control trials and take into account the dropout, censoring, and other threats to the validity of the study. These studies are very expensive, highly regulated, which limits sample sizes and the kind of interventions allowed. Moreover, it is very difficult to assess rare side-effects and the long-term consequences of drugs and treatments.17\n17 This is why a good statistician is essential to pharmaceutical firms and research labsIn market research, the question is what is the potential market size, consumer interests, perceptions, preferences, and patterns of behavior. This helps firms decide whether to enter into a market, what products to develop, and where they stand with respect to competition.\nIn political science, as in market research, surveys and self-report is the main quantitative research method used – which poses challenges of non-representative samples, non-response, measurement error, and the mismatch between self-report and actual behavior. Statistical approaches like multilevel modeling with post-stratification are essential tools to deal with these challenges.\nMoving on to sports science, Liverpool F.C. won a title and a key part of their success was leveraging AI and ML to discover new tactis on the field with the highest payoffs. NBA teams invested a lot in the data infrastructure and decision-making capabilities: LA Lakers found the best player for a particular position they were lacking. Houston Rockets won the regular season by going all in on the 3-point shot.18 Golden State Warriors simply revolutionised basketball with data, before everyone else was doing it – giving them a competitive edge. 19\n18 Moreyball: The Houston Rockets and Analytics – an article in Harvard’s MBA Digital Innovation19 Check out the following video by The Economist on how data transformed the NBA. For more details on the statistical methodology, I enjoyed an youtube channel called Thinking Basketball and their playlist about the statistical methodology.A classical application in automotive industry is predictive maintenance, where we can use time-to-event modeling to replace risky parts before they go out of function. In manufacturing, we can leverage what we learned about hypothesis testing and apply it for quality control.\n\nPricing and revenue management\nIn the case of Uber and ride sharing platforms, we’re not only concerned with matching passengers and drivers, allocating routes, and setting the right prices at scale – we’re talking about managing a whole competitive market (supply-side and demand-side). 20\n20 When it doesn’t work out – I’m pretty upset at their data scientists and domain experts. Here is where ethical issues creep up: jacking up prices, monopolies, drivers struggling to make a living wage.Thus, a dynamic pricing solution should capture the causal patterns and uncertainty in demand, which is driven by weather, seasonality, competition, and exogenous events. Because they are a platform and market-maker, machine learning models for demand forecasting won’t be enough, especially when quantifying the effects of pricing decisions – so they must take into account the economic theory.\nIt is possible that they need to do lots of experiments in order to collect the necessary data to train and estimate those models. At last, even when having reliable predictions of the consequences of pricing decisions, they’re left up with a large allocation and optimization problem, as there are only so many drivers.\nWhen optimizing prices under limited capacity of airplane seats, concert tickets, hotel rooms – we need econometric and ML models for demand forecasting and sophisticated optimization algorithms, including dynamic programming. This new field of revenue management has a rich research literature and sophisticated proprietary solutions.\n\n\nDemand Planning\nDemand forecasting and inventory optimization is needed any time there is uncertainty in demand and we have to decide how much to produce, purchase, or capacities to allocate.21 Things get much more complicated as the product catalog grows, as products are distributed in multiple places (like stores) and channels.\n21 In retail, fashion, apparel, e-commerce, hospitality and recreation, pharma and healthcare, automotive, etcMoreover, errors in forecasting propagate and amplify through the supply chain, making the sourcing and manufacturing decisions harder if there is no end-to-end sharing of information.\nIn the paper “Newsvendors Tackle the Newsvendor Problem”, Koschat, Berk et. al. show how an analysis and optimization of printing decisions led to TIME INC. to revise its policies and generate additional \\(\\$3.5m\\) in profit. I will present this problem in the context of simulation and statistical modeling lectures, and we’ll apply it to Yaz restaurant data.\nDespite the simplicity of formulating the newsvendor problem, one can go into a lot of depth both on forecasting and optimization, as shown in a meta-analysis of data-driven approaches to it, by S. Buttler, A. Philippi. Generally, machine learning models like LightGBM with thoughtful feature engineering (including promotions, availability, and other demand drivers) work very well for demand forecasting, but not always.22 Sometimes, a top-down forecast or traditional time-series methods make more sense.\n22 In the meantime, Conformal Prediction has emerged as an important technique to quantify the uncertainty in forecasts of ML models, but there are also other ways based on simulationIf you have a demand planning use-case at work, I highly recommend Nicolas Vandeput’s books, articles, and lectures on best practices in demand planning and I. Svetunkov’s Lancaster University CMAF seminars.\n\n\nChurn and Lifetime Value\nBesides acquisition, customer retention and repurchase is what makes or breaks businesses, especially in the past few years, with so many subscription and SAAS products. Therefore, we need a set of methods to predict customer churn and lifetime value in contractual and non-contractual settings. At a first glance, these might look like a simple classification problem which is easily solved with a ML model, but things are more complicated.\nFirst, we might need some tools from survival analysis, as we care not only about the probability of churn in the next month, but the expected time until churn.23 Second, we really want to know who can be convinced to remain or purchase again (via an intervention like promotions or loyalty program).\n23 Survival analysis answers the question of what is the probability a customer remains for a given time horizon, given they are active until nowThere is a body of work spanning two decades by Bruce Hardie and Peter Fader, who developed statistical models that estimate the remaining LTV (Lifetime Value) of customers, which depends on their survival, activity, and repurchase patterns. These “Buy till you die” models work very well at the level of a customer base, which is useful in financial planning and forecasting.",
    "crumbs": [
      "Introduction and Background",
      "iii. Business context"
    ]
  },
  {
    "objectID": "sci_process.html",
    "href": "sci_process.html",
    "title": "Scientific process is all you need",
    "section": "",
    "text": "People doing analytics, machine learning, and applied statistics recognize the importance of methodology. They design processes and workflows for their domain of expertise, which helps them solve problems systematically. However, it is confusing for a beginner to see so many of them and decide which one to use for each particular application, as I show in the card below.\nI claim that an adaptation of the scientific process which includes agency, can place each of these processes in its appropriate context and take into account actions and outcomes\nBefore I explain each box on the diagram1 and how are they connected, I’ll show some processes that you might encounter in literature and practice. Also, think how the scientific process you’re probably familiar with is encompassed by the diagram above: Observations – Hypotheses – Experiment – Data collection – Hypothesis testing – Communicating conclusions.\nOut of all these processes, I found C. Kozyrkov’s 12 steps to AI to be extremely useful in predictive applications, 12 steps to statistics in designing A/B tests, and R. McElreath’s process when developing statistical models to draw causal conclusions.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#action-and-agent-arena",
    "href": "sci_process.html#action-and-agent-arena",
    "title": "Scientific process is all you need",
    "section": "Action and Agent-Arena",
    "text": "Action and Agent-Arena\nFirst, let’s take the exmple of a firm and think of it as an agent. Hopefully, it has formulated its mission, vision, a strategy to overcome obstacles, and has set reasonable objectives. Employees and leadership of the firm will try to make good decisions and develop capabilities in order to improve performance. At this level of analysis, we don’t need to consider the full implications of a complex adaptive system.\nIn a happy and lucky case, these actions are coordinated, aligned with strategy and generate desired outcomes. Besides performance, we can also consider an increase in value added brought by improvements in its processes, products, and services.\nThe firm gathers data about its current and past state \\(\\{\\mathcal{S}_t\\}_{1:t}\\), observes the environment \\(\\mathbfcal{O}_t\\), its actions \\(\\mathcal{A}_t\\) influence the environment, and vice-versa. It also interacts with other agents in the network, be it customers, suppliers, manufacturers, government, stakeholders, etc.\n\n\nThis perspective is pretty standard in AI research. We can use it to tease out different sources of uncertainty.\nI called this element in the scientific process an “agent-arena” relation, because we usually don’t consider the whole firm, but our framing depends on what point of view in the value chain we take. For example, a team responsible for conversion-rate optimization will have its own objectives, decisions, and data they care about \\((\\mathcal{S}, \\mathcal{O}, \\mathcal{A} | {POW} )\\). Of course, there is a concern of alignment, of local optimization which results in suboptimal global outcomes – but this topic is outside the scope of our course.\n\n\n\n\n\n\nSources of uncertainty\n\n\n\nIn their book Algorithms for Decision Making, MIT Press - 2022, M. Kochenderfer, T. Wheeler, and K. Wray make a helpful classification of the sources of uncertainty, based on agent-environment interactions:\n\nOutcome uncertainty suggests we can’t know the consequences and effects of our actions with certainty. We can’t take everything into account when making a decision\nModel uncertainty implies we can’t be sure that our understanding, assumptions, and chosen model are correct. In decision-making, we often misframe problems and in statistics, well, choose the wrong model.\nState uncertainty means that the true state of the environment is uncertain, as everything changes and is in flux. This is why statisticians argue that we always work with samples\nInteraction uncertainty due to the behavior of the other agents interacting in the environment. For example, competitive firms, and social network effects.\n\nWe will focus very little the last aspect of uncertainty, but you have some tools to reason about it: game-theoretic arguments, ideas from multi-agent systems, and graph theory. I think it is an important dimension of reality, however, taking it into account in this course would make it incomparably more complicated and advanced.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#testing-ideas-and-solutions",
    "href": "sci_process.html#testing-ideas-and-solutions",
    "title": "Scientific process is all you need",
    "section": "Testing ideas and solutions",
    "text": "Testing ideas and solutions\nAt the core of scientific process is experimentation, systematic observation, and testing of ideas – which results in evidence with respect to a hypothesis or theoretical prediction. For our purposes, we consider experimentation in its large sense, not just randomized control trials and highly-controlled experiments.\nBy including statistical models, simulations, ML models; prototypes, software systems and features – we can benefit from this self-correcting feedback loop not just in science, but in decision-making. For example, leaving aside a test dataset in machine learning is a way to assess and approximate how well a model generalizes beyond training data.\n\n\nLet’s consider Agile principles and Shape-Up methodology for software development. I think that they fit well into our framework when we’ll sketch out the two remaining pieces\nThis means that inferential and causal statistical models are equally as important ways to test ideas. Moreover, the ultimate test is whether the recommended decisions based on modeling insights work well in practice. Meaning, we get better outcomes in the agent-arena, our problem space. Let’s see how testing of ideas interacts with other elements in our diagram:\n\nThe insights and recommendations from our experiments inform actions in Agent-Arena. After operationalization, we get feedback on how well it worked and what can we improve\nWe have to communicate our insights clearly and persuasively to the people we collaborate with. Our colleague’s expertise will give us best-practices in the field and feedback about the analysis\nWith respect to discovery and explanation, we don’t want to test a vague idea, but a well-thought, sharp hypothesis or causal model\n\nThe tests will result in stronger or weaker evidence for or against the explanation. We can use simulation to validate that our statistical models work in principle, meaning we recover the estimand (unobservable quantity or effect of interest).\nSimulating synthetic data from a causal process model in order to validate if a statistical model can draw correct inferences about the estimand can be considered an experiment, although, one which lives in pure theory, until we apply it to real data. This iterative process lives at the intersection of asking questions, building theories and testing ideas.\nSince the data comes from messy real world systems, we will have to consider very carefully how it was collected and measured. This means that when we build models, we might need to learn new tools to correct for biased samples, differential non-response, missing data, measurement error, repeated observations, etc.\n\n\nFor some research questions, measurement and sampling is a complicated problem. This is why I’m trying to integrate them in this course.\n\n\n\n\n\n\nThe value of research methods\n\n\n\nQuantitative research methods are a valuable when dealing with the issues outlined above, but are often not part of a statistical curriculum. I highly recommend A. Zand Scholten’s introductory course on Quantitative Methods. It is worth to invest a bit of time to get familiar with the following:\n\nMeasurement is especially relevant and tricky in social science. Sometimes, we do not measure what we think we do. For measuring abstract things like happiness, preferences, or personality – we need to dive into scales, instrumentation, operationalization, validity, and reliability\nSampling isn’t limited to simple random samples, as these rarely occur in practice. We need to be aware what tools exist so that we can generalize to the population of interest\nThreats to validity, including confounds, artifacts, and biases. On the one hand we use statistics as a guardrail against foolishness, on the other hand there a lot of the same problems that can compromise our study.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#discovery-and-explanations",
    "href": "sci_process.html#discovery-and-explanations",
    "title": "Scientific process is all you need",
    "section": "Discovery and explanations",
    "text": "Discovery and explanations\nOne can make a case that wisdom starts in wonder and science with curiosity. Don’t get fooled by statistics classes were hypotheses and theories are taken for granted. Our field has caught up to scientists, in the sense of openness in how we gather inspiration.3\n3 Many books have been written about how theories are being developed, the role of curiosity, inspiration, observation, previous theories, and evidenceModel-driven exploratory data analysis an pattern recognition have become acceptable ways to formulate hypotheses. I would argue that for most problems I encountered, this exploratory process was essential. This is precisely the way in which a good analyst is extremely valuable. Note that with this power comes a lot of responsibility to not fall for confirmation bias, hence we’ll need discipline and guardrails.\nYou will probably agree that asking a good question and formulating / framing a problem well gets us “halfway” towards a solution. We can approach a research design data-first or question-first, but in business practice it’s almost always a mix.\n\n\n\n\n\n\nPICOT criteria for a research question\n\n\n\nWhen starting question-first, it’s helpful to think what is the target population of interest, potential interventions, whether we have a comparison group, what are the relevant outcomes, and the time-frame of the study.\n\n\nSpeaking about business decisions, what I mean by theory is our current knowledge and understanding of the problem / domain.4 Don’t get fooled by claims that AI and ML are exempt of theory, assumptions, and will figure out those from data. The mere fact of selecting what to measure and what features to include in the model, involves human understanding and a judgement of relevance.\n4 For example, customer preferences and behavior, drivers of the demand, and sourcing of raw materialsIn the course we will try our best to ensure that statistical models are answering the right quantitative question, which we call an estimand. It might not be obvious why we worry about the underlying, unobservable, theoretical constructs and ways we could measure them. After all, we’re not doing quantitative psychology.\nIn my experience, these constructs appear implicitly, for example: willingness to pay, customer satisfaction, engagement, brand loyalty and awareness, who can be convinced to buy via promotions, email fatigure, etc. Hence, the decisions we make in operationalizing these abstract concepts are critical to a project’s success.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#steering-and-collaboration",
    "href": "sci_process.html#steering-and-collaboration",
    "title": "Scientific process is all you need",
    "section": "Steering and collaboration",
    "text": "Steering and collaboration\nModern research and science in general do not happen in isolation. The same is true in firms and almost any other domain. I will not spill much ink in trying to convince you of the power of collaboration, since almost any problem which is complex enough can’t be tackled by a single individual.5\n5 This argument is developed much further in C. Hidalgo’s book “Why information grows” and his research into measures of economic complexityIn the context of this course, we’re thinking of collaboration with clients, stakeholders, experts, engineers, and decision-makers. A few outcomes of this collaboration are the strategy we mentioned at the beginning, policies, interventions, best-practices, feedback, and review. Collaboration informs what should we do, prioritize, build and test, and hypothesise / think about.\nI can’t emphasize enough the importance of developing your writing skills. It is both a way of thinking, problem-solving, and communicating ideas. When it comes to academic writing, the structure of a scientific paper is very helpful even in businesses, as it forces you to articulate the relevance of your research, results, and contribution.\nLast, but not least, reproducible research and literate programming has become a de-facto standard in the data science world. It is a very important aspect to keep in mind, as your work should be easily checked and reproduced by others if you expect a contribution from them. We have amazing tools for that right now, but it’s still not easy to achieve end-to-end reproducibility and automation.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "sci_process.html#particular-processes",
    "href": "sci_process.html#particular-processes",
    "title": "Scientific process is all you need",
    "section": "Particular processes",
    "text": "Particular processes\nNow that you have this very general version of the scientific process in your conceptual toolbox, we can look at some particular workflows, see how they map onto it, and what can we learn from them.\nI really like the idea of “Business analyst’s workflow” from A. Fleischhacker 6, which served as a major inspiration for this chapter. The normative criteria for such a workflow are very well aligned with our “project”: it should focus on outcomes, be strategically-aligned, action-oriented, and computationally rigorous.\n6 A Business Analyst’s Introduction to Business Analytics (2nd ed)\n“You will translate real-world scenarios into both mathematical and computational representations that yield actionable insight. You will then take that insight back to the real-world to persuade stakeholders to alter and improve their real-world decisions.”\n\n\n\n\nSource: causact.com; “(The workflow) starts with strategy, expertise, and data as inputs. By modeling, the business analyst gains insight and persuades the firm to act on it to improve outcomes”\n\n\n\n12 Steps to Applied AI\nMoving on to machine learning and predictive applications, out of all the workflows, I prefer C. Kozyrkov’s “12 steps to applied AI”. It encompasses all others and gives a set of concrete steps when defining and planning for a ML use-case.7 This short reading will dramatically increase your chances to make your predictive use-case successful.\n7 Cassie Kozyrkov has a set of lectures for complete beginners which I highly recommend: “Making friends with machine learning”.First, we need to make sure that ML is the right approach for the problem and that our project is feasible: is there a pattern and do we have relevant data? Then, we need to define our objectives, decisions, constraints and assess how costly is each type of mistaken prediction.\nIn the second phase, we need to collect data and leave aside a test dataset, so we can check if our model generalizes before we launch it in production. Otherwise, we won’t know if the model is reliable on unseen data. Next, explore the data, clean it up, choose the models you’re going to try, and train them. You might need to debug, iterate, and tune your model pipeline to solve the problems which will inevitably arise.\nIn the third phase, you have to validate your models, pick the most promising one and test it on the pristine test dataset which was left aside. If the performance is good enough and the model survived our critiques and ways to break it, we’re ready to operationalize and build it for production. Often, we’ll have to run live tests and randomized experiments in order to see if it actually works in practice and it’s safe to launch. At last, we’ll have to monitor its performance, fix bugs, re-train, and maintain the system.\nComing back to our scientific process, you see how these 12 steps have a component of problem formulation, exploration, experimentation, and testing in production. The latter is a ultimate test if our idea (ML use-case) works. We’ll have to improve it based on the feedback we get, then test again.\n\n\n12 Steps of Statistics\nIn order to show how this workflow is applied in experiment design, think of an A/B test for improving conversion-rate on a website, a randomized trial which evaluates a new treatment, a quality control sample, or a survey for the next parlamentary election. All of them try to answer a research question.8\n8 The relationship between the scientific process and statistical methods has been well explored and establishedBefore jumping into hypothesis testing, we should ask whether we need an experiment at all. Maybe we want to explore, predict, or we already have access to the whole population of interest (e.g. our customer base)? Or maybe we’re limited to work with observational data and self-reports due to experiments being unethical or unfeasible.\nIn the frequentist, Neyman-Pearson school of statistical thought, we have to first define our default action, i.e. what would we do in the absence of evidence. When it comes to knowledge, hypotheses, or beliefs – think of acting as if that claim was true. Then, statistics can tell us how to change our actions in the face of evidence, so we’re not wrong too often in the long run. Really strong evidece resulting from a well-designed study and modeling will make our default action ridiculous and unreasonable.\nOf course, we have to choose our target population of interest, as our conclusions do not apply beyond it without a strong justification. We also have to define our causal and real-world assumptions and a really good way to do that is by drawing causal diagrams (DAGs). This is where we’ll have to worry about potential threats to validity.\nIn operationalization, we’ll have to make our research question really sharp and specific, define procedures of how are we going to measure the observable quantities and outcomes of interest, choose the appropriate interventions. As a part of research design, we have to define our data collection strategy and procedures, for which we have to account in the modeling stage.\n\n\n\n\n\n\nMeasurement and threats to validity\n\n\n\nR. Crump gives an excellent overview of the threats to validity in Chapter 1 of his “Answering questions with data”. Here are some good readings regarding threats to validity in A/B testing:\n\n“Simple and Complete Guide to A/B Testing” by K. Tatev\n“Avoid the pitfalls of A/B testing” by I. Bojinov\n“8 common pitfalls of running A/B tests” and “An A/B Test Loses Its Luster If A/A Tests Fail” by L. Ye\nOn user interference by S. Kumar\n\nOne of the most difficult aspects in practice is measurement and metric design. A useful way to think about properties of good metrics is described in this paper by S. Gupta and W. Machmouchi. The STEDII acronym stands for sensitivity, trustworthiness, efficiency, debuggability, interpretability (actionability), and inclusivity (fairness). Here’s another article by A. Rustgi which discusses most common mistakes.\n\n\nThe null and alternative hypotheses tell under what conditions we’ll choose the default or alternative action. At this point, we have all the information needed for us to choose or build the appropriate model or method and validate it on a simulation study. This exercise will tell us if our approach works in principle and in what ways could it fail, especially when our assumptions don’t hold.9\n9 It is quite common that our statistical models don’t answer the question we think they do. This is due to poor design and is often jokingly referred to as a “type 3 error” or “the art of solving the wrong problem”In power analysis, we’ll define our type 1 and type 2 errors, the minimum effect sizes of interest, and figure out how many observations we need for our study to answer the research question.\nNotice how much preparation we have to do before we actually collect the data, analyze it, and apply a model or statistical test. This is not because I want rigor for rigor’s sake, but missing a single aspect outlined above in preparation can render our expensive study useless.\nFinally, we can start collecting data, launch the experiment and apply the statistical model we defined before. If we did everything right, there is not much work left to do, besides loading and conforming the data to the specification, reporting and communicating our findings.",
    "crumbs": [
      "Introduction and Background",
      "iv. On scientific process"
    ]
  },
  {
    "objectID": "appendix/open_datasets.html",
    "href": "appendix/open_datasets.html",
    "title": "Open Datasets",
    "section": "",
    "text": "Overview of open datasets\n\n\n\nIt is very hard to find good, realistic datasets which map well on representative use-cases from this course. This is why I curated a list of public datasets in a variety of domains. These should help if you don’t know where to start your project, that is don’t have particular problems, hypotheses, or research questions in mind."
  },
  {
    "objectID": "appendix/open_datasets.html#a-selection-of-realistic-datasets",
    "href": "appendix/open_datasets.html#a-selection-of-realistic-datasets",
    "title": "Open Datasets",
    "section": "A selection of realistic datasets",
    "text": "A selection of realistic datasets\nDeciding what topic, domain, and problem to choose for your project is a daunting task in itself. During the course, I present new tools to formulate good questions and how to design your research study. Unfortunately, in most cases, we will be limited by what open data is available. We encounter the same problem of “too much content”: there is so much data to work with, but most of it isn’t any good for our purposes.\n\n\nYou can rely entirely on simulation, but the way you desing the underlying data-generating process has to be very well thought out, informed by the specificities of the problem and an expectation of what data we’ll encounter in practice.\nFinding good data about business challenges is incredibly hard, since few firms will be open to sharing it. Even when we get our hands on a good kaggle competition, the problem is that the data has been already framed, curated, and put together for us. This means that we don’t get the real experience of end-to-end problem solving that we would encounter in practice. Therefore, I collected a list of datasets which are realistic and diverse enough. You can consider them as an inspiration and starting point.\n\n\n\n\n\nDataset name\nDomain\nProblem / Area\nComments\n\n\n\n\n1\nYaz restaurant demand\nbusiness\nNewsvendor problem\nA real dataset used for benchmarking inventory optimization algorithms from a restaurant in Stuttgart. Source: ddop\n\n\n2\nMercari vintage clothes marketplace\nbusiness\nPrice suggestion\nAn excellent dataset for GLMs and machine learning, where the text data is important. Source: kaggle / Mercari\n\n\n3\nAvocado prices and volumes from Hass board\nagriculture\nMarket research\nThis is a good opportunity to understand the dynamics of a whole industry. Source: kaggle / hass\n\n\n4\nOlist e-commerce\nbusiness\nEDA, databases\nA very rare example of freely available data published as a relational database. Good for open-ended projects. Source: kaggle\n\n\n5\nCorporacion favorita\nbusiness\nDemand planning\nOne of the best datasets to practice demand forecasting for groceries. Source: kaggle\n\n\n6\nTevec retail sales\nbusiness\nInventory management\nGood for short-term demand forecasting for the purposes of inventory optimization. Source: kaggle\n\n\n7\nLending club loans\nfinance\nCredit risk\nA large dataset of peer-to peer, graded loans, with data about clients, loan, and interest. Source: kaggle\n\n\n8\nDataCo orders\nbusiness\nLogistics and fulfillment\nOne of very few datasets for you to get a better grasp over outbound and inbound logistics. Source: kaggle\n\n\n9\nCriteo Campaign\nbusiness\nMarketing\nThe biggest dataset of randomized control trial marketing campaign for Uplift modeling. Source: kaggle\n\n\n10\nEase my trip\nairlines\nPrice prediction\nThis dataset has a few gotchas related to how the flights were selected and unavailability of seats remaining. Source: kaggle\n\n\n11\nAmazon reviews for beauty products\nbusiness\nNLP, Customer analytics\nAnalyzing customer reviews and feedback is a widespread use-case in businesses. Lots of data is available. Source: nijianmo\n\n\n12\nExpresso churn prediction\ntelekom\nChurn\nWe will discuss the challenges around modeling customer churn, especially survival models and interventions. Source: kaggle\n\n\n13\nSantader customer satisfaction\nbanking\nChurn\nSince the data is anonymized and mostly numeric, we would have to take a ML approach. Source: kaggle\n\n\n14\nSupply chain allocation\nbusiness\nLogistics and shipments\nAnother rare dataset on logistics, in which you assign routes to purchase orders from manufacturers. Source: kaggle\n\n\n15\nHospital customer satisfaction\nhealthcare\nCustomer analytics\nA pretty large-scale, general-purpose survey on client satisfaction. A lot of EDA and data cleaning is needed. Source: kaggle\n\n\n16\nBike sharing demand\ntransportation\nDemand planning\nSeveral datasets from different cities and ride sharing firms: Washington, Boston, London\n\n\n17\nNYC subway rides\ntransportation\nDemand planning\nAnother aspect of demand planning is load prediction and minimizing delays: NYC traffic, Toronto subway delays, NYC entry and exits\n\n\n18\nTaxi trips\ntransportation\nDemand planning, pricing\nMultiple datasets from different firms and cities: Chicago, NYC taxis, NYC uber\n\n\n\n\n\nThe curated list of datasets doesn’t appear in most books and resources that I recommend. The challenges they present are not easy and require quite a lot of work. Some of them will also teach you how to work with larger amounts of data. That said, there is still a lot of value in the didactic examples, so here are a few more directions I recommend to look into:\n\nP. Fader, B. Hardie, and E. Ascarza research on BTYD (buy till you die) models of customer repurchase behavior, churn, and LTV\nThis website on Bayesian networks has a few amazing case-studies on self-worth and depression, which are perfect for practicing causal thinking\nFacebook has synthesised the recent research in Marketing Mix Modeling into their open-source project called Robyn.\nRohan Alexander has a good example of multilevel modeling with post-stratification on US 2020 elections. Andrew Gelman in “Regression and other stories” has lots of great examples from political and social science.\n\n\n\n\n\n\n\nCase-Study: DuckDB and e-commerce database\n\n\n\nI use a relational dataset from a real e-commerce which was made public in Kaggle, in order to showcase how to interact with databases and highlight the importance of knowing SQL.\nWe use a recent innovation in databases, duckDB, which allows us to have an analytic, in-process database, with almost zero setup or dependencies. Once the e-commerce data is loaded and modeled inside the DB, we can start cleaning it, putting it all together, and extracting insights from it. This particular dataset is very rich in information and lends itself well to open-ended investigations."
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Modern Decision Science",
    "section": "",
    "text": "At my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale. I really believe in the idea of “skin in the game”\n\n\n\n\n\nMihai Bizovi | VP of Decision Science @AdoreMe (acquired by Victoria’s Secret)\n\n\n\nResponsible for AI strategy, decision science, generative AI governance, and systems design. Enabling teams to be more effective and efficient\nHistorically had many hats – statistics, ML and data engineering, AI product management, developing data apps, teaching / mentoring\nCritical applications along the value chain:\n\ndemand forecasting and inventory optimization systems\nrecommender systems and NLP/LLMs for try-at-home businesses\nmarketing, acquisition, CRO experiment design, pricing, CRM"
  },
  {
    "objectID": "about_me.html#whoami-as-engineer-and-leader",
    "href": "about_me.html#whoami-as-engineer-and-leader",
    "title": "Modern Decision Science",
    "section": "",
    "text": "At my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale. I really believe in the idea of “skin in the game”\n\n\n\n\n\nMihai Bizovi | VP of Decision Science @AdoreMe (acquired by Victoria’s Secret)\n\n\n\nResponsible for AI strategy, decision science, generative AI governance, and systems design. Enabling teams to be more effective and efficient\nHistorically had many hats – statistics, ML and data engineering, AI product management, developing data apps, teaching / mentoring\nCritical applications along the value chain:\n\ndemand forecasting and inventory optimization systems\nrecommender systems and NLP/LLMs for try-at-home businesses\nmarketing, acquisition, CRO experiment design, pricing, CRM"
  },
  {
    "objectID": "about_me.html#whoami-as-teacher-and-researcher",
    "href": "about_me.html#whoami-as-teacher-and-researcher",
    "title": "Modern Decision Science",
    "section": "~whoami: as teacher and researcher",
    "text": "~whoami: as teacher and researcher\nAs a teacher, I aspire to contribute to the understanding of AI’s complex landscape; how to navigate it, develop valuable skills, and become more effective at problem-solving.\n\n\n\n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\nThink of me as a maven (yid: meyvn), i.e. someone experienced, who has been there before and will help you find a good path.\n\nGraduate of Cybernetics (BSc) and Quantitative Economics (MSc)\n\nThesis & Dissertation on Bayesian Microeconometrics\nResearch in Probabilistic Methods for Time Series\nStarted in Systems’ Dynamics and Economic Complexity\n\nSpeaking at conferences, meetups, and panels since 2018:\n\nBDW, DevFest, Google Cloud, CNCF, Romanian AI Days, DigitalShift\n\nTeaching at ASE’s business analytics masters since 2021"
  },
  {
    "objectID": "about_me.html#whoami-as-a-person",
    "href": "about_me.html#whoami-as-a-person",
    "title": "Modern Decision Science",
    "section": "~whoami: as a person",
    "text": "~whoami: as a person\nAs a person, I aspire to a life of wisdom and meaning, towards the good, true, and beautiful. Learning how to be present and deeply understanding reality.\n\nCultivating wisdom – commitment to philosophy as a way of life\nPainting as a craft, creative expression, and therapy. Learning about art history and art appreciation in multiple media (music, film, painting)\nReading about cognitive science, evolution, history, and economics\nHiking, hipster coffee, 14 years of pro-ish chess"
  },
  {
    "objectID": "about_me.html#how-does-it-fit-together",
    "href": "about_me.html#how-does-it-fit-together",
    "title": "Modern Decision Science",
    "section": "How does it fit together?",
    "text": "How does it fit together?\n\n\n\n\n\nsynoptic is to take the best of multiple perspectives, reduce conflict and contradiction – then see where the evidence converges\n\n\nOn another note, my interests in AI, art, cognitive science, and philosophy have an aspect of science, craft, worldview, and deep participation, engagement. Moreover, they are deeply interconnected and I would even say, synoptically integrated. It is important to me that these conceptual frames fit into a coherent whole and contribute in a practical way towards a good life.\nI can’t emphasise enough the importance of different levels of “knowing”: propositional, procedural, perspectival, and participatory – as it is not enough to know the facts (or have beliefs), but to know how to do something, to have a perspective of the “world” and a sense of participation in whatever you’re engaged in.\nI mean that we’re agents in different arenas of life and the sense of meaning comes from an attunement to those arenas. We participate in a course of something, which has impact on the environment, which changes us and how we view and relate to the world, self and others.\nUnfortunately, I have none of the answers of how does Intelligence, Rationality, Wisdom contribute to meaning in life – at best, some plausible hypotheses\nUnsurprisingly, there will be lots of painting metaphors when it comes to simplicity, and cognitive science references when talking about ways in which we’re biased and foolish. Chess, of course, inspires analogies of competition, strategy, and tactics to its service."
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "Course philosophy",
    "section": "",
    "text": "Decision science is by definition interdisciplinary, which makes studying it challenging, interesting, and rewarding. We will need to borrow ideas, models, and techniques from probability, statistics, causal inference, machine learning, optimization, economics, cognitive science, etc. In one sense, we’re fundamentally limited to quantitative questions in social science and applied problems, which places the field closer to engineering, than science.\nThis interdisciplinarity means jumping through many fields, which we’ll call “buckets” and we’ll resist the temptation to find and settle on THE tool or THE explanation. Any individual field, like machine learning, econometrics, or optimization is complex enough for an entire career of study and practice. The danger is, when the only tool you have is a hammer, everything starts looking like a nail.\nImagine we’re exploring a map by travelling through various islands: familiar or unknown places. Sometimes, we’ll take a little detour for a fun fact, other times we need to settle for a while and master a craft. Maybe, it’s not enough to talk to locals, but we have to live among them in order to understand their challenges. If we don’t have a map and guide, we might not be even aware that we’re missing important knowledge and understanding from an unexplored place.\nProblem space is the land of challenges. Our goal here is to understand the domain where we have to make decisions, figure out how to improve relevant outcomes for clients and stakeholders. There is much uncertainty here, questions about what will happen and how should we act. It’s the real world, seen as a Complex Adaptive System, which I will explain in the next section. This is where we get our data from and who we build software for.\nScience, especially cognitive science, which will give us insights about our intelligence, rationality, wisdom, foolishness, and biases. This is the place where we’ll get the process, method, learn how to systematically observe, formulate scientific hypotheses, use theories and causal models to make predictions and perform experiments.\nIn probability, we reason about uncertainty in the real world, build narratives and tell stories with DAGs (directed acyclic graphs) of random variables, which are little machines generating data. These stochastic models, when informed by theory and domain understanding, result in plausible patterns. We’ll spend a lot of time simulating economic processes, being the masters of these alternative, synthetic multiverses.\nIn statistics, we change our minds and actions in the face of evidence. By carefully thinking about how the data came to be, we can take into account potential problems in sampling, data collection, and measurement. On the one hand, statistical inference prevents us from getting fooled by randomness and enables quantification of how confident are we in our conclusions and predictions. Moreover, when paired with ideas from causal inference, it generates insight into the potential consequences of our interventions.2\nMachine learning and deep learning, the younger tribes of statistics, are the future: they learn from data and when things go well, make reliable and robust predictions, in order to optimize the heck out of any process. Think of them as shamans or oracles, who sometimes overfit by seeing patterns which are not real, therefore are prone to acting foolishly.\nWe come back to the homeland of many of you: computer science and software engineering, the place where nowadays everything on this map becomes reality. We will learn best practices of the guild, the principles of reproducible analysis, and how to build full-stack, data-driven software. While spending time here, we will develop an appreciation for the contribution of CS to all other places we already visited.\nAh! We forgot about mathematics. It is an essential prerequisite for everything we do, however, it is hard to do rigorous mathematics in the setup we outlined, as it will take a decade. The good news is, we will be fine with the starter pack!\nLast, but not least, do not underestimate philosophy. It will help us reason about the ethical aspects of AI and ML, teach us how to evaluate an argument, understand the limitations of our methods, the nature of evidence, and more importantly force us to make our assumptions explicit.",
    "crumbs": [
      "Introduction and Background",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#cybernetics-and-ai",
    "href": "philosophy.html#cybernetics-and-ai",
    "title": "Course philosophy",
    "section": "Cybernetics and AI",
    "text": "Cybernetics and AI\nWhat is cybernetics and how is it relevant to the course philosophy? Does it have something to do with robotics or cybersecurity? Not really. It is studying complex adaptive systems (CAS) and is very close to what we call today weak or specialized AI – recall the idea of “decision-making under uncertainty at scale” from the introduction.\nThe term comes from greek (kybernetes) – to steer towards a desired outcome or trajectory. It highlights the aspects of agency, action, pattern recognition, and prediction.\n\n\n\n\n\n\nUnpacking the definition of Cybernetics\n\n\n\nThe science of general regularities of control and information processing in animals, machines, and humans 3\n\nControl means goal-directedness, the ability to solve problems and achieve objectives by taking action and stirring the system towards a trajectory. The goal can also be perserving the structural-functional organization of the system itself, an autopoesis.\nInformation processing can be interpreted as pattern recoginition, perception, how you understand and model the world, what inferences do you draw based on “data inputs”\nGeneral regularities means what is true and plausible of control and information processing across fields and a variety of complex systems, not only in particular cases.\nAnimal refers to applications in biology, machine – in engineering, and human – in our society\n\nWhen applied to economics, cybernetics is ultimately concerned with human behavior. It views consumers, households, firms, institutions, markets, and countries as complex adaptive systems.\n\n\n3 There are many definitions of Cybernetics, but I find this one by P. Novikov to be the most useful for usIf we are to put decision science on a philosophical foundation, ideas from cybernetics, cognitive science, scientific process (philosophy of science), and ethics should be included. I don’t do this for the sake of rigor, but because it is an useful way of thinking in practice. These four aspects appear again and again in my teaching and applications.Utilitarism won’t do the job for us, although, we have to be aware of how influential it has been\nPerhaps, the most valuable skill from studying cybernetics is the ability to think in systems and bring clarity in messy, dynamic problems. I will not use any of its theory or tools, but it justifies and informs the way I put decision science in a larger context.\n\n\n\n\n\n\nCybernetics and the history of AI\n\n\n\nI highly recommend that you stop here and read two brilliant interviews by Michael I. Jordan, in which he tells the story of how the meaning of AI changed throughout the decades and what role did Cybernetics play:\n\nStop Calling Everything AI, Machine-Learning Pioneer Says\nArtificial Intelligence: The Revolution Hasn’t Happened Yet",
    "crumbs": [
      "Introduction and Background",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#four-principles-for-teaching",
    "href": "philosophy.html#four-principles-for-teaching",
    "title": "Course philosophy",
    "section": "Four principles for teaching",
    "text": "Four principles for teaching\nIt is hard to teach decision science in a way which is both theoretically sound and can be immediately applied. Recognizing that there is no silver bullet, my conclusion is that following the principles outlined below consistently, dramatically increases the chances of preparing a new generation ready to tackle the messy, ill-defined problems we encounter.\nFirst and foremost, provide motivation for why something is important (a field, theory, model, method, technology), then showcase relatable examples, use-cases, and the story behind the idea.4 Then, develop a conceptual undestanding, an intuition about the problem and the tool we think is appropriate in tackling it.\n4 Imagine how helpful such a 5 minute introduction would be in heavily mathematical classes\nUse more geometric intuitions, diagrams, and graphs over equations, more simulations and stories over proofs. Low-level details need careful, individual, hands-on study\nConnect the concept with previously encountered mathematical, statistical, and economic subjects\nPresent the tool theoretically rigorous and sound, but only where it matters. I believe we can make this field much more accessible without sacrificing a lot of depth\nFor the mathematically inclined, add some elements of abstract math to understand the underlying foundations of these methods and models\n\nHeavily use simulations as a safe playground that we control, to get a feel for the behavior of models and algorithms. It forces us to declare our assumptions about the problem and investigate their implications. Before we commit to a costly real-life experiment or modeling project, it is essential to know that a model works in principle for our application: is able to recover the parameters, causal structure, and generalize beyond the sample?\nDiscuss practical applications and challenges firms face, from an insider’s perspective. Those are our case-studies, outlined systematically in the following way:\n\nFirst, we try to deeply understand the problem and frame it in the “language” of decision-making. We clarify our assumptions and ask the right questions to reduce ambiguity\nThen, we reduce the problem to the simplest model, to illuminate some key aspect, like uncertainty in pricing, demand planning, and quality control decisions\nOnly then, we bring back all the realism, nuances, and complexities. Applications are based on realistic or real data, which can be messy, hard to access, biased and incomplete\nIn projects, we implement software solutions, with the main focus on the real-world challenges of operationalizing models and decision-making to improve performance. Interactive data visualization will help a lot in communicating persuasively.\nKeep in mind best practices from reproducible research and software engineering, so that our apps and research code are easy to maintain and extend\n\n\n\nIn teaching and learning, I heavily rely on the Dreyfus model of skill acquisition, which describes the following stages: Novice, Competent, Proficient, Expert, Master. This 10 minutes presentation is well worth your time.\n\n\n\n\n\n\nUnderstand by building it (Feynman)\n\n\n\nI often find myself truly understanding something, only after I code it up and get a sense of the behavior and mechanics of a model. Then, I try to think of how I would apply it in practice in different contexts.\n\n\nSpeaking of prerequisites, there are some tools we have to dust off the shelves and cultivate an appreciation of their importance: linear algebra, calculus, probability theory and mathematical statistics.\n\n\nThose prerequisites are placed in context of the business practice, with swift reviews and references to resources which should fill in all the gaps.\nAt the same time, we have to gradually get rid of the bad habits that were accumulated: analyses which can’t be reproduced, mechanically following a statistical procedure (because the flowchart of statistical tests said so), jumping to a conclusion (as if there is only one, correct, textbook answer), and rushing the learning process. On the other side of the spectrum, perfectionism doesn’t help either, as this field is inherently iterative and experimental.\nTherefore, we need to develop a set of processes and methodologies to iterate and improve effectively. We focus and pay close attention to the process of problem-solving: from formulation to modeling and operationalization.",
    "crumbs": [
      "Introduction and Background",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#lessons-in-humility",
    "href": "philosophy.html#lessons-in-humility",
    "title": "Course philosophy",
    "section": "10 Lessons in Humility",
    "text": "10 Lessons in Humility\nIf you found yourself on this long journey towards mastery, I hope you will find these lessons helpful:\n\nThere is no shortcut to deep understanding\n\nOf a domain, especially in an interdisciplinary setting\nWith communities engaged in an evolving dialogue\n\nThere is no shortcut to being skillful at something\nThe journey from novice to expert is not linear, however, the “interest compounds”\nThe journey need not be painful, but it can be seriously playful, a source of wonder and meaning\nWithout skin in the game, we can’t claim we truly get something\nWithout a vision which is flexible enough, but at the same time long-lived:\n\nIn the case of rigidity - there is a risk of being stuck, pursue obsessively, counterproductively the wrong thing\nIn the case of everything goes - there is a risk of wandering aimlessly and not finding a home\n\nFixating on beliefs and propositional knowing (the facts!) is counterproductive. Which should put into question all written above\nFixating on skills makes you lose the grasp of the big picture",
    "crumbs": [
      "Introduction and Background",
      "ii. Course philosophy"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Are you ready for an adventure?",
    "section": "",
    "text": "There are three questions I lose sleep over: “What is data science?”, “Is it still relevant?”, and “How should it be taught?” The standard definition “intersection of statistics, economics, and computer science”, answers none. Instead, I find it helpful to think of this applied, quantitative, and interdisciplinary field as decision science.\nIn the context of businesses, we want to improve financial and non-financial outcomes like revenue, profitability / EBITDA, market share, unit economics, production efficiency, customer acquisition, user experience, customer lifetime value, etc. A firm will increase its chances to improve performance if it has an accurate diagnosis of the current state and formulated a good strategy, but it still has to make many good decisions under uncertainty, test and validate new ideas.\nA decision scientist should collaborate with domain experts, decision-makers, clients, and stakeholders to understand their domain, challenges, ask the right questions, and formulate a problem well. They will probably use tools from systems’ thinking,1 data analysis, statistical modeling, and simulation to come up with hypotheses, ideas, and an understanding of the problem space.\nThis effort could already be valuable as a diagnosis of the causes of performance, inspiration for decision-making and strategy. However, we still need to test if our ideas bring an improvement and whether our hypotheses are plausible. For that, we design experiments, operationalize predictive models and optimization algorithms, implement interventions and policies.2\n\\[\n\\text{Question}     \\rightleftharpoons \\text{Model} \\implies [ \\text{Insight} \\rightarrow \\text{Action} \\rightarrow \\text{Outcome}]\n\\]\nThe models we’re building should not only be statistically rigorous and implemented well, but bring insight into the consequences of our actions. Moreover, a decision scientist should have the skills to communicate these insights in a plain language, persuasively, and be transparent about the assumptions they’re making and the uncertainty associated to their predictions and inferences.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#why-focus-on-fundamentals",
    "href": "index.html#why-focus-on-fundamentals",
    "title": "Are you ready for an adventure?",
    "section": "Why focus on fundamentals?",
    "text": "Why focus on fundamentals?\nNot gonna lie, the task and endeavor I outlined is not easy at all. In this website, I will explain how to study and master the fundamentals of statistical modeling, business economics, and “programming for data science”, which will help you become more effective at problem-solving. These skills will be valuable in the majority of career paths you might choose, but require effort and commitment.\nLast, but not least, I aspire to teach people how to navigate this interdisciplinary landscape and gain an understanding of how statistics, machine learning, econometrics, operations research, AI, cognitive science, and scientific method fit together. 3 The mastery of fundamentals, together with an understanding of “the landscape” will enable you to develop a powerful toolbox for tackling most problems that you will encounter in businesses.\n3 The terms of AI, data science, quant, analytics are overused. We will have to define them more precisely\n\n\n\n\n\nWho should read this guide?\n\n\n\nAnyone lost, confused, stuck, or overwhelmed by data science and machine learning complexities, who wants to see the big picture, a path forward, and the possibilities afforded by decision science.\nIf you stumbled upon this website, you’re probably a student in Quantitative Economics, Business Analytics, or know me personally – well, because I shamelessly promoted it.\nMaybe, you’re an engineer getting curious about ML or an analyst with a knack for the business, looking to improve your workflow and expand the quantitative toolbox. Maybe you’re a product manager or an entrepreneur who wants to infuse AI into your startup.\nIn my opinion, junior data scientists and ML practitioners a few years into their journey will benefit the most from the re-contextualization of fundamentals that I’m doing here, which could enable them to take another leap in career.\n\n\nThis website is not a self-contained course, book, or bootcamp – but a vision, philosophy, and concrete roadmap for learning and teaching decision science. It emerges out of:\n\nLessons learned from my industry experience as a data scientist, engineering and product manager, then leader\nTeaching to creative and motivated students, who are confused by the overwhelming amount of content available online\nThe need to bridge the gap between theory and practice, between mathematical world of elegant abstractions and messy real world of data and decisions in businesses 4\nThe work of outstanding teachers and researchers who made their courses freely available. Without them, both my career and this guide would be impossible\n\n4 There are many excellent textbooks, but we need better stories and case-studiesA lot has been written about the journey from novice to expert and the process of mastery in programming, chess, tennis, painting, music, etc. Decision science is challenging due to its breadth across disciplines and depth (starting from fundamentals). It will require two kinds of intentional practice and active learning over a long period of time:\n\nContemplating in the library: understanding the inner workings and practical relevance of key theoretical ideas\nEngineering in the trenches: practicing battle-tested technologies and solving increasingly messier, more complicated, and realistic problems\n\n\n\nI my teaching, I present stories and use-cases which motivate probability distributions, LLN/CLT, Bayes’ rule, sampling, measurement, etc.\nIn contemplation, we try to see how fundamental theoretical concepts and models translate into real-world stories and solutions to practical problems. It might become confusing at times, but I hope you bear with me until you see the benefits of those abstractions.\nWhen exploring the problem space in various domains,5 I recommend that you take the point of view of a decision-maker inside a business and not of a detached scientist. This practice will force you to focus on actions, relevance of insights, and understanding clients’ needs, challenges, and objectives. In practice, it’s little consolation if we picked the right model and methodology for the task if it’s not actionable and useful – interesting is not good enough!\n5 Like banking, e-commerce, tech platforms, finance, insurance, public health, etcOn the other hand, engineering in the trenches is a hard skill. The only way to become better at modeling and data analysis is hands-on practice – with your code editor, book, and pen/paper near you at all times. You will truly understand something only by building it.\nIn order for our models to be useful at scale, we have to implement these ideas in code and integrate into existing software systems. If we are to increase the chances of success, we’ll also have to follow best practices of reproducible research, automation, and model operationalization. We’ll also have to think as a product manager.\n\n\n\n\n\n\nHow is modern decision science different?\n\n\n\nA fair question to ask is how this vision is different from data science. In my experience, data science teaching and jobs focus much more on exploration, data mining, and inspiration – to the point of becoming “analytics + machine learning”. There is nothing wrong with it when applied appropriately, but something important is missing.\nIn decision science, actions and outcomes are central, not the insight, inspiration, or prediction. This means that we need more statistical and methodological rigor, scientific and causal thinking / inference, economics and cognitive science of decision-making.6\nTherefore, we don’t pay much attention to data mining, LLMs, computer vision, and deep learning. Nor we do complicated ensembles, stackings, and hyperparameter optimization of machine learning models. Having said that, we do respect robust predictions and software engineering aspects of data science – but choose to focus on what do we do with it next.\n\n\n6 One has to be very careful when using a predictive model in situations when we want to intervene in a system, not just to use predictions in a downsteam optimization task.At last, I want to tell you two important lessons from Patrick Winston’s talk at MIT, “How to speak”, which motivate the intentional practice and the value of mastering the fundamentals\n\nThe quality of your idea and solution is a function of knowledge, practice, and talent: \\(Q = f(\\mathbb{K}, \\mathbf{P}, T)\\), where talent is the least important and we shouldn’t over-emphasize it\nYour success will be largely determined by your ability to speak, write, and the quality of your ideas – \\(g(S, W, Q)\\)",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#why-is-this-journey-rewarding",
    "href": "index.html#why-is-this-journey-rewarding",
    "title": "Are you ready for an adventure?",
    "section": "Why is this journey rewarding?",
    "text": "Why is this journey rewarding?\nYou might’ve heard that data scientist is the sexiest job of 21st century, that AI is going to take over repetitive jobs, deep reinforcement learning models are beating people at go, Dota, chess, solving almost-impossible protein-folding problems. The world is awash in the newest ChatGPT and Midjourney frenzy, with new developments every month and week. There are so many cool applications of AI and modeling and so little time.\n\n\n\n\n\n\n\n\n\nHow do I keep up?\n\n\n\nYou don’t, at least if you want to have a balanced life. That’s why I choose to focus on fundamentals which stood the test of time. You will be surprised how many problems that businesses encounter can be solved well with simple, even linear models.\nThese (statistical) fundamentals are anyways a prerequisite before diving into understanding the technicalities of those cutting-edge models and systems.\n\n\nI want you to realize, that despite all the (justified) hype around deep learning and generative AI – well established fields of statistics, econometrics, causal inference, operations research, control theory, dynamical systems, cognitive science, and computer science have been evolving as well. These are not just prerequisites and intellectual forerunners for AI, but tools routinely and successfully used in a large problem space.\nWe live in a volatile, uncertain, complex, and ambiguous world,7 but we still have to make decisions. Those decisions will bring better outcomes if they are informed by understanding the causal processes, driven by evidence, and robust predictions. People study the fields outlined above in order to be well equipped for such challenges.\n7 VUCA: a mental model to better understand the worldIn businesses, data science and AI can have a function of decision-making support, process improvement, be an essential part of the system or product itself – like in the case of Uber, Amazon, Netflix, Spotify, Google and many others.\nTo get a better sense of what I mean by decision-making support, I will present a few challenges many data scientists are working on in different industries. Notice the common thread of optimization at a large scale and that many of these applications are related to important decisions in the value chain of a firm.\n\nDemand forecasting and inventory optimization: if we produce or procure too little, we’ll lose sales and customers, if we order too much, we’ll end up with excess inventory and that cash could’ve been spend for growth or R&D\n\nCan we quantify the uncertainty in demand? Do our methods scale to millions of SKUs like in the case of Zara?\nProduction planning and quality control is perhaps the most tangible of applications, where we can see the improvements brought by our models on the ground\n\nRevenue management, pricing optimization, and personalized promotions. This is what ride sharing platforms like Uber and Bolt do, airlines, and when you buy a concert ticket8\nEstimating the impact of advertisement, marketing mix modeling, and conversion optimization. Love it or hate it, the reality is that most direct-to-consumer firms have to advertise in order to grow – and they better allocate that marketing budget smartly\nCustomer churn, repurchase, engagement, lifetime value (LTV) is perhaps where most data scientists’ time is wasted. Knowing which customers are at a risk of unsubscription is interesting, but what we really want is how to improve their experience with the service and convince them to stay\nFraud detection, credit default prediction, insurance risk are classical and important applications in banking and insurance, if you’re into that kind of stuff\nChoice modeling, recommender systems, targeting and uplift models help the firm better understand customer preferences and choices, thus, suggesting products which are most relevant for them out of an overwhelmingly big catalog\nImproving products, assortment, and merchandising also require the kind of experimentation and quantitative methods we’re going to investigate on this website\n\n8 The first two areas are all about balancing the demand and supply, which requires careful economic thinking and quantitative methodsDon’t worry if you’re not familiar with some of the applications or if it’s not clear yet which models and fundamentals will be used. If none of these examples appeal to you, there are no less applications in other fields like finance, public policy, healthcare, political science, manufacturing, psychology, ecology, etc.\n\n\nI’m having a hard time coming up with examples where the methods we’re going to study are NOT useful. Even anthropologists have extremely sophisticated statistical models in order to make the most out of small and poor quality data. Maybe accounting (?)\n\nI want you to take away one thing, that is “AI” and “data science” in businesses boils down to Decision-Making under Uncertainty at Scale\n\n\n\n\n\n\n\nThink of youself as a business person with superpowers\n\n\n\nThis is the best advice I ever got as a novice data scientist and it will be really helpful for people who care a lot about the technical aspects of statistical and machine learning models.\nYour superpower is the ability to solve problems at scale and answer difficult questions by building models and programming. However, you should always put yourself in the shoes of the client and deeply understand their domain, data, objectives, tradeoffs, and decisions. If you want your work to bring change, communicate persuasively, in their language.\nThis mindset will ensure that your solution is valuable and used. Otherwise you might risk that your high-quality contribution is not understood, not trusted, not actionable and thus, not adopted.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#dont-travel-without-a-map",
    "href": "index.html#dont-travel-without-a-map",
    "title": "Are you ready for an adventure?",
    "section": "Don’t travel without a map",
    "text": "Don’t travel without a map\nWhen you travel to an unknown place, you need a map to know your location and where are you going. Consider this guide a conceptual frame which ties together everything you have learned so far and can be built upon as you progress in your career and studies. You will probably go back to the same idea years later, with greater wisdom and skill – to unlock its real power. We should embrace the fact that learning is not linear.\nAt the risk of annoying you with my metaphors, let me explain what is wrong with our existing maps. Some are too simplistic and low resolution, resulting in too much wandering around. Others are not maps at all, but a sequence of subjects to study – which misses the interdisciplinary nature of decision science. At last, many are too detailed, overwhelming, and unclear about what is most important.\n\n\n\n\n\nThis is a big picture course, which re-contextualizes everything you have learned before, but didn’t see how it fits together or how can it be implemented in practice to bring value to organisations, that is: be useful\n\n\n\n\n\n\n\n\nOverwhelming amount of content\n\n\n\nIf you go to the data science section in Coursera, you will see a hundred pages of courses and specializations. The situation is even worse with the amount of available tutorials and Medium.com articles.\nA student is guaranteed to get lost in details and be overwhelmed by all those 800 page hardcore textbooks required by some classes. On the other hand, seemingly pragmatic, cookbook and bootcamp-style approaches miss depth, nuance, key theoretical ideas, and methodological aspects.\nInevitably, you have to use the recommendations provided by other people. I really hope you will like the references I provide, which were carefully curated during the past 10 years. Almost all are free, open-source, and supplement the lectures / theory with code examples.\n\n\nThis is the course I wish I had when starting my journey in data science, which would prepare me for the realities of industry, often very different from the academic world. In the following sections and chapters, I will outline a powerful set of fundamentals, the common threads and connections between them, where to read or watch to gain understanding, and how to practice in order to develop your skills.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#what-is-a-model",
    "href": "index.html#what-is-a-model",
    "title": "Are you ready for an adventure?",
    "section": "What is a model?",
    "text": "What is a model?\nBy now you’ve heard the word “model” a lot of times. In the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. The real world is overwhelming and combinatorially explosive in its possibilities – we can’t take into account everything when making a decision. 9\n9 Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etcWe build models, because we want to understand a system and capture, explain the essential and relevant aspects of the phenomena of interest (e.g. what drives demand), in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen. The reverse side of the medal are biases: patterns we think are causal, but are not real.\nA model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions. I know this sounds very abstract, but be patient, I’ll formalize what a statistical model means and give plenty of practical applications and examples.\n\n\n\n\n\nWhat does Pollock and Picasso have to do with statistical modeling and quantitative questions?\n\n\n\n\n\n\n\n\nPhilosophical detour on latent processes\n\n\n\nOne of the most important ideas in statistics is that the causes are not found in data, but in scientific theories. In other words, the answer to our quantitative question is an unobserved, latent quantity or process. Hence, the data is the phenomenon, at the surface (Pollock’s canvas) and we would like to know what mechanism could plausibly generate it.\nOn the other hand, models are useful because if successful, they separate the noise from the signal and synthesize all the relevant information we have in our sample. This generalization (abstracting away from idiosyncrasies of each observation) allows us to make predictions and gain insights. Think of the way Picasso drew a camel – lots of artists will agree that simplification is also beautiful.\n\n\nWe collect data, perform experiments, and build models in order to minimize the effect of our biases and foolishness. It’s also important to remember that all models have assumptions, pressupositions, and limitations. They are little machines, golemns of prague which follow instructions precisely, and can backfire if used outside their range of validity. They can be powerful, but lack wisdom, which is found in your domain / problem understanding and scientific thinking.\n\n\n\n\n\n\nThree challenges in statisical modeling\n\n\n\nA. Gelman highlights three different aspects of statistical inference. We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.10\n\\[Sample \\longrightarrow Population\\]\n\\[Treatment \\longrightarrow Control\\]\n\\[Measurement \\longrightarrow Construct\\]\nThe holy grail is to build statistical models based on the causal processes informed by theories and hypotheses. If we take into account how we measured, and collected data, we’ll increase our chances to generalize our conclusions and will have stronger evidence.\n\n\n10 You can also think of prediction and inference as the problem of missing dataIn other words, our vague economic or scientific hypotheses translate into concrete process models (that we can analyze mathematicaly and in simulations), for which we build statistical models in order to draw empirical conclusions and quantify the uncertainty in our beliefs (or the expected outcomes of long-run action).A good hypothesis is equivalent to asking a good question, and the process model is informed by our domain knowledge",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Are you ready for an adventure?",
    "section": "Prerequisites",
    "text": "Prerequisites\nI think this roadmap will bring the most value to masters’ students, professionals, and students in their last year of BSc. If you never had a linear algebra, calculus, statistics, probability, and programming classes (or are very rusty in them) – read appendix 1 for a guide on how to integrate these foundational subjects into your study plan.\nMost people I know studied statistics and probability at some point, but the presentation was highly theoretical and without any programming or data analysis. I propose we go back to the most important ideas, discuss their practical relevance, and really understand them by coding up the simulations for our real-world stories. We will see that statistics is not just useful, but can be a lot of fun!\n\n\n\n\n\nPractice the fundamentals with patience and care, develop competence. Then, a beautiful world will open up to you!\n\n\n\nFor Linear Algebra and Calculus – only exposure is needed, but competence and mathematical maturity will help a lot. It is a personal choice how deep to dive, but the more you know, the more comfortable you will be with mathematical abstraction.\nFor Probability Theory – competence is needed, even though we start from the very beginning with a review of combinatorics. I suggest you read along and practice with a more comprehensive resource, like Joseph Blitzstein’s “Probability 110”.\nFor Mathematical Statistics the story is the same as for Probability. You will need at least to be familiar with sampling distributions, CLT, hypothesis testing, and regression. This course attempts to clear up misunderstandings so that you don’t fall into common pitfalls of statistical practice.\nPython or R programming for data science is mandatory in order to do anything even remotely sophisticated in practice. 11 Besides the basics of the language, we need to develop competence in data wrangling, visualization, SQL / databases, and reproducible data analysis. I recommend two free books:\n\n“R for Data Science” by Hadley Wickham\n“Python for Data Analysis” by Wes McKinney.\n\n\n11 Most universities introduce it far too late, but things are changing. I would argue that one has to start coding and analyzing data from stats 101 and linear algebra. Of course, the more experience and proficiency you have in one or both, the better12 And if you’re interested in investment and portfolio management, check out Ben Felix, who recommends many foundational papers in the fieldIf you have to deal with financial statements at your job or have an interest in corporate finance, I strongly suggest you check out A. Damodaran’s NYU MBA lectures.12 Depending on your role in the firm, knowledge of microeconomics, marketing, management, operations, and logistics might be essential.\nLast, but not least, there is value in understanding the scientific process and quantitative research methods. In Chapter 2, I adapt it to the context of decision science in businesses and show how most processes and workflows like CRISP-DM (data mining), Tuckey’s EDA (exploratory data analysis) are a part of it.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#what-will-you-learn",
    "href": "index.html#what-will-you-learn",
    "title": "Are you ready for an adventure?",
    "section": "What will you learn?",
    "text": "What will you learn?\nFirst, in “Course Philosophy”, you will get some important tips and perspectives on how to learn in the context of this challenging, interdisciplinary field. I can attribute most of my past failures in learning to either a lack of solid fundamentals or to little intentional practice. Generally, you will truly understand by building it.\n\n\n\nThe top and bottom rows are the fundamentals, out of which we don’t cover linear algebra, calculus, and the basics of programming. I will highlight the practical relevance of probability and statistics through stories and case-studies. The four rectangles in the middle correspond to different modeling approaches\nThen, I will frame everything we do in the context of the scientific process and show how methodology is important in maximizing our chances of a successful project. I think that most AI/ML and research initiatives fail because of poor planning and ad-hockery, but we can do better by not falling into most common pitfalls.\nWe also need to disucuss how our quantitative toolbox fits with what businesses are trying to do. I will briefly present a few ways to think about performance, strategy, decisions, and the value chain. It’s a good place to dive into more details on the wide range of applications and use-cases in various domains and industries. This is where we will introduce our three problem-solving approaches and perspectives: predictive (machine learning), inferential (statistical modeling), and causal (causal inference).\nNext, we will use stories, case-studies, and simulations to get a solid grasp of most important concepts and ideas from probability and statistics. Many errors in applied statistics stem from a fundamental misunderstanding of the nature of statistical inference – so it’s very important that we think carefully about our problem, assumptions and not blindly follow a procedure.\nWe will also see what was the original motivation and practical relevance for the theory and methods you encountered before (t-test, ANOVA, linear regression, CLT etc). You can find this material, which is work-in-progress, in the “Simulation of economic processes” section – with both conceptual lectures and hands-on labs.13\n13 Alternatively, you can look into the “Probability and Statistics study guide” for resources of what to read, code, and practice on each topic.In the “Bayesian Statistics study guide”, you will find a clear sequence for studying statistical modeling and the fundamentals of causal inference. The set of techniques you’re going to encounter on the way towards hierarchical gerneralized linear models (GLMs) will be useful in about any practical application you’re going to encounter in practice. Moreover, this will prepare you for both machine learning and more advanced techniques in causal inference.\nAt this point, a good way to apply what you have studied is to pick a research question you’re passionate about, pick a problem-solving and modeling approach, do simulations, collect data, implement a model, and share the results in a 5 page paper or report. This will also teach you how to apply a methodology and communicate your ideas clearly in writing.\nLast, but not least, I’ll share practical advice on how to operationalize your models and build full-stack data apps. This is important if you want your solution to bring the most impact for your clients and stakeholders. For that, you need their trust and buy-in, thus, an interactive application is one of the best ways to showcase what the models can do.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Are you ready for an adventure?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI mentioned that my teaching and industry success stands on the shoulder of giants. This is a metaphor scientists use to emphasize that we incrementally build on the work of others.\nWhen it comes to statistical modeling and machine learning, the following people had a large influence on the way I think and in turn, I will refer you to their excellent books and courses: Trevor Hastie, Bradley Efron, Yaser Abu-Mostafa, Shai Ben-David, Richard McElreath, Andrew Gelman, Jennifer Hill, Aki Vehtari, Stephen Boyd, Gilbert Strang, Steven Burton, Nathan Kutz, Cassie Kozyrkov.\nI have to especially thank Dr. Gheorghe Ruxanda for repeatedly pointing out the beauty, elegance, and importance of the deep foundations underlying probability theory and statistical modeling. I hope to pass on these insights to the future generations of students in quantiatative economics.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "index.html#why-not-another-textbook",
    "href": "index.html#why-not-another-textbook",
    "title": "Are you ready for an adventure?",
    "section": "Why not another textbook?",
    "text": "Why not another textbook?\nAt last, I have to explain why I’m not writing a book yet and linking you instead to the best resouces I know of. I think we have more than enough excellent books to learn the theory and develop skills in the fields within decision science.\nNo one has time to study it all and there is no single book which does everything best.14 So, by cherry-picking lectures, chapters, examples, and applications; then arranging them into a coherent roadmap, we get different perspectives and will start developing preferences for one way of teaching and one approach over another.\n14 A cheeky reference to the no free lunch theorem15 Ok, enough hyphenated adjectives, you get the pointFrom the point of view of teaching, the only reason to write my own book is the following: we need engaging, high-quality, in-depth, hands-on, realistic case-studies. 15 Sometimes, the only criticism I can address to some of the books is that the code examples are applied to boring and overused problems and datasets. It is not a fair criticism, since their main goal is teaching how the models work and no one will publish a thousand page manuscript.\n\nThe best kind of applications are relatively simple, but interesting and pose real modeling challenges. They capture the essence of the problem. This is why I heavily rely on Richard McElreath’s “Statistical Rethinking, 2nd ed.” and “Bayesian Data Analysis, 3rd ed.” by Andrew Gelman et. al. \n\nIn my opinion, writing such case-studies takes more time than actually solving the problem inside a firm. I will do it, someday. In general, after you studied from the linked materials, tried to write the code without reference, and understood the examples in depth – it is your responsibility to come up with a project or case-study. It can be on openly available or private data, on messy, real data, or your own, clean simulations. It’s a good idea to choose a domain, problem, and question you’re passionate about.\nAt last, the cleanest code associated to applications is not always written by the authors. Often, the open-source community significantly improves it and we’ll take advantage of this fact. Sometimes, I don’t like any code and will do it myself, but this is really, polishing an existing idea rather than an original contribution.",
    "crumbs": [
      "Introduction and Background"
    ]
  },
  {
    "objectID": "04_engineering/roadmap.html",
    "href": "04_engineering/roadmap.html",
    "title": "Modern Decision Science",
    "section": "",
    "text": "flowchart LR\n  DSc(Data Apps)  --&gt; Data(Data Wrangling)  --&gt; EDA(EDA) --&gt; DV(Visualization)\n  EDA --&gt; LP(Literate Programming)\n  \n  DSc --&gt; Repr[Reproducibility] --&gt; DP[Data Pipelines]\n  DSc --&gt; M[Modeling] --&gt; Pymc(PyMC) --&gt; Tr[Torch]\n  DSc --&gt; MLP[ML Pipes] --&gt; Srv(Serving Models)\n  \n  DV --&gt; FS[/Full Stack Apps/]\n  Tr --&gt; FS\n  Srv --&gt; FS\n\n  Repr --&gt; LP\n  DP --&gt; FS\n  FS --&gt; Dep[Deploy]\n\n\n\n\nFigure 1: We will need to learn a lot of engineering and new tools, so that we’re able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients."
  },
  {
    "objectID": "04_engineering/roadmap.html#module-iv-full-stack-data-apps",
    "href": "04_engineering/roadmap.html#module-iv-full-stack-data-apps",
    "title": "Modern Decision Science",
    "section": "",
    "text": "flowchart LR\n  DSc(Data Apps)  --&gt; Data(Data Wrangling)  --&gt; EDA(EDA) --&gt; DV(Visualization)\n  EDA --&gt; LP(Literate Programming)\n  \n  DSc --&gt; Repr[Reproducibility] --&gt; DP[Data Pipelines]\n  DSc --&gt; M[Modeling] --&gt; Pymc(PyMC) --&gt; Tr[Torch]\n  DSc --&gt; MLP[ML Pipes] --&gt; Srv(Serving Models)\n  \n  DV --&gt; FS[/Full Stack Apps/]\n  Tr --&gt; FS\n  Srv --&gt; FS\n\n  Repr --&gt; LP\n  DP --&gt; FS\n  FS --&gt; Dep[Deploy]\n\n\n\n\nFigure 1: We will need to learn a lot of engineering and new tools, so that we’re able to collect, clean, explore, visualize data, train models, and build useful applications which improve outcomes for our clients."
  },
  {
    "objectID": "sim/1_intro.html",
    "href": "sim/1_intro.html",
    "title": "Simulation of economic processes",
    "section": "",
    "text": "Welcome to the simulation course! First, let’s talk a little bit about movies. You’re probably well aware how popular films about multiverses and time travel have become. 1 Their main point is that our actions have consequences and that there is a “garden of forking paths” leading the protagonist to very different outcomes – some very likely, others almost impossible. This idea is faithful to real life: we have to make good decisions under uncertainty and with limited resources. It’s not an easy task.\nIf we had a crystal ball which flawlessly predicts the future state of the world and the outcomes of our decisions, we wouldn’t be here in this class. The next best thing is if the oracle tells us the calibrated probabilities of relevant events happening (e.g. if our startup or investment will succeed), but we’ll have to ask the right questions and solve a pretty large optimization problem.2 It’s also useful to know that one of the most important ideas in statistics is the counterfactual, a probabilistic answer to a “what would happen if” question.",
    "crumbs": [
      "Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#decisions-under-uncertainty",
    "href": "sim/1_intro.html#decisions-under-uncertainty",
    "title": "Simulation of economic processes",
    "section": "Decisions under uncertainty",
    "text": "Decisions under uncertainty\nIn the context of businesses, we want to improve financial and non-financial outcomes like revenue, profitability (EBITDA), market share, unit economics, production efficiency, customer acquisition, user experience, customer lifetime value, etc. A firm will increase its chances to improve performance if it has an accurate diagnosis of the current state and formulated a strategy, but it still has to test their ideas and make many good decisions over a long period of time, i.e. execute well. 3\n3 Even then, there is no guarantee. Most startups fail in the first 5 years\n\n\n\n\n\nWhat makes a decision good?\n\n\n\nThink for a few minutes and write down what are your criteria which would make a decision good. How did you deal with cases in which we have a great outcome by sheer luck and chance?\nCome up with an example of an important decision a fashion e-commerce has to make. Define the objective, constraints, tradeoffs, and a few alternative choices. Test your criteria. Do they make sense? Are they necessary? Sufficient?\n\n\nI hope you figured where I am leading with the multiverse story. We study probability theory, statistical modeling, optimization (operations research), and economics in order to make better decisions under uncertainty, possibly at a large scale. We also need to know enough programming to analyze data, build models, and solve optimization problems. Think about how many decisions Uber has to make every day in each city to match drivers with passengers’ routes and set the right prices.\nI call this applied, quantitative, and interdisciplinary field “decision science”, which is also known as data science, specialized AI, and economic cybernetics. In practice, a decision scientist should collaborate with domain experts, decision-makers, clients, and stakeholders to understand their domain and challenges, ask the right questions, and formulate a problem.\n\\[Question \\longrightarrow Model \\longrightarrow Insight \\longrightarrow Action \\longrightarrow Outcome \\]\nThen, they will perform experiments, collect data, and build statistical models which bring insights into consequences of actions, interventions, and policies. 4 These insights, inferences, and predictions will inform the firm which decisions are more promising and whether their hypotheses hold up to evidence.\n4 Inspired by A. Fleischhacker’s Business Analyst Workflow\n\n\n\n\n\nWhat is a model?\n\n\n\nIn the most general sense, a mental, mathematical, or statistical model is a simplified representation of reality. Reality is overwhelming and combinatorially explosive in its possibilities. 5\nWe build models, because we want to understand a system and capture / explain the essential aspects of the phenomena of interest, in order to make good decisions. This notion of understanding hints to the idea of causality: if we intervene, this is what is likely going to happen.\nA model will take you only as far as the quality of your problem formulation, its assumptions, and the data you have. Its results are useful only if they inspire new questions or hypotheses, generate actionable insight, or make reliable predictions.\n\n\n5 Imagine how many possible paths are there if for every minute we have a choice between 20 actions: read, eat, move, watch, etcTo make things more concrete, let’s look at an example of a bakery which produces fresh, but perisable good(ie)s. We assume that if they don’t manage to sell the pastry in the same day, it will have to be thrown away. They choose to sell at the price \\((p)\\) after lots of market research and a bit of experimentation. The cost of producing an unit is \\((c)\\). The bakery has data on daily historical sales \\(\\{d_1, d_2, ..., d_t\\}\\) and for the sake of simplicity, we’ll assume that they never ran out of stock. The question is how much they should produce tomorrow \\((q)\\).\nThis is called the “newsvendor problem” and is a prototypical example of inventory management.6 If we order or produce too little, we’ll lose sales, and in the long term, customers. If we order too much, we’ll end up with waste. In a 2003 paper, M. Koschat and N. Kunz7 have shown how they brought an additional $3.5m of incremental profits to the Time Inc. magazine by solving this optimization problem.\n6 Even with this simple model, things can get complicated very quickly: both in terms of demand forecasting and optimization7 M. Koschat, N. Kunz - “Newsvendors tackle the newsvendor problem”, 2003\\[\n(\\max_q) \\; p \\cdot  \\mathbb{E}_D[\\min(D, q)] - c \\cdot q\n\\]\nAt first, the problem might look similar with what you did in other classes: we want to maximize the gross margin (profit) and q is our decision variable, which is a count (\\(q \\in \\mathbb{Z}_+\\)). However, the demand is a random variable which can be modeled as simply as the normal distribution or as complicated as a multilevel regression. Moreover, it’s not clear that reducing the problem to an expectation maximization is the best thing to do in practice.\nIf your hunch is that we will need to do some modeling and simulations in order to solve this problem, you’re absolutely correct.8 We will not be satisfied with \\(d_i \\sim N(\\mu, \\sigma)\\), as this will probably be an over-simplification of the processes generating the demand. In the real world, we don’t want to ignore seasonality, day of week effects, out of stock and promotions in historical data, trends, and serial correlation.\n8 It will quickly become unfeasible to find out an analytical solutionOf course, the modeling approach will be pretty different if the product is slow-moving, selling a lot, or has an intermittent demand. We will investigate these issues in great detail in the following weeks, but for now you have an example of the idea that models are a simplified representation of reality and have assumptions.\n\n\n\n\n\n\nSources of uncertainty\n\n\n\nIn their book Algorithms for Decision Making, MIT Press - 2022, M. Kochenderfer, T. Wheeler, and K. Wray make a helpful classification of the sources of uncertainty, based on agent-environment interactions:\n\nOutcome uncertainty suggests we can’t know the consequences and effects of our actions with certainty. We can’t take everything into account when making a decision\nModel uncertainty implies we can’t be sure that our understanding, assumptions, and chosen model are correct. In decision-making, we often misframe problems and in statistics, well, choose the wrong model.\nState uncertainty means that the true state of the environment is uncertain, as everything changes and is in flux. This is why statisticians argue that we always work with samples\nInteraction uncertainty due to the behavior of the other agents interacting in the environment. For example, competitive firms, and social network effects.\n\nWe will focus very little the last aspect of uncertainty, but you have some tools to reason about it: game-theoretic arguments, ideas from multi-agent systems, and graph theory.\n\n\nIn our toy example of the newsvendor problem, the uncertainty in the state (demand) translated into outcome uncertainty (profits). If we chose the wrong model for the demand, our conclusions might also change significantly. As an exercise, think of a few scenarios where the interaction uncertainty would need to be taken into account.",
    "crumbs": [
      "Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#simulation-and-numerical-methods",
    "href": "sim/1_intro.html#simulation-and-numerical-methods",
    "title": "Simulation of economic processes",
    "section": "Simulation and numerical methods",
    "text": "Simulation and numerical methods\nNow that you have the multiverse metaphor in mind and the context of decision science, simulation can be seen as an extremely useful set of numerical methods and algorithms used in estimation, optimization, and modeling. Its practical value boils down to the fact that most problems and models of interest to us won’t have efficient and scalable closed-form or algorithmically exact solutions. 9 Therefore, we need to learn techniques which make good approximations.\n9 In plain english, it means that we can’t do it by hand and have to implement in code. Even if there is an exact algorithm, it might not scale with the amount of data and problem size – which most often renders it useless In the following sections, I explain different ways in which simulation and numerical methods can be applied, which is much more than the “Monte-Carlo” method you might be familiar with. Last, but not least, simulation is extremely useful when learning probability, statistics, and econometrics. It’s a way to take an abstract mathematical idea, for example, Central Limit Theorem or HAC robust standard errors, see how it behaves under different assumptions, and how it could be useful in practical applications.\n\nTheoretical and process models\nThere is a famous saying that explains some differences between natural and social sciences: “Atoms are not people”. Human behavior is an endlessly fascinating and complex area of inquiry, but we can’t hope to achieve the same level of confidence in theories and hypotheses we develop as physics, chemistry, or biology. Also, we can’t perform experiments in the way engineers do – it will be highly unethical and often unfeasible.\nTherefore, in applied economics, mathematical modeling takes a back seat to statistical, econometric, and machine learning methods.10 Whatever “laws” we might conjecture, it will be incredibly difficult to empirically validate them and “prove” that the decisions / interventions made bring an improvement. Often, when it comes to consumer behavior, we don’t even have a theory: think about the choices of what movies to watch on Netflix or the premium we’re willing to pay for a branded hoodie. But we do have an understanding and we might have tons of data.\n10 Recent Nobel prize awards in economics show how important econometrics and causal inference has been for the fieldYou have to know that “theory-free methods”, for all practical intents and purposes, do not exist. Even when we don’t make distributional assumptions (in nonparametric statistics) and learn a pattern from data (machine learning), we still do have to decide what are the constraints, objectives, relevant data, measurement, sampling, interventions, and most plausible causal mechanism underlying the behavior / observed phenomenon.\nThere are multiple techniques for model validation which you will study in econometrics / data analysis classes and they’re extremely important. They will signal to you possible ways in which the model fails and that you might’ve picked the wrong one, but they’re not sufficient. How so?\n\n\n\n\n\n\nThe causes are not to be found in data\n\n\n\nImagine two different models with wildly different assumptions which give similar predictions. We’ll conveniently call them “heliocentric” and “geocentric”. How can we know which one is more plausible?\nThe short answer is not from one dataset, and not from a single thing we measure, but from “networks of cumulative evidence” and competing against the next best hypothesis. 11\nThe idea is not to celebrate the triumph of Galileo/Copernicus, but to point out that such debates happen routinely. A few examples are the fields of molecular biology and ecology which spanned decades-long bitter rivalries.\nThe explanation is simple once you are aware of it – that different vague hypotheses can be operationalized in multiple ways as mathematical / process models, which might result in very similar statistical and distributional patterns.\n\n\n11 I highly recommend that you read a little bit about the philosophy of science and how the scientific process works.I hope you bear with me along this justification to finally get to the point of the value of simulation. It’s a way to implement probabilistic models of economic phenomena in code, which forces us to declare explicitly our assumptions about the causes and processes underlying the data. Recall the idea of “data generating process” from statistics.\nThink of these models as an airplane or driving simulator, which will resemble real piloting if enough effort has been put into its development and enough scenarios have been taken into consideration. Even though they’re just a simulation, we can use them to practice and gain understanding.\nIn this course we’ll be much less confident that the patterns which result from our simulations are realistic,12 but it will be immensely helpful when building statistical models and using real data to solve business problems. This preparation will increase our chances of success in practice and will give more confidence to stakeholders that the approach we’re taking is reasonable.\n12 We’ll have to make many simplifying assumptions and can’t take into account all relevant factors or ways things can go wrong\n\n\n\n\n\nThink generatively. Does the model work in principle?\n\n\n\nThink for a minute what is the difference between probability and statistics. Probability is generative (of data) and forward-looking, predictive, given a set of “rules” and assumptions we’ve chosen. Probability is mathematics – a logic of uncertainty.\nOn the other hand, statistics is about chaning our actions and opinions in the face of evidence. It looks backwards, given the data and a chosen model – trying to infer and estimate what the “rules” or parameters were. Once we have those estimates, we can make predictions and generalizations. It uses mathematical methods, but is closer to art and engineering.\nSo, one important way in which simulation is used in practice is checking that our modeling strategy works in principle and what are different ways in which it can fail and mislead us. This is important before commiting to an expensive real-world experiment or intervention, be it a medical study, marketing campaign, or a macroeconomic policy.\nIsn’t it important to know if our modeling idea or experiment is doomed or unfeasible from the very start?\n\n\nThis process in which we try to see how the model behaves under different assumptions is called sensitivity analysis. It is an important exercise to do, because we can’t be certain our assumptions are correct and it’s likely we won’t have the necessary data or experiment to check one key assumption. We might be lucky and our models robust, or we could get wildly different results and recommendations on a slight change. It is clear that if we don’t check, we’ll never know before the costly project.\n\n\nWhat do engineers do?\nBefore moving on to the applications of simulation to optimization and statistical inference, I think it’s important to differentiate this class from what engineers would typically study in “Numerical Methods and Scientific Computing” or “Computational Methods for Data Analysis” classes.\n\n\n\n\n\n\nWhy do engineers put so much effort into simulation?\n\n\n\nYou might have friends which study physics, chemistry, or all kinds of engineering. If you ask for their numerical methods textbook, you will be shocked at how hard it is. Why?\nIn short, because their fields require complicated mathematical modeling, like partial differential equations, fluid dynamics, signal processing, 3D rendering, and so on. So, it’s not only challenging to implement/solve them on a computer, but it’s hard to do it efficiently at scale. And when we talk about climate modeling, for example, it’s the damn planet-scale.\nI’ll also give credit to quantitative finance, where modeling becomes incredibly hard – with stochastic differential equations and other mathematical nightmares.\n\n\nThere is a series of important tools which we will NOT cover,13 but might be useful in data analysis. These include SVD, QR, LU, and Choletsky matrix factorizations which pop-up in machine learning; Fourier transforms and wavelets which are important in signal processing and high-frequency time series; interpolation, approximation methods like Runge-Kutta, Euler-Maruyama for differential equations and SDEs.\n13 It’s best that you study these methods when you encounter them in context, in order to understand what do those software packages do behind the scenes\n\nStatistical inference and optimization\nAt this point you probably encountered only the simplest optimization and estimation methods, like least squares, generalized method of moments, and simplex algorithm. These algorithms are fundamental and have a very nice set of properties, but they apply to a very specific (and narrow) set of problems and models.\nEfficient parameter estimation and highly-dimensional, nonlinear optimization is an active area of research which has lead to recent breakthroughs in machine learning and AI. You could build an applied or research career in this field, if you wish so – it will be in demand for a long time. However, from the point of view of decision science, these are “just” methods so that we can get stuff done, i.e. answer our research question and make the right business decision.\nIn frequentist statistics, we want point estimates of parameters and their confidence intervals, but for more complicated models we often don’t closed-form solutions. Even worse, we might not even have an idea of how the sampling distribution looks like, which will make us doubt if our hypothesis test was reliable.\nIt is the job of theoretical statisticians to figure these things out, but we’ll inevitably encounter a problem in which we don’t want to make certain statistical assumptions – therefore, no assumptions, no guarantees. Some methods which come to rescue are maximum likelihood, versions of gradient descent, and bootstrap.\nIn Bayesian statistics,14 we view parameters as distributions and data as fixed. The advantage is that we can report the whole posterior distribution as our estimate, capture the uncertainty, and propagate it when making predictions. Does it sound more intuitive than frequentist inference? Yes, but there are modeling gotchas and computational gotchas.\n14 Don’t worry if it doesn’t make sense right now, I’ll show some simple models and the benefits of Bayesian inferenceAgain, the most interesting models to us will not have a closed-form solution, but this time, because we work with samples from probability distributions, the estimation will be muuuuch slower. This is where modern sampling methods like Markov Chain Monte Carlo (MCMC) and Hamiltonian Monte Carlo (HMC) allow us to have our cake (reap the benefits of the Bayesian approach) and eat it too (make it computationally feasible for “mid-sized” datasets). If we work with millions of data points with a large number of features, we’ll definitely have to reconsider.\nIn the previous section we were concerned with causal and scientific / economic aspects of statistics. But it’s also clear that we need reliable and efficient estimation, data analysis, model validation and diagnostics. It’s important that you keep in mind both of these aspects, as most classes focus on one or another.\n\n\n\n\n\n\nThree challenges in statisical modeling\n\n\n\nA. Gelman highlights three different aspects of statistical inference. Always remember this when designing your study or learning about a new statistical tool! We want to generalize from sample to the population of interest, from treatment to control group, and from measurement to the underlying theoretical construct.\n\\[Sample \\longrightarrow Population\\]\n\\[Treatment \\longrightarrow Control\\]\n\\[Measurement \\longrightarrow Construct\\]\nThe holy grail is to build statistical models based on the causal processes informed by theories and hypotheses. If we take into account how we measured, and collected data, we’ll increase our chances to generalize our conclusions and will have stronger evidence.\n\n\nIn machine learning, for the vast majority of models, the training process boils down to a nasty, nonlinear optimization problem, in which we hope that we won’t end up in a local minimum. As a slight detour, the training process is an algorithm which takes as an input the model, its hyperparameters, and data, which returns another model (with optimal parameters) which hopefully has the best out-of-sample, predictive performance.\n\\[\\mathcal{A} : \\{\\mathcal{(X_i, y_i)}\\}_{i=1}^N, m, \\gamma \\longrightarrow m^* \\]\nThe good news is we have many variations of stochastic gradient descent which work well for most models and a whole array of more sophisticated techniques when it fails. Think of the transformer architecture which made ChatGPT possible – training would be unfeasible with more “naive” optimization algorithms.\nA hyperparameter is a quantity which governs the behavior of the model and is not directly learned from data, so we’re responsible for making an informed choice or determining it empirically. This process is called hyperparameter optimization.15 If we have a big model which trains for a long time, clearly, we will have to try as few hyperparameters as possible.\n15 There are many tree-based models like LightGBM and Random Forests which are pretty good out of the box, and in practice don’t need much hyperparameter tuningGenerally, this problem is known as optimization of (expensive) black box functions. The key here is to approximate the slow model via a flexible, but much quicker and simpler one, then iteratively pick hyperparameters which will get us closer to optimal values. We will not cover this techniques here, or in introductory data science classes, but you should know about this solution when you encounter such a problem in practice.",
    "crumbs": [
      "Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1_intro.html#stories-and-case-studies",
    "href": "sim/1_intro.html#stories-and-case-studies",
    "title": "Simulation of economic processes",
    "section": "Stories and case-studies",
    "text": "Stories and case-studies\nSo far, you studied probability, statistics, operations research, and economics from an ultimately mathematical and procedural point of view (i.e. steps to solve a problem). This is a solid foundation to have, but it needs to be balanced out with practical aspects of modeling, data analysis, and programming.\nI will be using stories and real world case-studies to highlight the practical relevance of theoretical ideas like Central Limit Theorem (CLT), Law of Large Numbers (LLN), p-values, conditioning, common distributions, etc. Moreover, by simulation, we’ll reinforce our knowledge and understanding, which will help us avoid most common pitfalls in statistical modeling.\nA funny thing is that we can use simulation to better understand probability and statistical inference, and at the same time, probability justifies why simulation works. I can’t emphasize enough how much your learning will improve if you will apply what you learn here in your econometrics, data analysis, and quantitative economics classes. If you do not trust me, trust Richard Feynman, who said: “What I cannot build, I do not understand.”\nYou can think of this course as having two parts. First, we use simulation as a problem-solving approach to gain insight into an economic problem. The second part develops specialized methods for estimation, sampling, approximation, and optimization – which can be viewed as tools to overcome a range of technical problems in statistical modeling.\n\n\n\n\n\n\nThe bare minimum\n\n\n\nSimulation is perhaps the most beginner-friendly course you had, because we need to know just two things to get started.\n\nHow to generate iid, uniformly distributed pseudo-random numbers. This problem is solved, since all programming languages have good RNG (random number generators).16\nHow to generate samples from any probability distribution which is “well-behaved”. Fortunately, a theorem called “Universality of the Uniform” proves that we can and gives us the method for doing so. In R or Python we have access to efficient implementations for the vast majority of distributions we’ll ever need.\n\nThis is not sufficient to understand why simulation works, how to apply it effectively, or how to sample from complicated distributions which don’t have a closed-form solution. However, you can still go a long way in practice with these two simple facts.\n\n\n16 You should still know how are they computed behind the scenes and what happens when they are not-so-randomFirst, we’ll need to set-up the programming environment (R and RStudio), create a script or (quarto) notebook and we’re ready to go! Take your time to understand how to navigate the IDE, run commands, investigate the errors, and read the documentation. We want to solve problems and program, thus techical issues like how to run a line of code or where can I find the output shouldn’t stand in our way.\n\n\n\n\n\n\nThe full-luxury development setup\n\n\n\n\nR v4.4.2 (later than 4.3.x)\nRStudio as our main IDE\nQuarto or Rmarkdown for literate programming (only needed towards the end of the course)\nInstalling tidyverse will get us most needed packages:\n\ndplyr for data wrangling, purrr for functional programming, and stringr / glue for making our life easier with text\nggplot for data visualization\n\n\nR is a beginner-friendly language, but has many gotchas because of its weak data types. Tidyverse is an important ecosystem of packages which solves a lot of the issues in base R and makes our life easier and coding much more pleasant.\nIf you’re really serious about becoming a data scientist or a ML engineer, you will have to study and practice a lot on your own. This is a non-exhaustive list of practical skills you will need in the future.\n\ngit & github for versioning your code and collaborating on a code-base\nrenv for managing environments and packages\nduckdb to practice SQL and data engineering on a analytical, columnar, in-process database\ndata.table and arrow for processing huge amounts of data\nhow to build interactive applications in Shiny and model serving APIs in plumbr\nhow to use the command line interface and automate stuff in Linux\nhow to package and deploy your training and prediction pipelines, possibly to a cloud provider\n\n\n\nWithout further ado, here are the stories and case-studies we’re going to discuss and implement in code, along with theoretical ideas they highlight.\nWe’ll start with a warm-up. Birthday paradox is a straightforward, but counter-intuitive result which is a good opportunity to review key concepts from combinatorics and apply the naive definition of probability. We’ll derive the analytical solution, visualize it, and compare with our simulation. This simple exercise will teach us most things we will need for the near future: how to sample, generate sequences, work with arrays, functions, data frames, and repeat a calculation many times.\n\nNewsvendor problem and inventory optimization is a great case-study of decision-making under uncertainty\nShowing up to a safari. This is a cute story which will teach us about the intuitions behind Binomial distribution, probability trees, independence, and sensitivity analysis\nSimulations which exemplify convergence in probability and the law of large numbers. We’ll discuss why Monte Carlo methods work and what happens if we forget to take into consideration sample size\nUS schools, the most dangerous equation. This is a great example of what can go wrong when we draw conclusions from data via a sloppy analysis.17 CLT is just one of key theoretical ideas from statistics which could’ve prevented the policy makers to start a costly and wasteful project.\nQuality control and measurement error. We’ll discuss the story of Gosset in Guiness beer factories, the original purpose for which the t-test was invented, the philosophy and practice of hypothesis testing. A key idea is one of action in the long run and error control (not being too wrong too often). This perspective of statistics justifies our definition of “changing your actions in the face of evidence”.\nWhat p-values can we expect? Most people misunderstand the idea of p-values. We will use an example of a scientific study and its probability to find a true, significant finding. In simulation, we can see the distribution of p-values, which is impossible to know for sure in practice, even after a rigorous meta-analysis.\nWikipedia A/B test and Landon vs Roosevelt elections. These stories serve as drills so that you remember how to calculate confidence intervals for proportions and interpret them correctly. They also serve as a warning of what can go wrong if we fail to randomize.\nBayes rule, medical testing, and coding bugs. Bayesian thinking and the idea of updating your beliefs is key for rational decision-making under uncertainty. You will also get into the habit of articulating and elliciting your prior knowledge (before seeing the data) about a problem.\nBeta-Binomial model of left-handedness and quality control. This will be perhaps the only time in your bachelor’s degree where you will encounter a fully Bayesian approach to inference (and not just an application of Bayes rule). We will learn what probability distributions are appropriate in modeling proportions and a principled approach to deal with misclassification error.\n\nI will mention how can we model counts of asthma deaths and kidney cancer with the Gamma-Poisson model, and how it can be applied to customer purchasing behavior.\nBy a similar reasoning we’ll see how can we model waiting times in a Starbucks by a Gamma-Exponential model. This is precisely the reason why you studied all of those discrete and continuous distributions in your probability class – they have a purpose and are useful!\n\nLinear regression and confounding. You probably heard a hundred times that correlation doesn’t imply causation. I will show three simple causal mechanisms and graphs of influence which can mislead us into a wrong conclusion: confounders, mediators, and colliders. We’ll discuss real-world studies about divorce rates, gender discrimination, and ethical intuitions.\n\nIn the context of linear regression, we’ll use bootstrap as an alternative way to compute confidence intervals. It’s a must-have technique in your quantitative toolbox, which will be useful any time you don’t have a theoretically proven sampling distribution.\n\nJustifying the sample size for an A/B test. Power calculations is what trips up most product managers and analysts, who end up confused by the online calculators or complicated formulas (which I’ve seen mostly misused). This is where simulation shines and will be helpful in choosing the appropriate sample size of data we need to collect (or how long the experiment should run), while being very clear about our assumptions and expectations.\n\n17 I really like the metaphor of “fooled by randomness”At last, after we get a sense how Markov Chain Monte Carlo works, we will gain a new superpower – to sample from unknown distributions. We can apply it to the modeling of grouped / clustered / correlated data, like in a classical case-study of radon (toxic gas) concentrations in U.S. homes in various counties.\nThe idea of partial pooling will help us to automatically adjust our inferences (means and variances) with respect to the sample size of each group (county). This is an example of a multilevel or hierarchical model, for which Bayesian inference has many benefits. However, we don’t have analytical solutions and will have to use a much more sophisticated algorithm to generate samples from the posterior distribution of parameters.\nTreat this topic of multilevel modeling as the culmination of the course, which also serves as a powerful complementary approach to what you study in econometrics.",
    "crumbs": [
      "Simulation of economic processes"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html",
    "href": "sim/1L_bday_problem.html",
    "title": "Introduction to simulation in R",
    "section": "",
    "text": "In the introduction, I tried to convince you that simulation can be used to understand fundamental concepts in probability and statistics in order to appreciate their practical relevance. By implementing the stories, toy examples, and case-studies in code, we will bridge the gap between theory and practice. Hopefully, this will help you gain confidence and get over the fear of mathematical abstraction in probability and hone your programming skills for data analysis. 1\nWhen having trouble understanding something, do not despair and remember this quote from the great physicist. Do not resort under any circumstance to rote memorization,2 as you will forget it in a few weeks, but instead think like an engineer. Once you build it by yourself, you will understand how a method works and can do it again in the future. At last, I want to tell you two important lessons from Patrick Winston’s talk at MIT, “How to speak”:\nIt might sound obvious, but we practice in a very specific way – with your reference book(s), code editor, and pen / paper at all times. Learn actively, by doing and problem solving, by testing yourself, by trying to come up with your own examples. We cannot afford to rush and take shortcuts when studying mathematics and computer science.3",
    "crumbs": [
      "Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html#the-birthday-problem",
    "href": "sim/1L_bday_problem.html#the-birthday-problem",
    "title": "Introduction to simulation in R",
    "section": "The Birthday Problem",
    "text": "The Birthday Problem\nThis classic probability problem about matching and coincidences will remind you of a few important ideas in combinatorics and will teach you some of the most important elements of R programming.\n\nWhat is the probability that in a room of \\(n=35\\) people, at least two have their birthday on the same day of year? We will assume that every day is equally likely, ignore leap year (\\(k = 365\\)), there are no twins (observations are independent with respect to birthday date).\n\n\n\n\n\n\nLet’s check your intuitions\n\n\n\nFind a colleague near you, discuss for two minutes how many people you need in a room so that the probability of any two people having a matching birthday is close to 0.5.\nThen, the other way around, for \\(n = 35\\), what is the probability of a matching birthday? In what range is your answer? Raise your hand for the appropriate bucket. Abstain if you know the exact answer from before.4\n\\[[0, 0.2), [0.20, 0.4), [0.4, 0.6), [0.6, 0.8), [0.8, 1]\\]\nThen, answer how appropriate are the assumptions we’re making – is it good enough for an approximation?\nAt last, think of boundary conditions: what is the probability for 365 and 400 people? Can you figure out the answer for 2 people?\n\n\n4 It’s important to realize that knowing the answer is not important, but the reasoning behind itWe solve this problem via simulation first and will check our answers versus the analytical solution. This is an important lesson – when you’re stuck on a problem, it’s a good idea to simulate it first and find the numerical answers, which will give us ideas about how to solve it mathematically. In reality, we need both, as a careful mathematical analysis will result in more insights about the properties of a problem or construct and will show us how to generalize to more complicated cases.\n\n\n\n\n\n\nSurprising applications\n\n\n\nSometimes, a simple problem like this can have surprising applications and provide valuable insights / tools in different, totally unrelated domains.\nThe birthday “paradox” is the simplest way to compute probability of coincidences. This is useful in cryptography (where matching hashes are a bad thing), satellite collisions with space junk, false DNA / genomic matches. A related problem is about partitions, where you try to balance weights on two scales – which is relevant in matchmaking systems, for example, in games like Dota2.\nSimilarly, simple urn models resulting from basic combinatorics and probability, were successfully applied by physicists like Bose, Einstein, Fermi, Botzmann to describe the large-scale behavior of different particles and physical phenomena. This is the power and elegance of choosing the right model for the right task.\n\n\n\nSimulation in base R\nWithout further ado, let’s simulate and solve the problem in the simplest possible way. Consult with your professor about how to arrive at this solution step-by-step. We will use the replicate function a lot in this course. I suggest you learn about vectorized operations like sapply or purrr::map as an elegant and performant way to replace for loops.\n\nset.seed(15317)  # has to be a larger odd number. why?\nnr_people &lt;- 35  # even integers and floats are vectors of size one!\n\nsimulate_birthdays &lt;- function(nr_people, nr_sim = 10000) {\n    birthday_events &lt;- replicate(n = nr_sim, {\n        birthdays &lt;- sample(x = 1:365, size = nr_people, replace = TRUE)\n        anyDuplicated(birthdays) &gt; 0  # returns for each sim\n    })\n    mean(birthday_events)  # this returns implicitly!\n}\n\npr_same_bday &lt;- simulate_birthdays(nr_people)\nbday_match_size &lt;- sapply(2:90, simulate_birthdays, nr_sim = 10000)\n\n\n\nThis little example teaches you about data types, random number generation, variable assignment, functions and their arguments, implicit returns, sampling with replacement, iteration without using for lops, operations and applying functions on vectors.\nA quick check of the result shows that the probability of n = 35 people having a matching birthday is 0.81. Is this close to your intuitive answer? Now, let’s visualize the results for a range of relevant n’s. How would you expect the curve to look like?\n\n\nShow the visualization code\nplot(\n    x = 2:90, y = bday_match_size, \n    type = \"l\", lty = 1, lwd = 5,\n    xlab = \"number of people\", ylab = \"probability\",\n    main = \"Probability of matching birthdays\",\n    xlim = c(0, 80), ylim = c(0, 1), # Limits\n    panel.first = abline(h = seq(0, 1, 0.2), col = \"grey80\")\n)\nsegments(\n    x0 = nr_people, y0 = bday_match_size[nr_people - 1], \n    x1 = nr_people, y1 = 0, \n    lty = \"dashed\"\n)\nsegments(\n    x0 = 2, y0 = bday_match_size[nr_people - 1], \n    x1 = nr_people, y1 = bday_match_size[nr_people - 1], \n    lty = \"dashed\"\n)\n\n\n\n\n\nIt takes a lot of effort to make a nice visualization in base R. This is why we will switch very soon to the tidyverse and ggplot grammar of graphics\n\n\n\n\n\n\nTry to keep the visualization as simple as possible. Don’t get lost into all the graphical parameters at the beginning, it will take time and practice to make beautiful graphs\nOne reason for this counterintuitive result is that probability grows relative to the number of possible pairings of people, not just the group’s size. For a group of 23, we’ll have \\(23 \\cdot 22 / 2 = 253\\) unique pairs of people. As in other applications of probability, having an intuition helps, but we can’t afford to be clever – we’ll get to the correct answer only by ruthless application of the rules of probability.\n\n\nAnalytical solution\n\n\nJoseph K. Blitzstein, Jessica Hwang - Introduction to probability, 2nd ed, 2019. (pg. 19-20)\nBy the multiplication rule, there are \\(365^n\\) ways to assign birthdays to the people in the room (sampling with replacement). First, you have to recognize that it is unfeasible to solve this problem directly via the inclusion-exclusion principle. Therefore, a useful trick in many problems is to focus on the complement: \\(P(A) = 1 - P(A^C)\\)\nCounting the number of ways in which we can assign birthdays to n people so that they don’t share a birthday is equivalent to sampling without replacement. Think of the metaphor of putting k flags on n poles. 5\n5 You might know the numerator as “Aranjamente”, but I like the idea of falling factorial and just remembering that we speak of ordered sampling without replacement\\[\n\\mathbb{P}(A^C) = \\frac{365 \\cdot 364 \\cdot ... \\cdot (365 - n + 1)}{365^n}\n\\]\nIn the original formulation of the problem, the question is how many people do we need for the probability to be close to 0.5. Let’s compute the analytical solution, visualize it, and use a simple base R mechanism for selecting / filtering the rows of interest.\n\nn_grid &lt;- seq(2, 70, by = 1)  # alternative to 2:70\n\n# notice the anonymous function notation \\(x) \nprob &lt;- sapply(n_grid, \\(x) 1 - prod(366 - 1:x) / 365^x)\nsim  &lt;- data.frame(nr_people = n_grid, prob = prob)\n\nsim[\"diff\"] &lt;- abs(0.5 - sim$prob)\nprob_half   &lt;- sim[which.min(sim$diff), ]\nprob_half\n\n   nr_people      prob        diff\n22        23 0.5072972 0.007297234\n\n\nIgnore the code for the following visualization, as it is unnecessarily complicated – I’m just showing off. 6 By this, I want to emphasize that a persuasive visualization is a very important practical skill and that it’s not easy to do in base R. So, in order not to suffer and achieve the same result much easier, we will have to use tidyverse and ggplot.\n6 treat it as a documentation of the things that can be customized in base R plots\n\nShow the visualization code\npar(# mar = c(3, 3, 3, 3),  # Dist' from plot to side of page\n    mgp = c(3, 1, 0),       # Dist' plot to label\n    las = 1,                # Rotate y-axis text\n    tck = -.01,             # Reduce tick length\n    xaxs = \"i\", yaxs = \"i\"  # Remove plot padding\n) \n\nplot(\n    x = sim$nr_people, y = sim$prob, type = \"l\", \n    ylab = \"probability\", xlab = \"number of people\", lwd=3,\n    main = paste0(\n        \"Only \", prob_half$nr_people, \" people for a \", \n        round(prob_half$prob * 100, 1), \"% chance of matching bday\"\n    ),\n    axes = FALSE, # Don't plot the axes\n    frame.plot = FALSE, # Remove the frame\n    xlim = c(0, 70), ylim = c(0, 1), # Limits\n    panel.first = abline(h = seq(0, 1, 0.25), col = \"grey80\")\n)\n\nsegments(\n    x0 = prob_half$nr_people, y0 = 0, \n    x1 = prob_half$nr_people, y1 = prob_half$prob,\n    lty=\"dashed\"\n)\nsegments(\n    x0 = 0, y0 = prob_half$prob, \n    x1 = prob_half$nr_people, y1 = prob_half$prob, \n    lty=\"dashed\"\n)\nat &lt;- pretty(sim$nr_people)\naxis(side = 1, at = at, col = \"grey40\", line = 1, cex = 0.7)\nat &lt;- seq(0, 1, 0.25)\nmtext(side = 2, text = at, at = at, col = \"grey40\", line = 1, cex = 0.9)\n\n\n\n\n\n\n\n\n\n\n\nSimulation with tidyverse\nFinally, let’s see another way of solving the same problem using the tidyverse. It takes some time to get used to the pipelines and working within data frames, but it is a much nicer way of working in R. It almost looks like SQL in its semantics.7\n7 I highly recommend Hadley Wickham’s R for Data Science free and open-source book. It will take a week or so of intensive study to go through it, but you can benefit a decade ahead from the very practical skills you will acquire.\nlibrary(tidyverse)\nlibrary(glue)\n\nsim_tidy &lt;- tidyr::crossing(\n        people = seq(2, 70, by=1), \n        trial  = 1:10000,\n    ) |&gt;\n    dplyr::mutate(\n        birthday = purrr::map(people, \\(x) sample(365, x, replace = TRUE)), \n        multiple = purrr::map_lgl(birthday, \\(x) any(duplicated(x)))\n    ) |&gt;\n    dplyr::group_by(people) |&gt;\n    dplyr::summarise(chance = mean(multiple)) |&gt;\n    dplyr::mutate(analytical_solution = \n        purrr::map_dbl(people, \\(x) 1 - prod(366 - 1:x) / 365^x)\n    )\n\nprob_half_tidy &lt;- sim_tidy |&gt; mutate(diff = abs(0.5 - analytical_solution)) |&gt;\n    arrange(diff) |&gt;\n    head(1)\nprob_half_ppl &lt;- prob_half_tidy |&gt; pull(people)\nprob_half_tidy\n\n# A tibble: 1 × 4\n  people chance analytical_solution    diff\n   &lt;dbl&gt;  &lt;dbl&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1     23  0.518               0.507 0.00730\n\n\n\nggplot(sim_tidy) +\n    geom_line(aes(people, chance)) +\n    geom_line(aes(people, analytical_solution), lty=\"dashed\", color = \"darkred\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        title = glue(\"Only {prob_half_ppl} people for a 50% chance of matching bday\"),\n        subtitle = \"Analytical vs simulated solution\",\n        y = \"probability\", x = \"number of people\"\n    ) + \n    theme_minimal()\n\n\n\n\nNotice how we got a visualization of a similar quality with much less code",
    "crumbs": [
      "Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "sim/1L_bday_problem.html#homework-and-study-resources",
    "href": "sim/1L_bday_problem.html#homework-and-study-resources",
    "title": "Introduction to simulation in R",
    "section": "Homework and study resources",
    "text": "Homework and study resources\nThe homework in this course has reading and coding assignments. Your readings will mostly be stories, case-studies, and papers published in academic journals – there won’t be much theory.\n\nIntroduction and background. Understand why simulation and numerical methods are important and how this class fits in a larger context of decision science. ~30min\nWhy did you study all of that? You will find an explanation of how the subjects you studied before are helpful in practice and what are their main idea. ~15min\nRead this short explanation on the differences between probability and statistics ~5min\nWatch this video of Santosh S. Venkatesh showing the vast array of practical applications of probability theory ~5min\n\nBesides this one hour of light readings, you should spend some time to install R, RStudio, learn how to run commands, where to look for output, documentation, and errors; how to create and save a script, how to install packages, load them and check if they installed successfully. 8\n8 Optional! You can attempt to generalize the birthday paradox to an arbitrary k and 3 or 4 matches. You can check out this post in SAS, but think for yourself how would you implement it in RIn order to make sure you’re comfortable with RStudio, try to reproduce the analysis of the 2022 Australian elections, in section 2.2 of “Telling stories with data”. You will not understand what each line of code does, but will get a sense of what we will do for the remainder of the course.",
    "crumbs": [
      "Simulation of economic processes",
      "1L. Birthday problem"
    ]
  },
  {
    "objectID": "sim/3L_CLT.html",
    "href": "sim/3L_CLT.html",
    "title": "The most dangerous equation",
    "section": "",
    "text": "During the third lecture, I did a review of three theorems you have to know and be aware of their practical implications. We used simulations to aid our mathematical understanding of CLT (central limit theorem), LLN (weak law of large numbers), and universality of uniform.1\nWe used the latter’s inverse method to generate exponential random variables, discussed how Gamma, Beta can be generated from it and how are they used in modeling. Similarly, we leveraged the relationship of Normal to distributions commonly used in statistics like Chi-squared, Student-t. This exercise allowed us to better understand what (implicit) assumptions are we making when working with these distributions.2\nIn this lab, we’ll apply the CLT to avoid common pitfalls when reasoning about an outcome under different sample sizes, use the LLN for numerical integration, and discuss in more technical detail about how can we generate random samples from common (and arbitrary) distributions.\nHoward Wainer chose to nominate the sampling distribution of the sample mean (de Moivre, CLT) as the most dangerous “equation”, because of how long it causes confusion, how widespread it is, and the serious consequences such ignorance has caused. He gives examples of:\nThe case-study we’re going to investigate today is about “The small schools movement” in the late 1990s U.S. You can find the full story and bibliographical references in the article, but I’ll present the short version.",
    "crumbs": [
      "Simulation of economic processes",
      "3L. Most dangerous equation"
    ]
  },
  {
    "objectID": "sim/3L_CLT.html#should-u.s.-split-large-schools",
    "href": "sim/3L_CLT.html#should-u.s.-split-large-schools",
    "title": "The most dangerous equation",
    "section": "Should U.S. split large schools?",
    "text": "Should U.S. split large schools?\nAs a result of heavy urbanization, the schools became bigger, more concentrated, and with more specialization in teaching. In the 90’s there was a growing dissatisfaction with the state of education, which still holds today, which lead to some initiatives, including Bill/Melinda Gates’ foundation financing5 and pushing for more small schools.\n5 Order of magnitude of $ billions, just from the GatesThis idea was picked up by other organizations and sometimes resulted in million-dollar efforts of splitting large schools (e.g. over 1000 students) in smaller ones.\n\n\n\n\n\n\nStop and think: what are the hypotheses?\n\n\n\nFind your closest colleague and discuss for 5 minutes what could’ve been the assumptions, hypotheses, reasoning, and evidence based on which these organizations concluded that it’s a good thing to split the big schools or encourage the creation of small ones.\n\n\nFor a series of papers and articles, Wainer collected data on around 1600 schools from the PSSA (Pennsylvania testing program), which has scores on a variety of subjects and broad sample of schools.6 Indeed, the small schools were over-represented in the top 50 schools according to the test scores (3% in sample vs 12% in the top). Could this be the empirical “evidence” that Gates foundation confirmed their beliefs with? Yes, I’m talking about confirmation bias, since this is a heavily loaded and divisive political topic.\n6 If you plan to make a project or research paper on education, open datasets like PSSA will be invaluable, but it takes a lot of effort to clean it up and put it all together for modelingBefore loading and analyzing some data, let’s first do a simple simulation in which we exaggerate the differences in sample size (number of students in small \\(n_s\\) vs big schools \\(n_b\\)). We will generate individual scores from the normal distribution, which is not unreasonable, as most standardized tests are designed precisely that way.\n\nlibrary(tidyverse)\nset.seed(1234)\n\n\n\nShow simulation code\nnr_stud_small &lt;- 200; nr_small_schools &lt;- 500\nnr_stud_big &lt;- 400; nr_big_schools &lt;- 100\n\nsimulate_schools &lt;- function(n_small, n_stud_small, n_big, n_stud_big) {\n    small_scores &lt;- replicate(n_small, { rnorm(n_stud_small) |&gt; mean() })\n    big_scores   &lt;- replicate(n_big,   { rnorm(n_stud_big)   |&gt; mean() })\n    \n    df &lt;- bind_rows(\n        tibble(\n            scores  = small_scores, \n            nr_stud = n_stud_small, \n            school_type = \"small\"\n        ),\n        tibble(\n            scores  = big_scores, \n            nr_stud = n_stud_big, \n            school_type = \"big\"\n        ) \n    )\n    df\n}\n\ndf &lt;- simulate_schools(\n    nr_small_schools, nr_stud_small, \n    nr_big_schools, nr_stud_big\n)\n\ndf |&gt; \n    ggplot(aes(x = scores, color = school_type)) + \n    geom_density(linewidth = 1) + \n    labs(x = \"Average standardized scores\") + \n    theme_minimal()\n\n\n\n\n\nNotice that small schools not only dominate the top of performance, but also the bottom. This is not surprising due to CLT, even when we have drawn from the same \\(N(0, 1)\\) at individual level.\n\n\n\n\n\n\nNotice that we even assume the same population variance \\(\\sigma^2\\), which might not be true in other practical applications when we compare groups.\nNow, let’s look at some data from Brazil’s ENEM scores, which is the equivalent of SAT in U.S. or BAC in Romania. You can find instructions on how to download it from Chapter 3 of Matheus Faucre’s “Causal Inference for the Brave and True” or download it from my github repo.\n\ndf_scores &lt;- readr::read_csv(\"data/enem_scores.csv\") |&gt; \n    mutate(\n        date = as.Date(paste0(year, \"-01-01\")), \n        school_id = as.factor(school_id)\n    )\n\ndf_scores |&gt;\n  arrange(desc(avg_score)) |&gt; \n  head(10)\n\n# A tibble: 10 × 5\n    year school_id number_of_students avg_score date      \n   &lt;dbl&gt; &lt;fct&gt;                  &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;    \n 1  2007 33062633                  68      83.0 2007-01-01\n 2  2007 33065403                 172      82.0 2007-01-01\n 3  2005 33062633                  59      81.9 2005-01-01\n 4  2005 33065403                 177      81.7 2005-01-01\n 5  2007 29342880                  43      80.3 2007-01-01\n 6  2007 33152314                  14      79.8 2007-01-01\n 7  2007 33065250                  80      79.7 2007-01-01\n 8  2007 22025740                 144      79.5 2007-01-01\n 9  2007 31311723                 222      79.4 2007-01-01\n10  2007 33087679                 210      79.4 2007-01-01\n\n\n\n\n\n\n\n\nDistribution of the number of students. Most schools have less than 200 students.\n\n\n\n\n\nShow visualization code\ndf_scores |&gt;\n    filter(year != 2005) |&gt;\n    group_by(school_id) |&gt;\n    summarise(\n        number_of_students = mean(number_of_students, na.rm = TRUE),\n        avg_score = mean(avg_score, na.rm = TRUE)\n    ) |&gt;\n    mutate(\n        performance = case_when(\n            avg_score &gt; quantile(avg_score, 0.975) ~ \"top\",\n            avg_score &lt; quantile(avg_score, 0.225) ~ \"bottom\",\n            .default = \"middle\"\n        )\n    ) |&gt;\n    ggplot(aes(number_of_students, avg_score)) +\n    geom_point(aes(shape = performance, color = performance)) +\n    geom_smooth(method = \"gam\", se = TRUE, color = \"dodgerblue3\") + \n    scale_shape_manual(values = c(4, 1, 8)) + \n    scale_color_manual(values = c(\"coral\", \"grey40\", \"skyblue\")) + \n    theme_minimal() + \n    labs(\n        x = \"Number of students\",\n        y = \"Average Score\", \n        title = \"ENEM Scores by School Size\",\n        subtitle = \"Fooled by small sample sizes\"\n    )\n\n\n\n\n\nNotice the pattern we’re interested in, but also the fact that we have many confounders we don’t take into account, like school location, funding, number of teachers, if it’s private or public, etc. Therefore, we can’t know from this data what causes better performance – but we know it’s not the school size!\n\n\n\n\n\n\n\n\n\n\nDistribution of average scores in 2005 tells us that there are important factors we didn’t take into account.\n\n\n\nThere are better ways to visualize this data, for example, we can split the x-axis (nr of students) into buckets, then display a boxplot or error-bars for each one, along with some of the outliers. We could also color the data points with a contrasting, continuous gradient and not discretize the scores.\n\nc(rnorm(2000, mean = 43, sd = 6), \n  rnorm(500, mean = 65, sd = 6)) |&gt; \n    hist(\n        col = \"skyblue\", border = \"white\", breaks = 40,\n        main = \"Example of a mixture of two distributions\", \n        xlab = \"ENEM Scores\"\n    )\n\n\n\n\nWhen a single distribution is not sufficient in describing the outcomes, there might be a latent mixture or heterogeneity in your population.\n\n\n\n\n\n\n\n\n\n\nTry to make your own simulation\n\n\n\nAfter analyzing the data from Brazil, we got more information about the distribution for number of students in schools and the distribution of average scores.\nUse that knowledge to make your own simulation with more realistic assumptions. For example, you will see below how we figure out which distribution is appropriate for the number of students per school.7\nMore specifically, you will now generate the number of students for a lot of schools (from the negative binomial), and for each school a vector of scores (from the mixture of gaussians) which you will summarize by taking the average. Will this lead to more realistic results?\n\n\n7 But how did I pick the negative binomial? Knowledge and experience of distribution’s stories and their properties.\n\nShow code for fitting data to the negative binomial\nnr_stud &lt;- df_scores |&gt; filter(year == 2005) |&gt; \n    sample_n(2000, replace = TRUE) |&gt; pull(number_of_students)\nnegbinom_params &lt;- MASS::fitdistr(nr_stud, \n    \"negative binomial\", method = \"SANN\")$estimate\n\nrnbinom(10000, size = negbinom_params[\"size\"], mu = negbinom_params[\"mu\"]) |&gt;\n    hist(breaks = 40, col = \"skyblue\", border = \"white\", \n         xlab = \"number of students\", main = \"Negative Binomial fitted to data\", \n         xlim = c(0, 700))\n\n\n\n\n\nNegative binomial captures well the asymmetric distribution of students and is appropriate for the task, because its support is non-negative integers (or counts).\n\n\n\n\n\n\nHere are the parameters we obtained by fitting the distribution to the data: 1.75 for size \\(n\\) and 139.7 for \\(\\mu\\).\nYou should take away one big lesson from this case-study – that we should always remember the consequences of sample size and its variability. By using basic statistics and simulations, we can prevent mistaken conclusions from a naive data analysis and getting fooled by randomness. You might point out that this example is slightly trivial, but trust me, in practice the pitfalls become much more subtle.\n\n\nSome pitfalls include sampling bias, differential non-response, dropout from study, right censoring, via confounders, mediators, or colliders.\nThese kinds of simulations have one more advantage, as we can check if different causal assumptions can generate the kinds of patterns we observe – even if we haven’t collected the data to verify it. You will get used to the situation when two different process models result in similar statistical patterns – which will force you to think very carefully about the underlying causal process in practice.",
    "crumbs": [
      "Simulation of economic processes",
      "3L. Most dangerous equation"
    ]
  },
  {
    "objectID": "sim/3L_CLT.html#drawing-samples-from-distributions",
    "href": "sim/3L_CLT.html#drawing-samples-from-distributions",
    "title": "The most dangerous equation",
    "section": "Drawing samples from distributions",
    "text": "Drawing samples from distributions\nIn the lecture, we saw how important is universality of the uniform and inverse method to simulation. I also presented the stories, use-cases, and relationships of exponential, gamma, beta, normal, chi-squared, and student-t distributions.\nToday we introduced the negative binomial, which can be represented as a mixture of gamma and Poisson. It was helpful in capturing the skewed distribution of the number of students in Brazil’s schools. In practice, you might encounter other, more exotic distributions like Weibull, inverse-gamma, Dirichlet, and Wishart – which have their own specialized roles and purposes.8\n8 Weibull is used in survival modeling, Inverse-Gamma as a prior for the variance, Dirichlet in modeling proportions, and Wishart for generating random, symmetric covariance matricesHowever, if you understand well the standard ones that I presented, you’re all good for the majority of applied problems. We will encounter each one in the context of the appropriate case-study and explain their properties in more detail, like we did with the binomial.\n\n\n\n\n\n\nMethods for generating random samples\n\n\n\nWe’re still ignoring the details of how do we generate pseudo-random numbers from the uniform, but it will be useful to summarize what methods do we have for other distributions.\n\n\n\n\ngeneral\nspecific\n\n\n\n\ndirect\ninverse method\ncustom algorithms\n\n\nindirect\naccept-reject\nmixtures\n\n\n\nWe have used the inverse method to simulate exponential random variables and a mixture for the remaining ones. The prior is a direct and general method, while the latter is an indirect and specific way, when theory tells us it can be represented by a mixture.\nBoth have a big downside of not being the most computationally efficient way, but are transparent and interpretable. Therefore, many software implementations, like R or Python’s RNG, often use specific algorithms for each distribution – which is more of a numerical methods problem.\n\n\nAt last, in statistical modeling we will sometimes need to simulate from really complicated random variables, which might not even have a closed-form representation, but we know the functional form up to a multiplicative constant.9 Thus, we’re out of luck with the direct and specific methods – but accept-reject algorithm and its generalizations in importance sampling are to the rescue in such situations.\n9 Often, that constant is a nasty integral which we can’t solve by hand or at least it’s not practical to do so\n\n\n\n\n\nNot covered on this website\n\n\n\nThese lectures and labs do not cover at all the details of how to draw random numbers via the inverse method and specific indirect methods.10 Please, consult with your professor about what is exactly required in the course from these topics.\nBut what can I recommend for sure if you need a deep-dive on the inverse method for discrete and continuous distributions, is this extensive tutorial by Alexandru Marioarei. Study and replicate the code for one or two distributions by using a different method, for example: Hypergeometric, Normal, Poisson (inverse and custom).\nRegarding the pseudo-random number generation algorithms, you can check out Chapter 5 of Niels Hansen’s “Computational Statistics with R” and Chapter 3 of Tom Kennedy’s “Monte Carlo Methods - a special topics course”.\n\n\n10 The reason for this is that I don’t have anything new or interesting to say hereWhile we can be ok with not being experts at the inverse method and efficient simulation of random variables, we will be missing a lot if we don’t have an understanding of the accept-reject approach. This is because it’s a building block in a sequence of increasingly complicated and powerful sampling methods like Importance Sampling, Metropolis-Hastings, Markov Chain Monte Carlo, Hamiltonian Monte Carlo, and Particle Filtering (Sequential Monte Carlo).\nWithout further ado, let’s look at the simplest form of Accept-Reject algorithm, a naive implementation, and apply it to an example.11 We’ll get the chance to analyze why it works and how to make it more efficient in more detail in future lectures.\n11 This is an extension of the Beta example in Casella and Robert, pg. 53\n\n\n\n\n\nCalibrating a classification model\n\n\n\nImagine you have developed a machine learning model which scores your customer’s propensity to unsubscribe in the next month (churn). You know that one group is loyal and another one at high risk of going to your competitors.\nThe problem is that your ML model gives you scores, not calibrated probabilities, and you can’t interpret them as such for downstream decisions (like offering a promotion). So, you leave aside a validation dataset, in which you count the number of scores / predictions for each bucket / decile of true frequency of unsubscription.\nWhat you need now is a model which maps the scores to probabilities. In practice, you would use something like an Isotonic Regression or a Mixture Model, but let’s not ruin a good story.\n\n\nSo, before running the calibration, you would like to know if your method works in principle, on some simulated data. First, you need to draw samples from the mixture model below and luckily we can do it in R with 0.6 * rbeta(10000, 2, 6) + 0.4 * rbeta(10000, 18, 3). But what would we do if we didn’t have it? This seems a bit too intimidating for the inverse method.\n\\[\nf(x) = w Beta(x | a_l, b_l) + (1 - w)Beta(x|a_r, b_r)\n\\]\n\n\nShow code for Accept-Reject method\nset.seed(16173)\nnr_sim &lt;- 1e4; g_a &lt;- 0.7; g_b &lt;- 0.4\n\nf &lt;- function(x) { 0.6 * dbeta(x, 2.1, 6.3) + 0.4 * dbeta(x, 18, 3) }\n\nM &lt;- optimize(\n    f = function(x) {f(x) / dbeta(x, g_a, g_b)}, \n    interval = c(0, 1), maximum = TRUE\n)$objective + 0.02\n\nu &lt;- runif(nr_sim)\ny &lt;- rbeta(nr_sim, g_a, g_b)            # generation from g\nx &lt;- y[u &lt; f(y) / (dbeta(y, g_a, g_b) * M)] # accepted subsample\n\nhist(\n    x, breaks = 30, probability = TRUE, \n    xlab = \"x (proportion)\", main = \"\", \n    col = \"skyblue\", border = \"white\", \n    xlim = c(0, 1), ylim = c(0, 3.1)\n)\ncurve(f(x), add = TRUE, lwd = 2, col = \"coral\")\ncurve(dbeta(x, g_a, g_b), add = TRUE, col = \"grey30\", \n      lwd = 2)\n\n\n\n\n\nThe histogram is the generated samples, the red curve the theoretical density, and the black curve is the instrumental distribution g. The proportion of accepted samples is 35%, which is not to bad for the weird distribution we have chosen.\n\n\n\n\n\n\nThis story is not that far from reality. If we manage to find the parameters of the mixture components which fit the data well - for any ML model score we can assign its probability.\nWhat does the code above do? For the Accept-Reject algorithm, we need a lot of uniformly distributed samples \\(U\\), an instrumental (proposal) probability density function \\(g(x)\\), which has the same support as our unknown PDF \\(f(x)\\). We should pick a \\(g()\\) with available simulation methods and which covers the support well.\nWe could choose the uniform, but I settled on a \\(Beta(0.7, 0.4)\\) in order to make sure we have enough “attempts” at the extremes (near 0 and 1). We generate the same number of \\(Y\\) from \\(g\\) and accept only those which satisfy the following condition:12\n12 You will see more performant and sophisticated implementations, but I think it’s better to start as simple as possible\\[\nU \\le \\frac{1}{M} \\frac{f(Y)}{g(Y)}\n\\] The constant \\(M\\) should be an upper bound on the ratio of \\(f(x) / g(x) \\le M \\quad \\forall x\\). I chose the approach of Casella and Robert, who find the maximum of that ratio with optimize(), but we would lose just in efficiency, not accuracy if we picked a larger number.",
    "crumbs": [
      "Simulation of economic processes",
      "3L. Most dangerous equation"
    ]
  },
  {
    "objectID": "sim/3L_CLT.html#monte-carlo-integration",
    "href": "sim/3L_CLT.html#monte-carlo-integration",
    "title": "The most dangerous equation",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nOne of the immediate applications of the law of large numbers that I mentioned in the lecture was Monte Carlo integration, which is an alternative to traditional numerical methods. In the case our integrals have the following form and we can easily simulate from the density \\(f(x)\\), we’re in luck.13 Otherwise, we might need to resort to indirect and more advanced methods like Importance Sampling.\n13 You can read in this Computational Statistics course why this form might not be that restrictive after all.\\[\n\\mathbb{E}_f[h(X)] =  \\int_\\chi h(x) \\cdot f(x) dx\n\\]\n\\[\n\\bar h_n = \\frac{1}{n} \\sum_{j=1}^n h(x_j)\n\\]\nBut first, as with any approximation method, we need a way to assess its accuracy. In our case, we can give probabilistic confidence intervals via the central limit theorem. More interesting are the concentration inequalities (remember Hoeffding from the lecture), which answer the question of how large \\(n\\) has to be if I want my error to be less than a threshold \\(\\tau\\) (e.g. \\(\\epsilon \\le 10^{-4}\\)) with probability 0.999?14\n14 You can check out this question in more details in Chapter 7 of Niels Hansen’s “Computational Statistics with R”\n\nShow code for the convergence of a Gamma r.v.\nset.seed(12321)\n\nn &lt;- 1000\nx &lt;- rgamma(n, 8)\nmu_hat &lt;- cumsum(x) / 1:n # cumulative average\nsigma_hat &lt;- sd(x)\n\nggplot(mapping = aes(1:n, mu_hat)) +\n    geom_ribbon(\n    mapping = aes(\n        ymin = mu_hat - 1.96 * sigma_hat / sqrt(1:n), \n        ymax = mu_hat + 1.96 * sigma_hat / sqrt(1:n)\n    ), fill = \"gray\") +\n    coord_cartesian(ylim = c(6, 10)) +\n    geom_line() +\n    geom_hline(yintercept = 8, lty = \"dashed\") +\n    labs(x = \"number of samples\", y = \"sample mean\") + \n    theme_minimal()\n\n\n\n\n\nConfidence bands for a Gamma random variable via CLT. The dashed line is the theoretical value of the mean\n\n\n\n\nDespite the usefulness of integration in finance, economics, statistics, and natural sciences, it is hard to avoid didactic examples in a simulation course. I will spare you from estimating the constants \\(\\pi\\), \\(e\\) or from wild integrals for which it’s not clear what is the story. So, let’s look at this example from Casella and Robert, then come back to the topic of integration in the context of statistical modeling and MCMC.15\n15 They also give a nice example of Gaussian CDF \\(\\Phi\\) and normal-Cauchy Bayes estimator\\[\n\\int_0^1 [cos(50x) + sin(20x)]^2\n\\]\n\nset.seed(12321)\n\nnr_sim &lt;- 1e4\nh &lt;- function(x) { (cos(50*x) + sin(20*x))^2 }\nx &lt;- runif(nr_sim) |&gt; h()\nest_int &lt;- cumsum(x) / 1:nr_sim\nest_err &lt;- sqrt(cumsum((x - est_int)^2))/1:nr_sim\n\n\ncurve(h, xlab=\"\", ylab=\"\", lwd=3)\n\n\n\n\n\nThis is how composing \\(sin()\\) and \\(cos()\\) can result in any kind of signal. This idea of decomposition is used in fourier transforms and has been applied in about any engineering and technology application you can imagine.\n\n\n\n\n\nShow visualization code\ny_lims &lt;- mean(x) + 20*c(-est_err[nr_sim], est_err[nr_sim])\n\nggplot(mapping = aes(1:nr_sim, est_int)) +\n    geom_ribbon(\n    mapping = aes(\n        ymin = est_int - 1.96 * est_err, \n        ymax = est_int + 1.96 * est_err\n    ), fill = \"gray\") +\n    lims(y = y_lims) +\n    geom_line() +\n    labs(x = \"number of samples\", y = \"Integral evaluation\") + \n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe won’t cover differential equations solvers\n\n\n\nIn some fields, systems of ordinary differential equations (and not only) are tremendously important. You encountered some simple ones in your Cybernetics classes and will study the dynamics of economic models in your third year class.\nAt this point, you should know that there are reliable numerical solvers in R, Python, and other languages. They use methods like 4th order Runge-Kutta for ODEs and Euler-Maruyama for stochastic differential equations.",
    "crumbs": [
      "Simulation of economic processes",
      "3L. Most dangerous equation"
    ]
  },
  {
    "objectID": "sim/3L_CLT.html#homework-and-study-resources",
    "href": "sim/3L_CLT.html#homework-and-study-resources",
    "title": "The most dangerous equation",
    "section": "Homework and Study Resources",
    "text": "Homework and Study Resources\nThe purpose of this week’s homework and readings is to get a deeper understanding of the law of large numbers, central limit theorem, and probability distributions.\n\nRead the lecture about three core theorems in simulation: LLN, CLT, and Universality of Uniform. Run the code snippets you encounter as a warm-up for the following exercises.\nRead this famous article by Howard Wainer - “The most dangerous equation” (Princeton university press, 2009)\nDo your own simulation based on Brazil’s ENEM scores by school, using the gaussian mixture and negative binomial distribution.\nTake the list of other examples from the beginning of this lab and Wainer’s paper, write a paragraph for each one on how would you do simulations to assess the claims and where could you find the data to verify your assumptions",
    "crumbs": [
      "Simulation of economic processes",
      "3L. Most dangerous equation"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html",
    "href": "01_fundamentals/prerequisites.html",
    "title": "Why did you study all of that",
    "section": "",
    "text": "Why did we have to go through all those excruciating months and years doing mathematical analysis, linear algebra, probability, statistics, econometrics, operations research, and lots of economics? What about computer science and low-level programming in C? What about more abstract subjects like cybernetics and game theory?\nIt was very frustrating for me, because it wasn’t clear how they fit together, what is the common thread, and more importantly – what part of the theory is relevant. It would’ve been great to know what works well in practice and why. I will try to be as clear as possible which ideas are helpful in solving the kind of problems in businesses we’re interested in, and which ones are designed to enhance our conceptual understanding.\nLet’s draw a map, stop at each field and in a sentence explain why we learned it and how it contributes to decision science, ML, and AI. I mentioned from the very beginning that decision science is an applied, quantitative, and interdisciplinary field, but it is not just an union of those subjects – the inspirations and tools are quite carefully picked.\nflowchart TD\n  LA[Linear Algebra] --&gt; OR[Operations Research]\n  MA[Mathematical Analysis] --&gt; OR\n  MA --&gt; SD[Systems Dynamics]\n  %% CS[CS Algorithms] --&gt; OR\n  \n  PT[Probability] --&gt; MS[Statistics] --&gt; EC[Econometrics]\n  EC --&gt; Caus[/Causal Inference\\]\n  EC --&gt; TS[Time Series]\n\n %% subgraph 1\n  Caus --- DM[/Data Mining\\] --- ML[/Machine Learning\\] \n  ML --- Caus\n %% end\n\n  OR --&gt; ML\n  MS --&gt; ML\n  MA --&gt; PT\n  SD --&gt; Caus\n\n  Caus --- Econ[[Economics]]  \n  Econ --- GT[Game Theory]\n  Econ --- DT[Decision Theory]\n\n  style Caus fill:#f7f5bc\n  style ML fill:#f7f5bc\n  style DM fill:#f7f5bc\n\n  DM --- FSDA[/Full-Stack Apps\\]\n  FSDA --- DB[Databases/SQL]\n  FSDA --- OOP[OOP]\n  Econ --- TS\n\n  style FSDA fill:#f7f5bc\n\n\n\n\nFigure 1: Think of this as a stuctural organization of the fields and courses you studied before. Some are more useful in analytics, some in ML and some in making causal inferences, that is, based on data + theory. This diagram is organized around those three central pillars and perspectives.",
    "crumbs": [
      "Appendix",
      "1. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#fundamentals",
    "href": "01_fundamentals/prerequisites.html#fundamentals",
    "title": "Why did you study all of that",
    "section": "Fundamentals",
    "text": "Fundamentals\nNote that all subjects below can be studied in various levels of depth and mathematical rigor. Where do you stop heavily depends on what you want to do in your carreer.\nLinear Algebra is a language of data. Most data types (structured, nested, images, video, text, voice, etc) can be represented as multidimensional arrays and tensors. The vast majority of machine learning and mathematical models can be reduced to operations on matrices, which is important if we want to implement them in code. Moreover matrix decomposition algorithms like SVD, Cholesky are useful tools in many practical applications. 1\n1 On a personal note, linear algebra was my gateway drug to applied mathematics and machine learning. Without that course it is possible I would’ve never discovered a passion for mathMy perspective over linear algebra is computational and geometric, in the sense of the “space” the data points live in and transformations of that space. It’s also a very concise and elegant way to define models, loss functions, and optimization problems.\nI highly recommend the following resources: Essence of Linear Algebra (conceptual), Coding the Matrix (computational), Linear Algebra Done Right (abstract, mathematical), Mathematics for ML (applied), and S. Boyd’s course (applied).\nMathematical Analysis is all about change, formalizing how a function behaves with respect to its arguments and parameters. It is an essential building block in probability theory,2 optimization, and deep learning (automated differentiation).\n2 Many mathematically rigorous degrees also require real analysis, measure theory, and even functional analysisI argue that in order to understand any complex system, be it a firm, an economy, the climate, or environnment, we have to model how it evolves in time. This suggests the importance of differential equations and systems dynamics, modeling the feedback loops. All of this would be very awkward to reason about without mathematical analysis.\nHowever, be warned, mathematical analysis is a rabbit-hole! The only courses which I can recommend without you going into years of proof-based study, is “Calculus, Applied!” from Harvard and John Hopkins’ specialization, which has some cool modeling applications with Python code!\nProbability theory is the language and logic of uncertainty. In statistics, we change our mind and actions in the face of evidence. Just to highlight how important these two complementary fields are, it is hard for me to find an applied domain or aspect of life where probability and statistics isn’t useful (at least, in principle). I dare you to find one!\nMoreover, our everyday intuitions make us terrible at probability, which is another reason why we need a more formal tool to reason and make decisions under uncertainty. Why then so many people hate it as a subject? Is it dry teaching, fear of math and abstraction? In the module two, I recommend a simulation and story-based approach to probability, with many resources which will make studying it fun and practically relevant. 3\n3 In my case, Taleb Nassim’s “Fooled by Randomness” was what motivated me to study probability seriously. I do highly recommend it",
    "crumbs": [
      "Appendix",
      "1. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#applied-mathematics",
    "href": "01_fundamentals/prerequisites.html#applied-mathematics",
    "title": "Why did you study all of that",
    "section": "Applied Mathematics",
    "text": "Applied Mathematics\nHere is an explanation for why other subjects are needed – which students from backgrounds other than quantitative economics or computer science might’ve not encountered. This is a point in which you see reseachers specializing.\nEconometrics, in my personal opinion, tries to separate the signal from noise and make inferences about the causal processes in economic phenomena. It specializes statistics to the domain of economics, which presents new modeling challenges. Remember that in order to draw causal conclusions, we need a scientific theory, not just data alone. It’s not sufficient to check if statistical assumptions hold, but we have to make clear our causal and methodological assumptions.\nYou should be careful in introductory econometrics, as you can claim only association with the tools you will learn. Even in literature, there is this weird tension between an emphasis that most conclusions aren’t causal, but them treating them as such for policy-making. Leaving the causal inference aside for a moment, I can safely recommend “Mostly Harmless Econometrics” and “Econometrics with R”.\nTime Series Analysis (senso largo), bridges the gap between theoretical dynamical models (systems of differential equations) with statistics and probability (stochastic processes). It adapts those tools to make inferences and predictions about phenomena which evolve in time, that are dynamic in their nature. I like the metaphor of data assimilation, which is actually an entire field trying to introduce the empirical dimension to all kinds of dynamical systems.\nI don’t particularly like any books I read or courses I took on time series. However, I can recommend N. Vandeput’s “Demand Forecasting Best Practices” and his other writing. For practitioners, R. Hyndman’s “Forecasting: Principles and Practice (3ed)” and I. Svetunkov’s “Forecasting and Analytics with ADAM” are a good starting point.\nOperations Research is about optimization with constraints, allocation of limited resources, and efficiency. The easy part is to apply an integer or linear programming algorithm to an already formulated problem. The hard part is to reduce a messy real world problem at a large scale to that formulation, especially under uncertainty and nonlinearity.\nI’d argue that without implementing a problem in code with established OR libraries and inferring the parameters of the problem from data – we don’t really know how to do optimization. The only free resources on Operations Research with code which I know of are C. Kwon’s “Julia Programming for Operations Research (2ed)” and Huber’s “Algorithms for optimization”. Others are strictly focused on optimization for ML and deep learning.\n\n\n\n\n\n\nA word of encouragement\n\n\n\nNone of those courses were useless. Think of how can we take parts from each of those prerequisites which are relevant for decision science, so that we have more tools to solve the complicated problems we encounter.",
    "crumbs": [
      "Appendix",
      "1. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prerequisites.html#a-note-on-economics",
    "href": "01_fundamentals/prerequisites.html#a-note-on-economics",
    "title": "Why did you study all of that",
    "section": "A note on Economics",
    "text": "A note on Economics\nIn mathematically-oriented courses, you can think of economics as optimization with constraints, which are given by limited resources and our positive or normative theories of decision-making. In the most general sense, economics studies behavior and interactions of economic agents.\nWhen we make decisions, we like to think of ourselfs as objective, but we have lots of biases and blind spots which prevent us to see the reality clearly. For example, we find patterns and regularities which are not real, but treat the conclusions as causal; we tend to seek data which supports our hypothesis and ignore contradictory evidence, etc.\n\nThis kind of knowledge about human behavior helps us to be wiser and practice active open-mindedness 4\nIn other words, we want to prevent self-deception and self-sabotage towards achieving the stated goals and objectives – and even evaluate if the goals chosen well\n\n4 Actively open-minded thinking (AOT) is measured by items that tap the willingness to consider alternative opinions, sensitivity to evidence contradictory to current beliefs, the willingness to postpone closure, and reflective thought.Moving on to more practical things, the question is not how much economics a decision scientist should know, but what kind of economics. This list will be wildly different for people going into quantitative finance, banking, insurance, or public policy.\nIf you studied in a program which provides a strong background in quantitative economics, econometrics, and operations research – I have some good news and bad news. The good news is that you know all the mathematics you will need in practice and you’re well versed in formulating good economic research questions. The bad news is that you probably need a lot of effort to improve your programming skills and that the kinds of economics you studied don’t translate well inside a firm.\nPlease, do not mistake corporate finance, marketing, management, operations, supply chain, logistics, CRM, and production as fluff. There is a good reason people do business degrees and get MBAs. If you look at a top marketing journal, you will be surprised at the complexity of models and innovations in econometrics and machine learning needed to tackle those challenges. Moreover, when you combine marketing with behavioral economics and psychology, it adds another layer of nuance and understanding of consumer behavior. We can’t learn this all from data alone.\nDepending on your role in the firm, you might need to read about demand planning, advertisement, pricing, manufacturing, etc, in order to speak the language of decision-makers and stakeholders. I’m leaving you with a list of youtube playlists and channels which I found helpful to keep up-to-date with the way modern businesses operate:\n\nFinance: “Money & Macro”, Patrick Boyle, “The Plain Bagel”, Ben Felix, A. Damodaran’s NYU MBA\nWhen I was in university, I was interested in the Global Financial Crisis and found Perry Mehrling’s course on Economics of Money and Banking to be invaluable in cutting through the finance/banking jargon.\nBusiness: “Modern MBA”, “How Money Works”, Slidebean, Logically Answered\nEconomics: “Rethinking Economics”, “Institute for New Economic Thinking”",
    "crumbs": [
      "Appendix",
      "1. Why study all of that?"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html",
    "href": "01_fundamentals/prob_study_guide.html",
    "title": "Probability and statistics",
    "section": "",
    "text": "This study guide is designed as a complementary resource to “Simulation of economic processes”. It provides additional references for studying and practicing the fundamentals of probability and statistics.\nIt’s possible you didn’t enjoy your statistics classes. This happened in my case, despite a passion for the field. In this journey of decision science, we have to give statistics another chance, but we dramatically change the strategy of how we learn it. We’ll heavily use simulations, stories, and case-studies which highlight the practical relevance of key theoretical ideas.1 Speegle’s Probability, Statistics, and Data: A fresh approach using R and Cetinkaya-Runde’s Introduction to Modern Statistics are excellent examples of this new way of teaching.\nThere is absolutely no shame in going back to a subject again with a mindset of mastering the fundamentals. For me, this exercise turned out to be one of the most valuable things I’ve done.",
    "crumbs": [
      "Appendix",
      "2. Probability and Statistics Study Guide"
    ]
  },
  {
    "objectID": "01_fundamentals/prob_study_guide.html#probability-theory",
    "href": "01_fundamentals/prob_study_guide.html#probability-theory",
    "title": "Probability and statistics",
    "section": "Probability theory",
    "text": "Probability theory\nFirst, read the introductory chapter in which I explain how probability fits in the larger context of decision science and how can we use simulation to learn and appreciate it better. I highly recommend these lectures by Santosh A. Venkatesh (SV) and Joseph Blitzstein’s Probability 110 course (JB) as your main references for probability theory.\n\nCombinatorics and sampling\nIn the first lab, where we simulated the birthday problem, I mentioned the importance of knowing the stories behind core concepts in combinatorics, like \\({n \\choose k}\\) “n choose k”, factorial, falling factorial, and the multiplication rule. They translate into urn models and sampling procedures (ordered, unordered, with and without replacement).\nThe first three lectures from SV and JB should be more than sufficient as a review of combinatorics and naive probability.2 In the latter you will see how urn models were successfully applied in modeling different phenomena in physics.\n2 Pay close attention to their story proofs and the effort they put in to get you over the fear of mathematical abstraction\n\n\n\n\n\nRabbit hole: unordered sampling with replacement\n\n\n\nIn S. Venkatesh, Tableau 3, there is an elegant proof of unordered sampling with replacement, which was used by Bose and Einstein to describe the behavior of bosons. The result will be useful to us when we’ll introduce bootstrap, but you can check out a neat explanation by Riaz Khan.\n\n\nI understand if your reaction at this point is: “Are you kidding me, this is just a warm-up?”. I would like to convince you that the time investment is not that big and the payoff is huge. You will encounter fun stories and applications in multiple domains, get comfortable with mathematical abstraction, and see that probability doesn’t have to be a chore or just a prerequisite you have to get over with.",
    "crumbs": [
      "Appendix",
      "2. Probability and Statistics Study Guide"
    ]
  }
]